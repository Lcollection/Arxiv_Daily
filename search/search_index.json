{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u8fde\u63a5\u5267\u96c6\u4e0e\u8bed\u4e49\uff1a\u4e00\u79cd\u7406\u89e3\u957f\u7bc7\u89c6\u9891\u7684\u65b0\u6846\u67b6 \u5c3d\u7ba1\u73b0\u6709\u7814\u7a76\u5e38\u5e38\u5c06\u957f\u89c6\u9891\u89c6\u4e3a\u6269\u5c55\u7684\u77ed\u89c6\u9891\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4e86\u4eba\u7c7b\u7684\u8ba4\u77e5\u3002\u672c\u6587\u4ecb\u7ecd\u4e86BREASE\uff1a\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u6865\u63a5\u60c5\u8282\u4e0e\u8bed\u4e49\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6a21\u62df\u60c5\u8282\u8bb0\u5fc6\u7684\u79ef\u7d2f\u4ee5\u6355\u6349\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u5206\u6563\u5728\u89c6\u9891\u4e2d\u7684\u8bed\u4e49\u77e5\u8bc6\u5bf9\u5176\u8fdb\u884c\u5f3a\u5316\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u6709\u4e24\u4e2a\u5173\u952e\u8d21\u732e\uff1a\u9996\u5148\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u60c5\u8282\u538b\u7f29\u5668\uff08ECO\uff09\uff0c\u80fd\u591f\u4ece\u5fae\u89c2\u5230\u534a\u5b8f\u89c2\u5c42\u9762\u9ad8\u6548\u5730\u805a\u5408\u5173\u952e\u8868\u5f81\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u68c0\u7d22\u5668\uff08SeTR\uff09\uff0c\u901a\u8fc7\u805a\u7126\u4e8e\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\uff0c\u7528\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u8fd9\u4e9b\u805a\u5408\u8868\u5f81\uff0c\u663e\u8457\u964d\u4f4e\u7279\u5f81\u7ef4\u5ea6\u540c\u65f6\u4fdd\u7559\u76f8\u5173\u7684\u5b8f\u89c2\u5c42\u9762\u4fe1\u606f\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBREASE\u5728\u96f6\u6837\u672c\u548c\u5b8c\u5168\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\u548c\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://joslefaure.github.io/assets/html/hermes.html\u3002 Gueter Josmy Faure PDF N/A Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html. SelectTTS\uff1a\u901a\u8fc7\u57fa\u4e8e\u79bb\u6563\u5355\u5143\u7684\u5e27\u9009\u62e9\u5408\u6210\u4efb\u4f55\u4eba\u7684\u58f0\u97f3 \u5408\u6210\u672a\u89c1\u53d1\u8a00\u8005\u7684\u58f0\u97f3\u662f\u591a\u8bf4\u8bdd\u4eba\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u4e2d\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002\u5927\u591a\u6570\u591a\u8bf4\u8bdd\u4ebaTTS\u6a21\u578b\u4f9d\u8d56\u4e8e\u5728\u8bad\u7ec3\u671f\u95f4\u901a\u8fc7\u8bf4\u8bdd\u4eba\u8c03\u8282\u6765\u5efa\u6a21\u8bf4\u8bdd\u4eba\u7279\u5f81\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u672a\u89c1\u8bf4\u8bdd\u4eba\u5c5e\u6027\u8fdb\u884c\u5efa\u6a21\u9700\u8981\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u8fd9\u4f7f\u5f97\u91cd\u73b0\u7ed3\u679c\u548c\u6539\u8fdb\u7ed3\u679c\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u4eec\u63d0\u51fa\u4e86SelectTTS\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4ece\u76ee\u6807\u8bf4\u8bdd\u4eba\u4e2d\u9009\u62e9\u9002\u5f53\u7684\u5e27\uff0c\u5e76\u4f7f\u7528\u5e27\u7ea7\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7279\u5f81\u8fdb\u884c\u89e3\u7801\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u672a\u89c1\u8bf4\u8bdd\u4eba\u7684\u7279\u5f81\uff0c\u5e76\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u6307\u6807\u4e0a\u4e0e\u5176\u5b83\u591a\u8bf4\u8bdd\u4ebaTTS\u6846\u67b6\u53d6\u5f97\u53ef\u6bd4\u7684\u7ed3\u679c\u3002\u901a\u8fc7SelectTTS\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4ece\u76ee\u6807\u8bf4\u8bdd\u4eba\u7684\u8bed\u97f3\u4e2d\u9009\u62e9\u5e27\u662f\u4e00\u79cd\u76f4\u63a5\u7684\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5728\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u5bf9\u672a\u89c1\u8bf4\u8bdd\u4eba\u7684\u6cdb\u5316\u3002\u6211\u4eec\u5728\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u6027\u80fd\u4e0a\u4f18\u4e8eSOTA\u57fa\u7ebfXTTS-v2\u548cVALL-E\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u4e868\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u4e86270\u500d\u3002 Ismail Rasim Ulgen PDF N/A SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data"},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}