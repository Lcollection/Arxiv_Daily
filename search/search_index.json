{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract RoboTwin\uff1a\u914d\u5907\u751f\u6210\u5f0f\u6570\u5b57\u5b6a\u751f\u7684\u53cc\u81c2\u673a\u5668\u4eba\u57fa\u51c6\uff08\u65e9\u671f\u7248\u672c\uff09 \u53cc\u81c2\u673a\u5668\u4eba\u53ca\u5176\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u6709\u6548\u534f\u4f5c\u5728\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u4e2d\u65e5\u76ca\u91cd\u8981\u3002\u8fd9\u4e9b\u6280\u80fd\u5728\u6269\u5c55\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u7684\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u80fd\u529b\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0c\u8fdb\u5c55\u56e0\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bad\u7ec3\u6570\u636e\u800c\u53d7\u963b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86RoboTwin\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u548c\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u5408\u6210\u6570\u636e\uff0c\u4e13\u4e3a\u53cc\u81c2\u673a\u5668\u4eba\u573a\u666f\u8bbe\u8ba1\u3002\u5229\u7528COBOT Magic\u5e73\u53f0\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u5173\u4e8e\u5de5\u5177\u4f7f\u7528\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u591a\u6837\u5316\u6570\u636e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528AI\u751f\u6210\u7684\u5185\u5bb9\u521b\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u5c062D\u56fe\u50cf\u8f6c\u5316\u4e3a\u8be6\u7ec6\u76843D\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e13\u5bb6\u7ea7\u522b\u7684\u8bad\u7ec3\u6570\u636e\u548c\u9762\u5411\u529f\u80fd\u7684\u4efb\u52a1\u7279\u5b9a\u59ff\u6001\u5e8f\u5217\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1) RoboTwin\u57fa\u51c6\u6570\u636e\u96c6\uff0c2) \u9ad8\u6548\u7684\u73b0\u5b9e\u5230\u6a21\u62df\u7ba1\u9053\uff0c\u4ee5\u53ca3) \u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u4e13\u5bb6\u7ea7\u522b\u6570\u636e\u751f\u6210\u3002\u8fd9\u4e9b\u8fdb\u5c55\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u8bad\u7ec3\u6570\u636e\u7684\u77ed\u7f3a\u95ee\u9898\uff0c\u6709\u671b\u52a0\u901f\u5f00\u53d1\u66f4\u591a\u80fd\u529b\u66f4\u5f3a\u3001\u9002\u5e94\u6027\u66f4\u5e7f\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u3002\u9879\u76ee\u9875\u9762\u4f4d\u4e8ehttps://robotwin-benchmark.github.io/early-version/\u3002 Yao Mu PDF N/A RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version) Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at https://robotwin-benchmark.github.io/early-version/ HiPrompt\uff1a\u4f7f\u7528\u5206\u5c42\u591a\u5c42\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u5b9e\u73b0\u65e0\u8c03\u4f18\u7684\u9ad8\u5206\u8fa8\u7387\u751f\u6210 \u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u6269\u5c55\u52304K\u5206\u8fa8\u7387\u53ca\u4ee5\u4e0a\u65f6\uff0c\u5e38\u5e38\u9762\u4e34\u7269\u4f53\u91cd\u590d\u548c\u7ed3\u6784\u4f2a\u5f71\u7684\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u95ee\u9898\u5728\u4e8e\u751f\u6210\u591a\u5c3a\u5ea6\u56fe\u50cf\u65f6\u4f7f\u7528\u5355\u4e00\u63d0\u793a\u7684\u6548\u7387\u4e0d\u8db3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86HiPrompt\uff0c\u4e00\u79cd\u65e0\u9700\u8c03\u6574\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u63d0\u793a\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5206\u5c42\u63d0\u793a\u63d0\u4f9b\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u7684\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5168\u5c40\u6307\u5bfc\u6765\u81ea\u63cf\u8ff0\u6574\u4f53\u5185\u5bb9\u7684\u7528\u6237\u8f93\u5165\uff0c\u800c\u5c40\u90e8\u6307\u5bfc\u5219\u5229\u7528\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff08MLLMs\uff09\u7684\u9010\u5757\u63cf\u8ff0\uff0c\u7cbe\u7ec6\u5730\u5f15\u5bfc\u533a\u57df\u7ed3\u6784\u548c\u7eb9\u7406\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u5728\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u7684\u566a\u58f0\u88ab\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u7a7a\u95f4\u6210\u5206\u3002\u8fd9\u4e9b\u6210\u5206\u5728\u591a\u4e2a\u63d0\u793a\u5c42\u7ea7\u4e0a\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u9010\u5757\u63cf\u8ff0\u548c\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u5728\u5206\u5c42\u8bed\u4e49\u6307\u5bfc\u4e0b\u5b9e\u73b0\u63d0\u793a\u5f15\u5bfc\u7684\u53bb\u566a\u3002\u8fd9\u8fdb\u4e00\u6b65\u4f7f\u5f97\u751f\u6210\u8fc7\u7a0b\u66f4\u4e13\u6ce8\u4e8e\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5728\u4fdd\u6301\u9ad8\u6e05\u6670\u5ea6\u7684\u540c\u65f6\uff0c\u5177\u6709\u8fde\u8d2f\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHiPrompt\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u91cd\u590d\u73b0\u8c61\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u8d28\u91cf\u3002 Xinyu Liu PDF N/A HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts The potential for higher-resolution image generation using pretrained diffusion models is immense, yet these models often struggle with issues of object repetition and structural artifacts especially when scaling to 4K resolution and higher. We figure out that the problem is caused by that, a single prompt for the generation of multiple scales provides insufficient efficacy. In response, we propose HiPrompt, a new tuning-free solution that tackles the above problems by introducing hierarchical prompts. The hierarchical prompts offer both global and local guidance. Specifically, the global guidance comes from the user input that describes the overall content, while the local guidance utilizes patch-wise descriptions from MLLMs to elaborately guide the regional structure and texture generation. Furthermore, during the inverse denoising process, the generated noise is decomposed into low- and high-frequency spatial components. These components are conditioned on multiple prompt levels, including detailed patch-wise descriptions and broader image-level prompts, facilitating prompt-guided denoising under hierarchical semantic guidance. It further allows the generation to focus more on local spatial regions and ensures the generated images maintain coherent local and global semantics, structures, and textures with high definition. Extensive experiments demonstrate that HiPrompt outperforms state-of-the-art works in higher-resolution image generation, significantly reducing object repetition and enhancing structural quality. SpecMon\uff1a\u5b89\u5168\u534f\u8bae\u7684\u6a21\u5757\u5316\u9ed1\u76d2\u8fd0\u884c\u65f6\u76d1\u63a7 \u5728\u5f62\u5f0f\u5316\u534f\u8bae\u89c4\u8303\u4e0e\u5176\u5177\u4f53\u5b9e\u73b0\u4e4b\u95f4\u5b58\u5728\u9a8c\u8bc1\u5dee\u8ddd\uff0c\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u76d1\u63a7\u4ee5\u7b26\u5408\u5f62\u5f0f\u5316\u89c4\u8303\u7684\u65b9\u5f0f\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002\u6211\u4eec\u901a\u8fc7\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u7684\u7f51\u7edc\u548c\u52a0\u5bc6\u5e93\u83b7\u53d6\u4e8b\u4ef6\u6d41\uff0c\u5373\u4f7f\u6ca1\u6709\u6e90\u4ee3\u7801\u8bbf\u95ee\u6743\u9650\u4e5f\u80fd\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u79cd\u9ad8\u6548\u7684\u7b97\u6cd5\u5c06\u8fd9\u4e9b\u89c2\u5bdf\u7ed3\u679c\u4e0e\u89c4\u8303\u6a21\u578b\u4e2d\u6709\u6548\u7684\u8f68\u8ff9\u8fdb\u884c\u5339\u914d\u3002\u4e0e\u4ee5\u5f80\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u975e\u786e\u5b9a\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u5904\u7406\u591a\u4e2a\u4f1a\u8bdd\u3002\u5b83\u8fd8\u5b9e\u73b0\u4e86\u4f4e\u5f00\u9500\uff0c\u6211\u4eec\u5728WireGuard\u53c2\u8003\u5b9e\u73b0\u548c\u4e4b\u524d\u5de5\u4f5c\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u8fd9\u4e00\u70b9\u3002\u6211\u4eec\u53d1\u73b0\uff0cWireGuard\u7684\u53c2\u8003Tamarin\u6a21\u578b\u53ea\u9700\u7a0d\u4f5c\u6539\u52a8\u5373\u53ef\u4f7f\u7528\uff1a\u6211\u4eec\u53ea\u9700\u8981\u6307\u5b9a\u7ebf\u8def\u683c\u5f0f\u5e76\u4fee\u6b63\u6211\u4eec\u5728\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u65f6\u53d1\u73b0\u7684\u4e00\u4e9b\u5c0f\u9519\u8bef\u3002\u6211\u4eec\u8fd8\u4e3a\u6211\u4eec\u7684\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5065\u5168\u6027\u7ed3\u679c\uff0c\u786e\u4fdd\u5b83\u53ea\u63a5\u53d7\u6839\u636e\u89c4\u8303\u6a21\u578b\u6709\u6548\u7684\u6d3b\u52a8\u6d41\u3002 Kevin Morio PDF N/A SpecMon: Modular Black-Box Runtime Monitoring of Security Protocols There exists a verification gap between formal protocol specifications and their actual implementations, which this work aims to bridge via monitoring for compliance to the formal specification. We instrument the networking and cryptographic library the application uses to obtain a stream of events. This is possible even without source code access. We then use an efficient algorithm to match these observations to traces that are valid in the specification model. In contrast to prior work, our algorithm can handle non-determinism and thus, multiple sessions. It also achieves a low overhead, which we demonstrate on the WireGuard reference implementation and a case study from prior work. We find that the reference Tamarin model for WireGuard can be used with little change: We only need to specify wire formats and correct some small inaccuracies that we discovered while conducting the case study. We also provide a soundness result for our algorithm that ensures it accepts only event streams that are valid according to the specification model. UC-NeRF\uff1a\u4ece\u5185\u7aa5\u955c\u7a00\u758f\u89c6\u56fe\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6761\u4ef6\u795e\u7ecf\u8f90\u5c04\u573a \u53ef\u89c6\u5316\u624b\u672f\u573a\u666f\u5bf9\u4e8e\u5728\u5fae\u521b\u624b\u672f\u8fc7\u7a0b\u4e2d\u63ed\u793a\u5185\u90e8\u89e3\u5256\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002\u65b0\u89c6\u89d2\u5408\u6210\u662f\u4e00\u9879\u5173\u952e\u6280\u672f\uff0c\u63d0\u4f9b\u51e0\u4f55\u548c\u5916\u89c2\u91cd\u5efa\uff0c\u589e\u5f3a\u5bf9\u624b\u672f\u573a\u666f\u7684\u7406\u89e3\u3001\u89c4\u5212\u548c\u51b3\u7b56\u3002\u5c3d\u7ba1\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u624b\u672f\u573a\u666f\u4f1a\u4ea7\u751f\u4e0d\u6ee1\u610f\u7684\u7ed3\u679c\uff0c\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u5185\u7aa5\u955c\u7a00\u758f\u89c6\u89d2\u548c\u663e\u8457\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6761\u4ef6\u6027NeRF\u7528\u4e8e\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4ee5\u89e3\u51b3\u4ece\u7a00\u758f\u624b\u672f\u89c6\u89d2\u4e2d\u4e25\u91cd\u7684\u5f62\u72b6-\u8f90\u5c04\u6a21\u7cca\u95ee\u9898\u3002UC-NeRF\u7684\u6838\u5fc3\u662f\u5c06\u591a\u89c6\u89d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5f15\u5165\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u5efa\u6a21\u4e25\u91cd\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0cUC-NeRF\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u7acb\u4f53\u7f51\u7edc\u5f62\u5f0f\u7684\u8fde\u7eed\u6027\u5b66\u4e60\u5668\uff0c\u4ee5\u5efa\u7acb\u7a00\u758f\u89c6\u89d2\u7684\u51e0\u4f55\u5bf9\u5e94\u5173\u7cfb\u5e76\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7279\u5f81\u5148\u9a8c\u3002\u5728\u795e\u7ecf\u6e32\u67d3\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u7840\u81ea\u9002\u5e94\u7684NeRF\u7f51\u7edc\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u663e\u5f0f\u5904\u7406\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u51e0\u4f55\u84b8\u998f\u6765\u589e\u5f3a\u51e0\u4f55\u5b66\u4e60\u3002\u5728SCARED\u548cHamlyn\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u5728\u6e32\u67d3\u5916\u89c2\u548c\u51e0\u4f55\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u6301\u7eed\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728\\url{https://github.com/wrld/UC-NeRF}\u4e0a\u53d1\u5e03\u3002 Jiaxin Guo PDF N/A UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches. Our code will be released at \\url{https://github.com/wrld/UC-NeRF}. \u5927\u8bed\u8a00\u6a21\u578b\u80fd\u8003\u53d6\u9a7e\u7167\u5417\uff1f \u2014\u2014 \u8fc8\u5411\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u53ef\u9760AGI\u7684\u57fa\u51c6\u6d4b\u8bd5 \u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u8fd1\u671f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u8bb8\u591a\u7814\u7a76\u81f4\u529b\u4e8e\u5229\u7528\u5176\u901a\u7528\u77e5\u8bc6\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0cLVLMs\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7684\u901a\u7528\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u548c\u5b89\u5168\u9a7e\u9a76\u6240\u9700\u7684\u4e13\u95e8\u77e5\u8bc6\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u9a7e\u9a76\u6570\u636e\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u573a\u666f\u7406\u89e3\u548c\u51b3\u7b56\u4e0a\uff0c\u6ca1\u6709\u63d0\u4f9b\u5173\u4e8e\u4ea4\u901a\u89c4\u5219\u548c\u9a7e\u9a76\u6280\u80fd\u7684\u660e\u786e\u6307\u5bfc\uff0c\u800c\u8fd9\u4e9b\u662f\u76f4\u63a5\u5173\u7cfb\u5230\u9a7e\u9a76\u5b89\u5168\u7684\u5173\u952e\u65b9\u9762\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86IDKB\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7\u4e00\u767e\u4e07\u6761\u6570\u636e\u9879\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6536\u96c6\u81ea\u591a\u4e2a\u56fd\u5bb6\uff0c\u5305\u62ec\u9a7e\u9a76\u624b\u518c\u3001\u7406\u8bba\u6d4b\u8bd5\u6570\u636e\u548c\u6a21\u62df\u8def\u8bd5\u6570\u636e\u3002IDKB\u6db5\u76d6\u4e86\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u51e0\u4e4e\u6240\u6709\u660e\u786e\u7684\u9a7e\u9a76\u6240\u9700\u77e5\u8bc6\uff0c\u7c7b\u4f3c\u4e8e\u83b7\u53d6\u9a7e\u9a76\u6267\u7167\u7684\u8fc7\u7a0b\u3002\u7279\u522b\u5730\uff0c\u6211\u4eec\u4f7f\u7528IDKB\u5bf915\u4e2aLVLMs\u8fdb\u884c\u4e86\u5168\u9762\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u80cc\u666f\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\u3002\u6211\u4eec\u8fd8\u5bf9\u6d41\u884c\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728\u4ee5\u4e0b\u94fe\u63a5\u627e\u5230\uff1a\\url{https://4dvlab.github.io/project_page/idkb.html} Yuhang Lu PDF N/A Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guidance on traffic rules and driving skills, which are critical aspects directly related to driving safety. To bridge this gap, we propose IDKB, a large-scale dataset containing over one million data items collected from various countries, including driving handbooks, theory test data, and simulated road test data. Much like the process of obtaining a driver's license, IDKB encompasses nearly all the explicit knowledge needed for driving from theory to practice. In particular, we conducted comprehensive tests on 15 LVLMs using IDKB to assess their reliability in the context of autonomous driving and provided extensive analysis. We also fine-tuned popular models, achieving notable performance improvements, which further validate the significance of our dataset. The project page can be found at: \\url{https://4dvlab.github.io/project_page/idkb.html}"},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}