# arxiv 2025-12-14

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 立体空间：通过规范空间中的端到端扩散实现无深度立体几何合成 | Tjark Behrens | [PDF](https://arxiv.org/pdf/2512.10959v1) | 我们提出StereoSpace，一种基于扩散模型的单目转立体合成框架，该框架仅通过视角条件建模几何关系，无需显式深度或形变操作。通过构建规范矫正空间并结合条件引导，生成器能够端到端地推断视差对应关系并填补遮挡区域。为确保公平且无数据泄露的评估，我们提出一种端到端评估协议，在测试阶段完全排除真实几何数据或代理几何估计的干扰。该协议重点关注反映下游应用价值的指标：以iSQoE衡量感知舒适度，以MEt3R评估几何一致性。StereoSpace在形变修复、潜在空间形变及形变条件控制三类方法中均取得领先，在分层场景与非朗伯表面场景中实现了锐利的视差效果与强大的鲁棒性。这确立了视角条件扩散模型作为无需深度信息的可扩展立体生成解决方案的有效性。 |
| WorldLens：现实世界中驾驶世界模型的全方位评估 | Ao Liang | [PDF](https://arxiv.org/pdf/2512.10958v1) | 生成式世界模型正在重塑具身人工智能，使智能体能够合成逼真的四维驾驶环境——这些场景视觉上令人信服，却常在物理规律或行为逻辑上出现破绽。尽管技术飞速发展，该领域仍缺乏统一标准来评估生成世界是否保持几何一致性、遵循物理法则或支持可靠控制。我们推出WorldLens全频谱基准测试体系，从生成、理解与行为三个维度系统评估模型在其构建世界中的综合表现。该体系涵盖五大层面——环境生成、场景重建、指令跟随、下游任务与人类偏好——共同检验视觉真实感、几何一致性、物理合理性与功能可靠性。

研究发现，现有世界模型在不同维度表现失衡：纹理精细的模型常违背物理规律，而几何稳定的模型又缺乏行为保真度。为实现客观指标与人类判断的对齐，我们进一步构建了WorldLens-26K数据集，包含2.6万段带数值评分与文本解释的人工标注视频，并研发了WorldLens-Agent评估模型——通过从标注数据中蒸馏学习，实现可扩展、可解释的自动化评分。这套基准测试、数据集与评估模型共同构成了衡量世界保真度的统一生态系统，为未来模型评估确立了新标准：不仅要看生成世界有多真实，更要看它们如何真实地运行。 |
| 场景构建器：基于解耦去遮挡与姿态估计模型的开放集三维场景生成技术 | Yukai Shi | [PDF](https://arxiv.org/pdf/2512.10957v1) | 在本研究中，我们提出了一种名为SceneMaker的解耦式三维场景生成框架。由于缺乏足够的开放集去遮挡与姿态估计先验知识，现有方法在严重遮挡和开放集场景下难以同时生成高质量几何结构与精确姿态。为解决这些问题，我们首先将去遮挡模型从三维物体生成过程中解耦，并通过融合图像数据集与收集的去遮挡数据集来增强模型对多样化开放集遮挡模式的适应能力。随后，我们提出一种统一姿态估计模型，该模型在自注意力与交叉注意力机制中融合全局与局部特征以提升精度。此外，我们构建了开放集三维场景数据集，进一步拓展了姿态估计模型的泛化能力。综合实验表明，我们的解耦框架在室内场景与开放集场景中均展现出优越性能。相关代码与数据集已发布于https://idea-research.github.io/SceneMaker/。 |
| 借助立体视觉与中层视觉技术，赋能动态城市导航系统 | Wentao Zhou | [PDF](https://arxiv.org/pdf/2512.10956v1) | 基础模型在语言和视觉领域的成功，推动了全端到端机器人导航基础模型的研究。这类模型直接将单目视觉输入映射为控制动作，完全跳过了中层视觉模块（如跟踪、深度估计等）。虽然视觉能力会隐式涌现的假设颇具吸引力，但这需要大量从像素到动作的监督数据，而这些数据难以获取。在动态和非结构化环境中，这一挑战尤为突出：鲁棒的导航需要精确的几何与动态理解，而单目视图中的深度尺度模糊性进一步限制了准确的空间推理。本文论证了仅依赖单目视觉并忽略中层视觉先验是低效的。

我们提出了StereoWalker模型，通过立体视觉输入及显式中层视觉信息（如深度估计与密集像素跟踪）来增强导航基础模型。我们的思路很直接：立体输入解决了深度尺度模糊问题，而现代中层视觉模型能为动态场景提供可靠的几何与运动结构。我们还构建了一个大规模立体导航数据集，通过互联网立体视频自动标注动作，以支持StereoWalker的训练并促进未来研究。实验表明，中层视觉使StereoWalker仅用1.5%的训练数据即可达到当前最优模型的性能，使用全量数据时则能实现超越。同时我们发现，立体视觉相比单目输入能带来更高的导航性能。 |
| 全属性：面向视觉概念个性化的开放词汇属性编码器 | Tsai-Shien Chen | [PDF](https://arxiv.org/pdf/2512.10955v1) | 视觉概念个性化旨在仅将特定图像属性（如身份、表情、光照和风格）迁移至未见过的场景中。然而，现有方法依赖于通用图像编码器提取的整体嵌入表示，这些表示往往纠缠了多种视觉因素，难以分离单一属性，常导致信息泄露与合成结果不连贯。为突破这一局限，我们提出了Omni-Attribute——首个开放词汇的图像属性编码器，专门用于学习高保真、属性特定的表征。我们的方法从数据和模型两个层面进行协同设计：（一）通过构建语义关联的图像对并标注正负属性，显式指导编码器学习保留或抑制特定信息；（二）采用双目标训练范式，在生成保真度与对比解耦之间取得平衡。实验表明，所得嵌入表示在开放词汇属性检索、个性化定制及组合生成任务中均表现优异，在多项基准测试中达到了最先进的性能水平。 |
| 双向归一化流：从数据到噪声及其逆向过程 | Yiyang Lu | [PDF](https://arxiv.org/pdf/2512.10953v1) | 归一化流（Normalizing Flows，NFs）已被确立为生成建模的一种原则性框架。标准NF包含前向过程与反向过程：前向过程将数据映射为噪声，反向过程则通过逆向映射生成样本。典型的NF前向变换受显式可逆性约束，确保反向过程可作为其精确解析逆。近期TARFlow及其变体通过结合Transformer与自回归流技术，为NF方法注入了新活力，但也暴露出因果解码成为主要瓶颈。本研究提出双向归一化流（$\textbf{BiFlow}$），该框架无需依赖精确解析逆。BiFlow通过学习近似底层噪声到数据逆向映射的反向模型，实现了更灵活的损失函数与架构设计。在ImageNet数据集上的实验表明，相较于因果解码方案，BiFlow在提升生成质量的同时，将采样速度加快达两个数量级。该框架在基于NF的方法中取得了最先进成果，并在单次评估（"1-NFE"）方法中展现出竞争优势。随着NF领域近期取得令人鼓舞的进展，我们希望本研究能进一步唤起对这一经典范式的关注。 |
| 高质量数据共享的分层数据集选择 | Xiaona Zhou | [PDF](https://arxiv.org/pdf/2512.10952v1) | 现代机器学习的成功依赖于高质量训练数据的获取。在许多现实场景中，例如从公共存储库获取数据或跨机构共享数据时，数据天然以离散数据集的形式存在，这些数据集在相关性、质量和效用上存在差异。因此，选择搜索哪些存储库或机构以获取有用数据集，以及将哪些数据集纳入模型训练，都是关键决策。然而，现有方法大多选择单个样本，并将所有数据视为同等相关，忽略了数据集及其来源之间的差异。本研究将数据集选择任务形式化：从大规模异构数据池中选择完整数据集，以在资源约束下提升下游性能。我们提出基于层次结构的数据集选择方法（DaSH），该方法在数据集和群组（如集合、机构）两个层面建模效用，能够从有限观测中实现高效泛化。在两个公共基准测试（Digit-Five和DomainNet）中，DaSH在准确率上比现有最优数据选择基线方法提升高达26.2%，同时所需探索步骤显著减少。消融实验表明，DaSH对低资源环境和相关数据集缺失具有鲁棒性，使其适用于实际多源学习工作流程中可扩展且自适应的数据集选择。 |
| 群体扩散：通过解锁跨样本协作增强图像生成 | Sicheng Mo | [PDF](https://arxiv.org/pdf/2512.10954v1) | 在本研究中，我们探索了扩散模型推理中一个尚未开发的信号。以往所有方法在推理时均独立生成图像，而我们提出新的思考：样本能否通过协作方式生成？我们提出分组扩散方法，将注意力机制从单张图像内部的局部区块扩展至跨图像共享，从而实现推理过程中图像的联合去噪，同时学习图像内部与图像间的关联性。我们观察到明显的规模效应——更大的分组规模会产生更强的跨样本注意力，进而提升生成质量。此外，我们引入定性度量方法捕捉这一特性，并证明其强度与FID指标高度相关。基于标准扩散变换器构建的分组扩散模型，在ImageNet-256x256数据集上实现了高达32.2%的FID提升。本研究揭示了跨样本推理作为一种高效且未被探索的生成建模机制的重要价值。 |
| E-RayZer：作为空间视觉预训练的自监督三维重建 | Qitao Zhao | [PDF](https://arxiv.org/pdf/2512.10950v1) | 自监督预训练已在语言、单张二维图像及视频的基础模型领域引发革命，但在从多视角图像中学习三维感知表征方面仍鲜有探索。本文提出E-RayZer——一种直接从无标注图像中学习真正三维感知表征的自监督大型三维视觉模型。与RayZer等通过潜在空间视角合成间接推断三维结构的先前自监督方法不同，E-RayZer直接在三维空间中运行，通过显式几何进行自监督三维重建。这种设计消除了捷径解，并产生几何基础坚实的表征。为确保收敛性与可扩展性，我们提出一种新颖的细粒度学习课程，以完全无监督的方式组织从易到难的训练样本，并协调异构数据源。实验表明，E-RayZer在姿态估计任务上显著优于RayZer，在三维重建任务上达到甚至有时超越VGGT等全监督重建模型的性能。此外，当迁移至三维下游任务时，其学习到的表征优于领先的视觉预训练模型（如DINOv3、CroCo v2、VideoMAE V2和RayZer），确立了E-RayZer作为三维感知视觉预训练的新范式。 |
| 我们是否已为文本到3D生成中的强化学习做好准备？一项渐进式研究 | Yiwen Tang | [PDF](https://arxiv.org/pdf/2512.10949v1) | 强化学习（RL）此前已被证明在大语言模型和多模态模型中具有显著效果，近期已成功扩展至增强二维图像生成领域。然而，由于三维物体具有更高的空间复杂性，需要全局一致的几何结构与细粒度局部纹理，将强化学习应用于三维生成的研究仍处于探索阶段。这使得三维生成对奖励设计与强化学习算法极为敏感。为应对这些挑战，我们首次从多个维度系统研究了基于强化学习的文本到三维自回归生成方法：（1）奖励设计：通过评估奖励维度与模型选择，我们发现与人类偏好对齐至关重要，且通用多模态模型能为三维属性提供稳健的信号反馈；（2）强化学习算法：我们研究了GRPO算法的多种变体，证明词元级优化的有效性，并进一步探索了训练数据与迭代次数的扩展规律；（3）文本到三维基准测试：针对现有基准无法有效衡量三维生成模型隐含推理能力的问题，我们提出了MME-3DR评测体系；（4）先进强化学习范式：受三维生成天然层次性启发，我们提出Hi-GRPO方法，通过定制化奖励组合优化从全局到局部的层次化三维生成。基于这些发现，我们开发了首个强化学习增强的文本到三维模型AR3D-R1，实现了从粗粒度形状到纹理细化的全流程优化。本研究旨在为强化学习驱动的三维生成推理提供新的见解。代码已发布于https://github.com/Ivan-Tang-3D/3DGen-R1。 |
