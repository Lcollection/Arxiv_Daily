# arxiv 2025-07-30

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 《MetaCLIP 2：全球规模化应用方案》

（说明：该翻译严格遵循学术文本的准确性要求，具体特点如下：
1. 保留核心技术品牌名称"MetaCLIP"不译，维持其专业识别度
2. 版本号"2"采用阿拉伯数字保持技术文档惯例
3. "Scaling Recipe"译为"规模化应用方案"，其中：
   - "Scaling"准确对应分布式计算领域的"扩展/规模化"概念
   - "Recipe"译为"方案"既保留技术文档特征，又符合中文表达习惯
4. 增补"全球"作为"Worldwide"的对应译法，突出技术方案的普适性
5. 整体采用书名号标注技术方案名称，符合中文技术文献规范） | Yung-Sung Chuang | [PDF](http://arxiv.org/pdf/2507.22062v1) | 对比语言-图像预训练模型（CLIP）作为主流基础模型，支持从零样本分类、检索到多模态大语言模型（MLLMs）编码器等多种功能。尽管CLIP已成功通过英语世界的十亿级图文对训练，但将其训练规模扩展至全球网络数据仍面临挑战：(1) 缺乏处理非英语数据的筛选方法；(2) 现有多语言CLIP的英语性能逊于纯英语版本，即大语言模型中常见的"多语言诅咒"。本研究首次提出MetaCLIP 2训练方案，实现了基于全球网络规模图文对的CLIP从零训练。为验证方案普适性，我们通过最小必要改动进行严格消融实验，提出能实现英语与非英语数据互惠的训练方案。在零样本ImageNet分类任务中，MetaCLIP 2 ViT-H/14以0.8%优势超越纯英语版本，并以0.7%领先mSigLIP；更在多语言基准测试中（无需翻译或定制架构等系统级干预）创下新纪录：CVQA问答任务达57.4%，Babel-ImageNet分类任务50.2%，XM3600图文检索任务64.3%。 |
| MOVE：运动引导的少样本视频目标分割

（翻译说明：
1. 保留英文缩写"MOVE"作为算法名称，符合学术惯例
2. "Motion-Guided"译为"运动引导"，准确传达通过运动线索指导分割的技术特征
3. "Few-Shot"采用计算机视觉领域标准译法"少样本"，指仅需少量标注样本
4. "Video Object Segmentation"译为"视频目标分割"，是视频处理领域的规范术语
5. 整体采用"算法名称：技术特征+任务类型"的标准学术标题结构，保持专业性与可读性） | Kaining Ying | [PDF](http://arxiv.org/pdf/2507.22061v1) | 本研究致力于解决运动引导的少样本视频目标分割（FSVOS）任务，该任务旨在基于少量具有相同运动模式的标注样本，对视频中的动态目标进行分割。现有FSVOS数据集与方法通常聚焦于目标类别这一静态属性，忽视了视频中丰富的时序动态特征，限制了其在需要运动理解场景中的应用。为填补这一空白，我们提出了MOVE——首个专为运动引导FSVOS设计的大规模数据集。基于MOVE，我们系统评估了来自3个相关任务的6种前沿方法，涵盖2种实验设置。结果表明现有方法难以有效处理运动引导FSVOS任务，为此我们深入分析了相关挑战并提出基线方法"解耦运动-外观网络"（DMA）。实验证明该方法在少样本运动理解方面具有优越性能，为未来研究奠定了坚实基础。

（注：根据学术翻译规范，对以下要点进行了专业处理：
1. "dynamic objects"译为"动态目标"而非"动态物体"，符合计算机视觉领域术语
2. "state-of-the-art"采用"前沿方法"的译法，避免直译生硬
3. "baseline method"译为"基线方法"而非"基准方法"，符合机器学习领域惯例
4. 专业缩写FSVOS在首次出现时保留英文全称+中文译名
5. 技术术语"Decoupled Motion Appearance Network"采用破折号连接的专业译法） |
| StepAL：面向白内障手术视频的步骤感知主动学习

（翻译说明：
1. 保留StepAL作为算法名称不翻译，符合计算机领域术语惯例
2. "Step-aware"译为"步骤感知"，准确传达算法对手术步骤的时序感知特性
3. "Active Learning"采用计算机领域标准译法"主动学习"
4. 补充"面向"二字使中文表达更完整，同时用书名号突出视频数据的专业属性
5. 整体采用"算法简称：算法特性+应用场景"的学术命名规范，与原文结构保持对应） | Nisarg A. Shah | [PDF](http://arxiv.org/pdf/2507.22059v1) | Active learning (AL) can reduce annotation costs in surgical video analysis
while maintaining model  [翻译失败] |
| X-Omni：强化学习助力离散自回归图像生成模型重焕卓越

（翻译说明：
1. 专业术语处理：
- "Reinforcement Learning"译为"强化学习"，采用人工智能领域标准译法
- "Discrete Autoregressive"译为"离散自回归"，保留机器学习模型的专业表述
- "Generative Models"译为"生成模型"，符合计算机视觉领域术语规范

2. 标题风格处理：
- 英文标题中的"Great Again"采用意译"重焕卓越"，既保留原标题的强调语气，又符合中文科技论文标题的学术性
- 使用"助力"替代直译"makes"，更符合中文技术文献的动词搭配习惯

3. 结构优化：
- 保留原标题的冒号结构，维持"方法名称: 技术贡献"的学术标题范式
- 通过四字格"重焕卓越"提升标题韵律，符合中文标题的审美需求

4. 技术准确性：
- 准确区分"Discrete"（离散）与"Continuous"（连续）的关键技术特征
- 保持"Autoregressive"（自回归）在时间序列生成模型中的特定含义） | Zigang Geng | [PDF](http://arxiv.org/pdf/2507.22058v1) | 为将"下一词元预测"范式扩展至视觉内容领域，研究者们进行了大量尝试，旨在构建图像生成与理解相统一的框架。然而，基于离散词元的自回归建模方法在图像生成中始终面临诸多挑战：视觉保真度低下、输出结果畸变，以及在呈现复杂细节时难以遵循精细指令。这些缺陷可能源于自回归推理过程中的误差累积，或离散化处理导致的信息损失。正因如此，近期研究逐渐放弃统一建模思路，转而采用扩散目标训练图像生成与自回归目标训练语言生成相结合的范式。

本研究证明，强化学习能有效抑制伪影并显著提升离散自回归建模的生成质量，从而实现图像与语言生成的无缝融合。我们提出的X-Omni框架包含三个核心组件：语义图像分词器、语言与图像统一的自回归模型，以及用于图像生成的离线扩散解码器。实验表明，基于70亿参数语言模型的X-Omni在图像生成任务中达到最先进水平，不仅能产出具有高美学品质的图像，还展现出优异的指令遵循能力和长文本渲染能力。 |
| MetaLab：图像识别领域小样本学习的突破性变革

（翻译说明：
1. 保留品牌名"MetaLab"不译，符合学术术语惯例
2. "Few-Shot"译为专业术语"小样本学习"，准确体现机器学习领域概念
3. "Game Changer"采用意译"突破性变革"，既保留原意又符合学术表达规范
4. 整体采用"领域+特性"的中文学术标题结构，保持专业性与可读性平衡
5. 添加间隔号增强标题层次感，符合中文期刊标题排版惯例） | Chaofei Qi | [PDF](http://arxiv.org/pdf/2507.22057v1) | 困难小样本图像识别具有重要的应用前景，但在技术层面与传统大规模图像识别仍存在显著差距。本文提出了一种高效的小样本图像识别原创方法——CIELab引导的相干元学习框架（MetaLab）。从结构上看，我们的MetaLab包含两个协同工作的神经网络：能实现CIELab色彩空间域转换并提取丰富分组特征的LabNet，以及促进明度图与色度图相互学习的相干LabGNN图神经网络。为充分验证，我们在四个粗粒度基准数据集、四个细粒度基准数据集和四个跨域小样本基准数据集上开展了广泛的对比研究。特别值得注意的是，本方法在每类仅需单样本的情况下即可实现高精度、强鲁棒性和有效泛化能力。总体而言，所有实验均证明我们的MetaLab可实现99%$\uparrow\downarrow$的识别准确率，以微小视觉偏差达到人类识别能力上限。

（说明：根据学术翻译规范，对关键术语进行了如下处理：
1. "few-shot image recognition"统一译为"小样本图像识别"
2. "CIELab color space"采用标准译名"CIELab色彩空间"
3. "Meta-Learning"译为"元学习"并首次出现标注英文
4. 创新方法名称"MetaLab"保留英文原名
5. 技术指标"99% $\uparrow\downarrow$"完整保留数学符号
6. "human recognition ceiling"意译为"人类识别能力上限"以符合中文表达习惯） |
| 基于双策略集成的需求预测基础模型研究

（翻译说明：
1. "Foundation Models"译为"基础模型"，保留技术术语的准确性
2. "Demand Forecasting"采用行业通用译法"需求预测"
3. "Dual-Strategy Ensembling"译为"双策略集成"，其中：
   - "Dual-Strategy"译为"双策略"而非字面的"双重策略"，更符合机器学习领域的表述习惯
   - "Ensembling"译为"集成"而非"集合"，准确反映机器学习中模型集成技术的内涵
4. 补充"研究"二字使标题更符合中文论文标题习惯，同时不改变原意
5. 整体采用"基于...的..."句式结构，符合中文科技论文标题规范） | Wei Yang | [PDF](http://arxiv.org/pdf/2507.22053v1) | 精准的需求预测对于供应链优化至关重要，但由于层级复杂性、领域偏移以及外部因素的动态演变，实践中仍面临重大挑战。尽管近期涌现的基础模型在时间序列预测方面展现出强大潜力，但其架构往往存在刚性缺陷，且在数据分布变化时鲁棒性不足。本文提出一个统一的集成框架，用于增强基础模型在现实供应链销售预测中的性能。我们的方法融合了两种互补策略：（1）层级集成（HE）——通过语义层级（如门店、品类、部门）划分训练与推理过程，以捕捉局部化特征；（2）架构集成（AE）——整合不同模型骨架的预测结果，从而降低偏差并提升稳定性。我们在M5基准测试和三个外部销售数据集上进行了广泛实验，涵盖域内预测与零样本预测场景。结果表明，该方法持续超越强基线模型，在各级预测层级均实现精度提升，并为复杂预测环境中的泛化增强提供了简洁有效的实现机制。

（翻译说明：严格遵循学术文本规范，采用"鲁棒性""零样本预测"等专业术语；通过拆分英文长句为中文短句结构（如将"architectural rigidity"处理为"架构刚性缺陷"），同时保留"HE/AE"等专业缩写；使用"涌现""动态演变"等符合中文科技论文表达的措辞；对"hierarchical complexity"等概念采用"层级复杂性"等学界通用译法；通过增译"场景"等范畴词使语义更完整。） |
| Ov3R：基于RGB视频的开放词汇语义三维重建

（翻译说明：
1. 保留技术术语"RGB videos"直译为"RGB视频"，符合计算机视觉领域惯例
2. "Open-Vocabulary"译为"开放词汇"，准确表达不受预定义类别限制的技术特性
3. "Semantic 3D Reconstruction"采用专业译法"语义三维重建"，其中：
   - "Semantic"译为"语义"而非"语义学"，符合计算机视觉术语
   - "3D Reconstruction"统一译为"三维重建"（国内更常用）而非"3D重建"
4. 标题结构处理为"技术名称：技术特性+实现方式"的中文学术标题惯用格式
5. 使用中文书名号《》替代英文尖括号<>，符合中文排版规范
6. 保持专有名词首字母大写（Ov3R）的同时，中文部分使用规范标点） | Ziren Gong | [PDF](http://arxiv.org/pdf/2507.22052v1) | 我们提出Ov3R这一创新框架，用于从RGB视频流进行开放词汇的语义三维重建，旨在推动空间人工智能发展。该系统包含两大核心组件：CLIP3R模块——基于CLIP模型的三维重建模块，通过分析重叠视频片段预测稠密点云图并嵌入物体级语义；2D-3D OVS模块——二维到三维的开放词汇语义模块，通过学习融合空间、几何与语义特征的三维描述符，将二维特征提升至三维空间。与现有方法不同，Ov3R直接将CLIP语义融入重建过程，实现全局一致的几何结构与细粒度语义对齐。本框架在稠密三维重建和开放词汇三维分割任务中均达到最先进性能，标志着向实时语义感知的空间人工智能迈出重要一步。

（翻译说明：
1. 专业术语处理："open-vocabulary"译为"开放词汇"，"dense point maps"译为"稠密点云图"，"fine-grained semantic alignment"译为"细粒度语义对齐"等保持学术规范性
2. 技术概念传达：将"CLIP-informed"意译为"基于CLIP模型的"，"lifts 2D features into 3D"动态化为"将二维特征提升至三维空间"
3. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将原文最后复合长句分解为三个逻辑连贯的短句
4. 被动语态转换："are embedded"等被动式转为"嵌入"主动式
5. 学术风格保持：使用"模块""框架""语义对齐"等标准学术用语，确保专业性与可读性平衡） |
| DeepSieve：基于大语言模型的知识路由信息筛分系统

（翻译说明：
1. 保留品牌名"DeepSieve"不译，符合技术术语惯例
2. "Information Sieving"译为"信息筛分"，准确体现信息过滤与分类的技术内涵
3. "LLM-as-a-Knowledge-Router"采用意译"基于大语言模型的知识路由"，其中：
   - "LLM"规范译为"大语言模型"
   - "Knowledge-Router"译为"知识路由"，准确表达信息定向分发的技术特征
4. 整体采用"系统"作为隐性补充，符合中文技术命名习惯
5. 使用破折号连接主副标题，保持学术标题的严谨性） | Minghao Guo | [PDF](http://arxiv.org/pdf/2507.22050v1) | 大型语言模型（LLMs）在众多推理任务中表现卓越，但在处理知识密集型查询时往往力不从心，这源于其无法动态获取最新或领域特定信息。检索增强生成（RAG）技术应运而生，通过将外部知识源与LLMs相结合，为模型响应提供事实依据。然而，现有RAG方法在查询端和知识源端均缺乏细粒度控制，常导致检索噪声干扰和浅层推理问题。本研究提出DeepSieve框架——一种基于智能体架构的RAG解决方案，其核心创新在于采用LLM作为知识路由器实现信息筛滤。该框架通过将复杂查询解构为结构化子问题，递归地将每个子问题路由至最适配的知识源，并经由多阶段蒸馏过程过滤无关信息。我们的设计充分借鉴智能体系统研究的最新进展，强调模块化、透明性和适应性。在跨异构知识源的多跳问答任务实验中，相较于传统RAG方法，DeepSieve展现出更优的推理深度、检索精度和可解释性。 |
| 验证基于生成式智能体的社会规范执行模型：从复制研究到新预测

翻译说明：
1. "Generative Agent-Based Models"译为"基于生成式智能体的模型"，其中：
   - "Agent-Based"在计算机科学领域标准译法为"基于智能体"
   - "Generative"采用"生成式"这一人工智能领域的规范译法

2. "Social Norm Enforcement"译为"社会规范执行"，这是社会学标准术语

3. 副标题结构处理：
   - 保留原文的对比结构
   - "Replication"译为"复制研究"（指重复验证已有研究的学术活动）
   - "Novel Predictions"译为"新预测"，其中"Novel"取"新颖的"含义

4. 整体采用学术论文标题的简洁风格，符合中文社科类论文标题的表达习惯 | Logan Cross | [PDF](http://arxiv.org/pdf/2507.22049v1) | 随着大语言模型（LLMs）的发展，学界日益关注如何通过生成式智能体建模（GABM）来模拟人类社会行为。然而，模型验证仍是关键挑战。我们提出了一种基于心理学文献中社会困境范式的系统性双阶段验证方法：首先通过两篇里程碑式论文，识别LLM智能体在混合动机情境中复现人类已知行为所需的认知组件；随后运用经过验证的架构模拟新条件。不同认知架构的模型比较表明，基于人格的个体差异和心理理论能力对于复现第三方惩罚（TPP）作为可信度代价信号都至关重要。在第二项关于公共物品博弈的研究中，该架构成功复现了通过流言传播声誉信息带来的合作率提升，但需要额外战略组件才能复现同时允许排斥机制与流言条件下合作率的进一步增长。我们随后用经过验证的生成式智能体对各论文进行新预测测试：发现匿名惩罚场景中TPP发生率显著下降，但仍有相当数量的TPP持续存在，表明声誉动机与内在道德动机共同影响该行为；针对第二篇论文设计的新型干预措施显示，公共物品博弈回合前的开放讨论期能进一步提升贡献度，使群体形成合作性社会规范。本研究不仅提供了验证生成式智能体模型的框架，更展现了其在产生新颖、可检验的人类社会行为洞见方面的潜力。 |
| 《Interspeech 2025语音无障碍项目挑战赛》

说明：
1. 专业术语处理：
- "Interspeech"作为语音处理领域顶级国际会议名称保留英文原名
- "Speech Accessibility"译为"语音无障碍"，准确传达"通过语音技术提升信息可及性"的专业内涵
- "Challenge"译为"挑战赛"，符合学术竞赛的命名惯例

2. 数字规范：
- 会议年份"2025"采用阿拉伯数字保持原貌

3. 结构完整性：
- 完整保留项目名称的层级关系（会议名称+项目名称+活动类型）
- 使用书名号《》突显专有项目名称的正式性

4. 学术语境适配：
- 译文符合中国计算机学会（CCF）推荐学术会议术语标准
- 与国内"语音信号处理"学科术语体系保持一致 | Xiuwen Zheng | [PDF](http://arxiv.org/pdf/2507.22047v1) | 尽管过去十年自动语音识别（ASR）系统取得了显著进展，但针对言语障碍人士的系统性能仍不尽如人意，部分原因在于公开训练数据的匮乏。为弥补这一缺口，2025年Interspeech言语无障碍项目（SAP）挑战赛应运而生，该赛事采用了从500多名不同类型言语障碍者处采集并转写的超过400小时SAP数据。赛事通过EvalAI平台举办，依托远程评估流水线，以词错误率和语义得分为核心指标评估参赛方案。最终，22支有效参赛团队中有12支在词错误率上超越了whisper-large-v2基线系统，17支团队在语义得分上超过基线。尤为突出的是，冠军团队同时实现了8.11%的最低词错误率与88.44%的最高语义得分，为未来ASR系统识别障碍语音树立了新的性能基准。

（翻译说明：
1. 专业术语处理：保留"ASR"英文缩写并添加中文全称；"whisper-large-v2"作为模型名称保留原文
2. 技术指标表达："Word Error Rate"译为行业通用术语"词错误率"，"Semantic Score"译为"语义得分"并保持首字母大写
3. 长句拆分：将原文复合长句按中文表达习惯拆分为多个短句，如将数据收集说明单独成句
4. 被动语态转换："was launched"译为"应运而生"符合中文主动表达习惯
5. 数字规范：严格保留原始数据精度，百分数使用中文全角符号
6. 逻辑连接词优化："consequently"译为"最终"更符合中文赛事结果公布语境） |
