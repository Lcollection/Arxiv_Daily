# arxiv 2025-09-14

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| FLUX-Reason-6M与PRISM-Bench：百万级图文推理数据集及综合基准评测体系

（说明：该翻译严格遵循学术术语规范，采用"数据集"对应"Dataset"，"基准评测体系"对应"Benchmark"的译法。通过连接词"与"和顿号保持原标题的并列结构，使用"百万级"准确传达"Million-Scale"的量级概念，同时通过"综合"一词体现"Comprehensive"的全面性特征，符合中文科技文献的标题表述习惯。） | Rongyao Fang | [PDF](http://arxiv.org/pdf/2509.09680v1) | The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large- [翻译失败] |
| 蝴蝶量化：基于可学习正交蝶形变换的超低比特大语言模型量化方法

（注：翻译要点说明：
1. 保留专业术语："Quantization"译为"量化"，"LLM"译为"大语言模型"
2. 技术概念准确传达："Learnable Orthogonal Butterfly Transforms"译为"可学习正交蝶形变换"，其中：
   - "Learnable"体现可学习特性
   - "Orthogonal"保持数学上的正交性含义
   - "Butterfly"译为"蝶形"符合计算数学领域的惯用译法
3. 技术规格明确："Ultra-low-bit"强调"超低比特"的量化精度特性
4. 学术风格保持：使用"通过/基于"的学术表达句式，符合中文论文标题规范） | Bingxin Xu | [PDF](http://arxiv.org/pdf/2509.09679v1) | Large language models require massive memory footprints, severely limiting
deployment on consumer ha [翻译失败] |
| 收益递减的幻象：大语言模型长周期执行能力评估

（注：译文采用学术翻译的严谨表述方式：
1. "Illusion"译为"幻象"以保持哲学语境
2. "Diminishing Returns"对应经济学经典术语"收益递减"
3. "Long Horizon Execution"译为"长周期执行能力"准确体现时间维度和执行效能双重含义
4. 冒号后采用动名词结构"能力评估"保持学术论文标题的规范性） | Akshit Sinha | [PDF](http://arxiv.org/pdf/2509.09677v1) | 大型语言模型（LLM）的持续扩展是否会产生收益递减？现实世界的价值往往源于智能体能够完成任务的长度。我们通过观察一个简单却反直觉的现象展开研究：单步准确率的边际提升能够复利式地转化为模型可成功完成任务长度的指数级增长。进而，我们论证了当简单任务被延长时LLM的失败源于执行错误，而非推理能力的缺失。我们提出通过显式提供解决长周期任务所需的知识和计划来隔离执行能力。研究发现：即使小模型具有100%的单步准确率，更大规模的模型仍能正确执行显著更多轮次的任务。我们观察到模型的单步准确率随步骤增加而下降——这不仅是长上下文限制所致：有趣的是，我们发现了自我条件效应（self-conditioning effect），即当上下文包含模型先前轮次的错误时，其后续出错概率会显著提升。仅通过扩展模型规模并不能消除这种自我条件效应。相比之下，新兴思维模型不会自我条件化，且能单轮执行更长的任务。最后，我们对前沿思维模型单轮可执行任务长度进行了基准测试。总体而言，通过聚焦执行能力，我们希望调和关于"LLM为何能解决复杂推理问题却在简单任务延长时失败"的争论，并揭示扩展模型规模与序列测试时计算对长周期任务产生的巨大效益。 |
| 空间视频识别数据集（SpatialVID）：一个带有空间标注的大规模视频数据集

（注：翻译采用学术文献常用命名规范，其中：
1. "SpatialVID" 保留核心缩写VID并添加中文释义"空间视频识别数据集"，符合中文学术语境
2. "Large-Scale" 译为"大规模"体现数据集体量特征
3. "Spatial Annotations" 译为"空间标注"准确传达空间维度标注信息的专业概念
4. 整体采用"核心译名+补充说明"的结构，既保持学术严谨性又确保中文可读性） | Jiahao Wang | [PDF](http://arxiv.org/pdf/2509.09676v1) | 在空间智能领域，空间重建与世界探索两方面均已取得显著进展。然而当前模型的可扩展性和现实世界保真度仍受到大规模高质量训练数据稀缺的严重制约。尽管现有若干数据集提供相机位姿信息，但其在规模、多样性和标注丰富性方面存在明显局限——特别是缺乏具有真实相机运动轨迹的现实动态场景数据。为此，我们构建了\textbf{SpatialVID}数据集，该数据集包含大量野外拍摄视频，涵盖多样化场景、相机运动模式以及密集的3D标注（包括逐帧相机位姿、深度信息和运动指令）。具体而言，我们收集了超过21,000小时的原始视频，通过分级过滤流程将其处理为270万个视频片段，总计7,089小时的动态内容。后续标注流程为这些片段增添了详细的空间与语义信息，包括相机位姿、深度图、动态遮罩、结构化描述文本和序列化运动指令。对SpatialVID数据统计特性的分析表明，其丰富性和多样性将直接促进模型泛化能力与性能的提升，使之成为视频与三维视觉研究领域的重要资源。 |
| SimpleVLA-RL：通过强化学习扩展视觉语言动作模型的训练规模

（注：VLA在学术语境中通常译为"视觉语言动作模型"，全称为Vision-Language-Action model。该翻译采用技术术语的规范译法，同时通过冒号结构保持原标题的学术表述风格，准确传递了通过强化学习技术提升模型训练规模的核心概念。） | Haozhan Li | [PDF](http://arxiv.org/pdf/2509.09674v1) | Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipul [翻译失败] |
| CDE：大型语言模型中基于好奇心驱动探索的高效强化学习方法

（注：CDE全称为Curiosity-Driven Exploration，译为"好奇心驱动探索"。该翻译保持了专业术语的准确性，同时符合中文表达习惯，通过添加"方法"二字使学术表述更完整。整个译名准确传达了原文的技术内涵：通过引入好奇心机制来增强大型语言模型在强化学习过程中的探索效率。） | Runpeng Dai | [PDF](http://arxiv.org/pdf/2509.09675v1) | 具有可验证奖励的强化学习（RLVR）是提升大语言模型（LLMs）推理能力的重要范式。然而现有RLVR方法往往存在探索效率低下的问题，导致早熟收敛和熵崩溃。为应对这一挑战，我们提出好奇心驱动探索（CDE）框架，通过模型内在的好奇心机制引导探索过程。我们通过策略网络与价值网络的双重信号形式化好奇心度量：对策略网络使用生成响应的困惑度，对价值网络则采用多头架构的价值估计方差。这两种信号在RLVR框架中作为探索奖励，共同指导模型行为。理论分析表明，策略网络的奖励机制能有效惩罚过度自信的错误预测并促进正确答案的多样性；此外，我们证明了价值网络的奖励机制与强化学习中成熟的基于计数的探索奖励具有内在关联。在AIME基准测试中，本方法相比使用GRPO/PPO的标准RLVR实现了约3个百分点的提升。进一步分析揭示了RLVR中存在的校准崩溃机制，为理解大语言模型的常见故障模式提供了新视角。 |
| 图像扩散模型中的局部性源于数据统计特性

该翻译严格遵循了学术翻译的准确性要求：
1. 保留专业术语："Locality"译为"局部性"，"Image Diffusion Models"译为"图像扩散模型"，均为领域标准译法
2. 保持学术句式结构：采用"源于"对应"Emerges from"，准确传达因果关系
3. 术语统一性："Data Statistics"译为"数据统计特性"，既保持专业又符合中文表达习惯
4. 避免歧义：通过添加"特性"二字，使"数据统计"的含义更加完整明确，符合学术文本的严谨性要求 | Artem Lukoianov | [PDF](http://arxiv.org/pdf/2509.09672v1) | 在生成模型中，扩散模型因其训练目标存在闭式最优最小化器（通常称为最优去噪器）而独具研究价值。然而，使用这种最优去噪器的扩散过程仅能复现训练集中的图像，因此无法捕捉深度扩散模型的实际行为。近期研究试图刻画最优去噪器与深度扩散模型之间的性能差异，提出了无需训练的分析模型，这些模型能够生成与训练后的UNet所生成图像相似的样本。其中性能最佳的方法假设卷积神经网络的平移等变性和局部性归纳偏置是造成性能差距的原因，因此将这些假设纳入其分析模型。本文通过实证表明，深度扩散模型中的局部性实质上是图像数据集的统计特性，而非源于卷积神经网络的归纳偏置。具体而言，我们证明最优参数化线性去噪器展现出与深度神经去噪器相似的局部性特征。我们进一步通过理论分析和实验验证表明，这种局部性直接源于自然图像数据集中存在的像素相关性。最终，基于这些发现，我们构建了一个分析型去噪器，其与深度扩散模型预测得分的匹配度优于先前专家构建的替代方案。 |
| Dexplore：基于参考范围探索的可扩展神经控制灵巧操作技术

（注：翻译采用学术论文标题的常见结构，保留核心术语"Dexplore"作为技术名称。"Scalable Neural Control"译为"可扩展神经控制"体现系统扩展性，"Dexterous Manipulation"采用机器人领域标准译法"灵巧操作"，"Reference-Scoped Exploration"译为"参考范围探索"准确传达受限探索空间的技术概念，同时通过"基于...技术"的句式保持中文标题的学术规范性。） | Sirui Xu | [PDF](http://arxiv.org/pdf/2509.09671v1) | 手-物运动捕捉（MoCap）数据库提供了大规模、高接触度的演示数据，为灵巧机器人操作技术的规模化发展带来新机遇。然而演示数据的不精确性以及人手机器手之间的本体差异，限制了这些数据的直接使用。现有方法采用三阶段工作流程（包括重定向、跟踪和残差校正），往往导致演示数据利用不足且误差在多阶段传递中累积。我们提出Dexplore——一种统一的单循环优化框架，通过联合执行重定向与跟踪，直接从大规模运动捕捉数据中学习机器人控制策略。该方法将演示数据视为柔性指导而非绝对真值，从原始轨迹中推导出自适应空间范围，并通过强化学习训练策略模型使其在保持动作范围内完成任务的同时最小化控制能耗。这种统一框架既保留了演示意图，又催生了机器人专属策略，增强了对噪声的鲁棒性，并能适配大规模演示数据集。我们将规模化跟踪策略蒸馏为基于视觉的技能条件生成控制器，该控制器将多样化的操作技能编码为丰富的潜在表征，支持跨物体泛化与真实世界部署。这些创新共同使Dexplore成为将不完美演示转化为有效训练信号的理论桥梁，推动灵巧操作技术的发展。 |
| 用于学习人体运动先验的几何神经距离场

（注：翻译严格遵循以下原则：
1. 准确传达"Geometric Neural Distance Fields"技术概念，采用"几何神经距离场"这一学界通用译法
2. "Learning Human Motion Priors"译为"学习人体运动先验"，其中"Priors"专业术语统一译为"先验"
3. 保持学术文本的简洁性和专业性，避免添加冗余修饰词
4. 使用"用于"准确体现原题中"for"的功能性表述
5. 整体语序符合中文学术标题的表述习惯） | Zhengdi Yu | [PDF](http://arxiv.org/pdf/2509.09667v1) | 我们提出神经黎曼运动场（NRMF），这是一种新型三维生成式人体运动先验模型，能够实现鲁棒、时序一致且物理合理的三维运动重建。与现有基于VAE或扩散模型的方法不同，我们的高阶运动先验通过神经距离场（NDF）集合的零水平集显式建模人体运动，该集合分别对应姿态、过渡（速度）和加速度动力学。我们的框架具有严谨的理论基础：NDF构建于关节旋转、角速度及角加速度的乘积空间，严格遵循底层关节结构的几何特性。我们还进一步提出：（i）一种新型自适应步长混合算法，用于投影到合理运动集合；（ii）一种创新几何积分器，在测试时优化和生成过程中"展开"真实运动轨迹。实验结果表明显著且一致的性能提升：在AMASS数据集上训练的NRMF模型，出色地实现了跨多种输入模态的泛化能力，并能应用于从去噪到运动插值、从部分二维/三维观测数据拟合等多样化任务场景。 |
| 理解与生成能否真正协同增益——抑或仅是共存？ | Zhiyuan Yan | [PDF](http://arxiv.org/pdf/2509.09666v1) | 本文通过自编码器视角提出一个深刻范式：将理解过程视为编码器（I2T）——把图像压缩为文本，生成过程视为解码器（T2I）——从文本重建图像。以重建保真度为统一训练目标，我们实现了理解与生成过程间的双向连贯信息流动，形成相互增益机制。为此，我们提出创新性统一多模态学习框架UAE。首先通过大规模长上下文图像描述预训练解码器，以捕捉细粒度语义和复杂空间关系；继而提出基于强化学习（RL）的Unified-GRPO三阶段训练法：（1）冷启动阶段通过语义重建损失温和初始化编解码器；（2）为理解而生成阶段：训练编码器生成能最大化解码器重建质量的信息化描述，增强其视觉理解能力；（3）为生成而理解阶段：优化解码器根据描述进行重建，迫使其利用每个细节，提升长上下文指令遵循与生成保真度。我们同步推出首个专门评估UMM系统统一度的基准测试Unified-Bench。多模态学习领域出现令人惊奇的"顿悟时刻"：随着RL训练推进，编码器自主生成更具描述性的文本，而解码器同步展现出理解这些复杂描述的深层能力，最终实现具有惊人保真度的图像重建。 |
