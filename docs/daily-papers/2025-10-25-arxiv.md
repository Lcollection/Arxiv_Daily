# arxiv 2025-10-25

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| HoloCine：电影级多镜头长视频叙事的整体生成技术

（注：译文采用"整体生成技术"对应"Holistic Generation"，突出系统性创作理念；"电影级"强化cinematic的专业制作属性；"多镜头长视频叙事"准确传达multi-shot long video narratives的技术特征，同时符合中文影像领域的术语习惯） | Yihao Meng | [PDF](http://arxiv.org/pdf/2510.20822v1) | 当前最先进的文生视频模型擅长生成独立片段，但在构建连贯多镜头叙事方面存在不足——而这正是故事叙述的核心。我们通过HoloCine模型弥合这一"叙事鸿沟"，该模型能够整体生成完整场景，确保从首个镜头到最终画面的全局一致性。我们的架构通过窗口交叉注意力机制将文本提示精准定位到特定镜头，实现精确的导演控制；同时采用稀疏镜头间自注意力模式（镜头内稠密连接，镜头间稀疏连接），确保分钟级生成所需的效率。除在叙事连贯性方面树立新标杆外，HoloCine还展现出显著涌现能力：对角色与场景的持久记忆，以及对电影技法的直观把握。本工作标志着从片段合成到自动化电影制作的关键转变，使端到端的电影创作成为可触及的未来。代码已开源：https://holo-cine.github.io/ |
| 层组合器：基于空间感知分层画布的交互式个性化文本到图像生成

该翻译保留了以下关键要素：
1. "LayerComposer"译为"层组合器"，准确体现图层组合功能
2. "Interactive Personalized T2I"完整译为"交互式个性化文本到图像生成"，其中T2I作为专业术语展开为"文本到图像"
3. "Spatially-Aware Layered Canvas"译为"空间感知分层画布"，精准传达空间定位与图层堆叠的技术特性
4. 整体采用学术文献标准的四字格标题结构，符合中文论文标题规范
5. 通过连接词"基于"建立技术逻辑关系，保持专业表达的连贯性 | Guocheng Gordon Qian | [PDF](http://arxiv.org/pdf/2510.20820v1) | 尽管现有个性化生成模型具有令人印象深刻的视觉保真度，但其缺乏对空间构图的交互控制能力，且在处理多主体场景时扩展性欠佳。为突破这些局限，我们提出LayerComposer——一个支持个性化多主体文生图的交互式框架。本方法包含两大核心创新：（1）分层画布：一种新颖的视觉表征形式，将每个主体置于独立图层，实现无遮挡构图；（2）锁定机制：在保持选定图层高保真度的同时，允许其余图层灵活适应周边语境。与专业图像编辑软件类似，所提出的分层画布支持用户通过直观的图层操作来放置、缩放或锁定输入主体。我们的通用锁定机制无需调整模型架构，而是通过固有位置编码与创新的互补数据采样策略实现。大量实验表明，在多主体个性化图像生成任务中，LayerComposer在空间控制与身份特征保持方面均优于当前最先进方法。 |
| 迈向通用模态转换：基于对比与预测的潜在扩散桥接方法

（解析：该翻译遵循以下原则：
1. 学术术语精准对应：
   - "Modality Translation"译为"模态转换"（非"模式翻译"）
   - "Latent Diffusion"保留专业术语"潜在扩散"
   - "Contrastive and Predictive"译为"对比与预测"

2. 句式结构优化：
   - 将英文介词结构"Towards..."转化为中文动词导向的"迈向..."
   - 使用冒号实现标题分层，符合中文论文标题规范

3. 专业概念完整传递：
   - "Bridge"译为"桥接方法"体现技术手段
   - 保持"潜在扩散"与"模态转换"的技术逻辑关联

4. 中文表达习惯：
   - 采用四字格"对比与预测"增强节奏感
   - 使用"方法"作为隐性中心词，符合中文标题省略倾向） | Nimrod Berman | [PDF](http://arxiv.org/pdf/2510.20819v1) | 生成建模领域的最新进展使扩散模型成为从复杂数据分布中采样的前沿工具。尽管这些模型在图像、音频等单模态领域取得了显著成功，但将其能力扩展到模态翻译——即在不同感官模态间转换信息——仍是一个待解决的挑战。现有方法通常依赖限制性假设，包括共享维度、高斯源先验和模态特定架构，这限制了其普适性与理论根基。本研究提出潜在去噪扩散桥接模型（LDDBM），这是一个基于潜在变量扩展的去噪扩散桥接模型的通用模态翻译框架。通过在共享潜在空间中操作，我们的方法能够学习任意模态间的桥梁，无需对齐维度。我们引入对比对齐损失以增强配对样本间的语义一致性，并设计了适用于潜在空间噪声预测的领域无关编码器-解码器架构。此外，我们提出预测损失以引导训练实现准确的跨域翻译，并探索了多种提升训练稳定性的策略。该方法支持任意模态对组合，在多视图到三维形状生成、图像超分辨率及多视图场景合成等多样化模态翻译任务中表现优异。系统实验与消融研究验证了我们框架的有效性，为通用模态翻译建立了新的强基准。更多信息请访问项目主页：https://sites.google.com/view/lddbm/home。 |
| VAMOS：一种支持能力调制与可控导航的分层视觉-语言-动作模型

（解析说明：
1. VAMOS保留原文大写形式作为专有模型名称
2. Hierarchical译为"分层"准确体现模型架构特性
3. Vision-Language-Action采用连字符保持原文术语结构，译为"视觉-语言-动作"符合人机交互领域表述规范
4. Capability-Modulated译为"能力调制"体现系统可调节性能的技术特征
5. Steerable Navigation译为"可控导航"准确传达可导向、可操控的导航能力
6. 整体采用"定语前置+核心词"的中文学术标题结构，符合中文论文命名惯例） | Mateo Guaman Castro | [PDF](http://arxiv.org/pdf/2510.20818v1) | 机器人导航领域的一个根本性挑战在于：如何学习能够适应多样化环境、同时符合特定实体独特物理约束与能力的策略（例如四足机器人可攀爬楼梯，但轮式机器人无法实现）。我们提出VAMOS——一种分层视觉语言动作模型，其核心创新在于将语义规划与实体基础解耦：通用规划器从多样化的开放世界数据中学习，而专用功能可供性模型则在安全低成本的仿真环境中学习机器人的物理约束与能力。通过精心设计接口实现这种分离，使得高层规划器可直接在图像空间生成候选路径，再由功能可供性模型进行评估与重排序。真实环境实验表明，VAMOS在室内及复杂室外导航任务中的成功率均优于当前最先进的基于模型的方法和端到端学习方法。我们还证明该分层设计能实现腿式与轮式机器人的跨实体导航，并可通过自然语言便捷引导。真实环境消融实验证实，专用模型是实现实体基础的关键，使得单一高层规划器可部署于物理结构迥异的轮式与腿式机器人。最终，该模型显著提升了单机器人可靠性，通过拒绝物理不可行方案使成功率提升至三倍。项目网站：https://vamos-vla.github.io/ |
| KL正则化强化学习旨在解决模型坍塌问题 | Anthony GX-Chen | [PDF](http://arxiv.org/pdf/2510.20817v1) | 普遍认为，优化反向KL散度会导致"模式搜寻"行为，而优化前向KL散度则产生"质量覆盖"效果——若目标是从多个多样化模式中采样，后者通常更受青睐。我们通过数学推导和实证研究表明：这种直觉认知未必适用于基于反向/前向KL正则化的强化学习（例如语言模型中常用的方法）。实际上，反向/前向KL的选择决定了由正则化系数参数化的最优目标分布族。模式覆盖主要取决于其他因素，包括正则化强度、奖励函数与参考概率之间的相对尺度等。进一步研究发现，常用设置（如低正则化强度和等值可验证奖励）往往指定单峰目标分布，这意味着优化目标在构造上就缺乏多样性。基于这些发现，我们构建了一个简洁可扩展且理论完备的算法。该算法对奖励量级进行最小化调整，却能优化目标分布使其在所有高质量采样模式上均保持高概率。实验表明，这一简单改进能有效提升大型语言模型与化学语言模型的后期训练效果，在无需外部多样性信号的情况下，同时增强解决方案的质量与多样性，且在使用前向或反向KL散度单独失效时仍能保持良好性能。 |
| GSWorld：面向机器人操作的闭环逼真仿真套件 | Guangqi Jiang | [PDF](http://arxiv.org/pdf/2510.20813v1) | 本文提出GSWorld——一个结合三维高斯泼溅与物理引擎的机器人操作仿真平台，兼具鲁棒性与照片级真实感。该框架实现了操作策略开发的闭环：通过对真实机器人数据学习策略进行可复现评估，并完成无需真实机器人的仿真到现实策略训练。为实现多样化场景的照片级渲染，我们提出名为GSDF的新型资源格式，将网格表面高斯表示与机器人URDF及其他物体模型相融合。通过标准化重建流程，我们构建了包含3种单臂/双臂操作机器人构型及40余件物体的GSDF数据库。结合物理引擎，我们展示了以下即时应用场景：（1）通过照片级渲染实现零样本仿真到现实的像素-动作映射策略学习；（2）采用自动化高质量DAgger数据采集实现策略对部署环境的自适应；（3）在仿真环境中进行真实机器人操作策略的可复现基准测试；（4）通过虚拟遥操作收集仿真数据；（5）零样本仿真到现实的视觉强化学习。项目网站：https://3dgsworld.github.io/ |
| SpectraMorph：基于结构化隐空间学习的自监督高光谱图像超分辨率方法

（注：采用"结构化隐空间学习"对应"Structured Latent Learning"，既保持机器学习领域的专业表述，又通过"自监督"准确传达Self-Supervised的范式特征；"高光谱图像超分辨率"符合遥感领域的术语规范，整体译名兼顾学术准确性与中文表达习惯） | Ritik Shah | [PDF](http://arxiv.org/pdf/2510.20814v1) | 高光谱传感器能捕获每个像素的密集光谱，但存在空间分辨率低的缺陷，导致边界模糊和混合像元效应。协同配准的辅助传感器（如多光谱、RGB或全色相机）可提供高分辨率空间细节，这推动了通过高光谱与多光谱图像融合（HSI-MSI）实现高光谱超分辨率的研究。现有基于深度学习的方法虽能实现强劲性能，但依赖缺乏可解释性的黑盒回归器，且当多光谱图像波段数极少时往往失效。我们提出SpectraMorph——一个具有结构化潜空间的物理引导自监督融合框架。该框架不采用直接回归，而是强制实施解混瓶颈：从低分辨率高光谱图像中提取端元光谱，通过紧凑型多层感知机从多光谱图像预测类丰度图。最终通过线性混合模型重建光谱，并借助多光谱传感器的光谱响应函数以自监督方式进行训练。SpectraMorph可生成可解释的中间结果，训练时间不足一分钟，即使面对单波段（全色）多光谱图像仍保持稳健性能。在合成与真实数据集上的实验表明，SpectraMorph始终优于最先进的无监督/自监督基线方法，同时与监督基线相比仍保持强劲竞争力。 |
| 小稿大判：基于推测的信息密集型视觉推理 | Yuhan Liu | [PDF](http://arxiv.org/pdf/2510.20812v1) | 大型视觉语言模型（VLM）在多模态理解领域取得了显著进展，但在处理信息密集型图像时仍面临挑战——这类图像密集交织着文本标注与细粒度图形元素。核心难点在于如何精准定位密集布局中的关键线索，并通过多跳推理整合分散的证据。我们提出"推测性裁决"（SV），这一免训练框架受推测解码思想启发，将多个轻量级草案专家与大型裁决模型相结合。在草案阶段，小型VLM作为草案专家生成提供多样化定位候选的推理路径；在裁决阶段，强VLM综合这些路径生成最终答案，在恢复正确答案的同时最小化计算成本。为提升效率与准确性，SV引入共识专家选择机制，仅将高一致性的推理路径提交至裁决阶段。实验表明，SV在具有挑战性的信息密集型和高分辨率视觉问答基准测试（包括InfographicVQA、ChartMuseum、ChartQAPro和HR-Bench 4K）中持续获得性能提升。通过从多个部分准确的推理路径中综合正确见解，相较于大型专有模型或训练流程，SV同时实现了错误修正与成本优化。代码已发布于https://github.com/Tinaliu0123/speculative-verdict |
| 关于LLM生成文本的可检测性探究：究竟何为LLM生成文本？ | Mingmeng Geng | [PDF](http://arxiv.org/pdf/2510.20810v1) | 随着大语言模型（LLM）的广泛使用，众多研究者已将目光投向对其生成文本的检测。然而对于检测目标——即“LLM生成文本”——尚未形成统一精确定义。使用场景的差异性与大语言模型本身的多样性，进一步增加了检测难度。当前普遍认定的检测目标，往往仅代表大语言模型潜在产出文本的一个子集。人类对模型输出的编辑行为，以及大语言模型对使用者产生的潜移默化影响，正在模糊机器生成与人工撰写文本的界限。现有基准测试与评估方法未能充分涵盖检测器实际应用中的多样化条件，导致检测器的数值结果常被误读，其重要意义正在减弱。因此，检测器在特定条件下仍具实用价值，但对其结果的解读应保持审慎，仅可作为参考依据而非决定性指标。 |
| 人工智能、机器人技术及更广阔领域的深度探索研究 | Xueyan Zou | [PDF](http://arxiv.org/pdf/2510.20809v1) | 随着人工智能与机器人学领域研究迅猛发展（年论文量已突破万篇），研究人员保持知识更新变得日益困难。快速演进的技术趋势、跨学科研究的兴起，以及探索本专业外领域的需求共同加剧了这一挑战。为应对这些问题，我们提出一种可推广的分析流程，能够系统审视任何研究领域：识别新兴趋势，发掘跨领域机遇，并为新研究提供具体切入点。本研究提出"真实深度研究"综合框架，将其应用于人工智能与机器人学领域，特别聚焦基础模型与机器人技术进展，同时将分析范围延伸至其他科学领域。主论文详述了RDR流程的构建方法，附录则呈现各分析主题的详尽结果。我们期望这项工作能为人工智能乃至更广泛领域的研究者提供启示。 |
