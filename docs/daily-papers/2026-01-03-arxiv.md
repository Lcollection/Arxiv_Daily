# arxiv 2026-01-03

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 时空导航者：跨时空动态场景的生成式渲染 | Zhening Huang | [PDF](https://arxiv.org/pdf/2512.25075v1) | 我们提出SpaceTimePilot——一种通过解耦时空实现可控生成渲染的视频扩散模型。给定单目视频输入，该模型可在生成过程中独立调整摄像机视角与运动序列，实现跨时空连续自由探索的场景重渲染。为实现这一目标，我们在扩散过程中引入创新的动态时间嵌入机制，使输出视频运动序列能够基于源视频进行显式调控。针对现有数据集中缺乏同一动态场景连续时序变化配对视频的问题，我们提出一种简洁高效的时序扭曲训练方案，通过改造现有多视角数据集模拟时序差异。该策略有效引导模型学习时序控制，实现稳健的时空解耦。为提升双控精度，我们进一步引入两项创新组件：改进的摄像机条件机制支持从首帧开始调整视角，以及首个合成时空全覆盖渲染数据集CamxTime——该数据集提供场景内完全自由的时空视频轨迹。通过时序扭曲方案与CamxTime数据集的联合训练，模型获得了更精确的时序控制能力。我们在真实与合成数据上验证了SpaceTimePilot的性能，相较于现有方法展现出更清晰的时空解耦效果与更优越的生成质量。项目主页：https://zheninghuang.github.io/Space-Time-Pilot/ 代码仓库：https://github.com/ZheningHuang/spacetimepilot |
| GaMO：面向稀疏视图三维重建的几何感知多视角扩散外绘技术 | Yi-Chuan Huang | [PDF](https://arxiv.org/pdf/2512.25073v1) | 三维重建领域的最新进展已通过密集多视角图像实现了高质量场景捕捉的显著突破，但在输入视角有限时仍面临挑战。为应对这一问题，各类方法相继被提出，包括正则化技术、语义先验和几何约束等。基于扩散模型的最新方法通过从新相机位姿生成新视角以扩充训练数据，展现出超越早期正则化及先验技术的显著改进。尽管取得这些进展，我们发现当前前沿方法存在三个关键局限：已知视角外围覆盖不足、生成视角间几何不一致性以及计算流程的高昂成本。本文提出几何感知多视角外绘框架GaMO，通过多视角外绘范式重构稀疏视角重建问题。与生成新视点不同，GaMO从现有相机位姿扩展视场范围，在提供更广阔场景覆盖的同时，本质保持了几何一致性。我们的方法以零样本方式采用多视角条件约束与几何感知去噪策略，无需训练过程。在Replica和ScanNet++数据集上的大量实验表明，该方法在3、6、9个输入视角下均达到最先进的重建质量，在PSNR和LPIPS指标上超越现有方法，同时相比基于扩散模型的SOTA方法实现25倍加速，处理时间低于10分钟。项目主页：https://yichuanh.github.io/GaMO/ |
| Edit3r：基于稀疏无位姿图像的即时三维场景编辑 | Jiageng Liu | [PDF](https://arxiv.org/pdf/2512.25071v1) | 我们提出Edit3r，一种前馈式框架，能够从无位姿、视角不一致且经过指令编辑的图像中单次完成三维场景的重建与编辑。与以往需要逐场景优化的方法不同，Edit3r直接预测符合指令的三维编辑结果，无需优化或位姿估计即可实现快速且逼真的渲染。训练此类模型的核心挑战在于缺乏多视角一致的编辑图像作为监督信号。我们通过以下方法解决该问题：（一）基于SAM2的重新着色策略，生成可靠且跨视角一致的监督数据；（二）非对称输入策略，将重新着色的参考视图与原始辅助视图配对，促使网络融合并对齐不同观测结果。在推理阶段，尽管训练过程中未接触此类编辑数据，我们的模型仍能有效处理由InstructPix2Pix等二维方法编辑的图像。为进行大规模定量评估，我们构建了DL3DV-Edit-Bench基准测试集，该基准基于DL3DV测试集划分，包含20个多样化场景、4种编辑类型，总计100项编辑任务。综合定量与定性结果表明，相较于现有基线方法，Edit3r在语义对齐度和三维一致性方面表现更优，同时推理速度显著提升，使其在实时三维编辑应用中具有广阔前景。 |
| 协调性人形机器人操作与选择策略 | Haozhi Qi | [PDF](https://arxiv.org/pdf/2512.25072v1) | 人形机器人在以人为中心的环境中展现出巨大应用潜力，然而实现头部、手部与腿部的鲁棒全身协调仍面临重大挑战。本研究提出一种结合模块化遥操作界面与可扩展学习框架的系统以解决该问题。我们的遥操作设计将人形机器人控制分解为直观的子模块，包括手眼协调、抓取基元、手臂末端执行器轨迹跟踪及移动控制。这种模块化设计使我们能够高效采集高质量演示数据。

基于此，我们提出"选择策略"——一种通过生成多候选动作并学习评分机制的模仿学习方法。该架构既能实现快速推理，又能有效建模多模态行为。我们在两项实际任务中验证了该方法：洗碗机装载作业与白板擦拭所需的全身移动操控。实验表明，选择策略在性能上显著超越扩散策略与标准行为克隆方法。研究结果进一步揭示，手眼协调能力对长周期任务的成功执行具有关键作用。本工作为在非结构化环境中实现可扩展数据采集与协调性人形机器人操控学习提供了切实可行的技术路径。 |
| 扩展开放式推理以预测未来 | Nikhil Chandak | [PDF](https://arxiv.org/pdf/2512.25070v1) | 高风险决策涉及在不确定的未来情境下进行推理。本研究通过训练语言模型对开放式预测问题作出预判。为扩展训练数据规模，我们基于每日新闻报道的全球事件，采用全自动化精细筛选方案，合成了全新的预测问题集。我们在自建数据集OpenForesight上训练了Qwen3思维模型。为防止训练与评估过程中未来信息泄露，我们采用离线新闻语料库，同时用于数据生成和预测系统的信息检索。通过小规模验证集的引导，我们证明了信息检索的优势以及改进后的强化学习奖励函数的有效性。最终构建的预测系统在2025年5月至8月期间进行了封闭测试。我们的专用模型OpenForecaster 8B达到了与更大规模商业模型相当的性能，其训练过程显著提升了预测的准确性、校准度和一致性。研究发现预测训练带来的校准改进可泛化至多个主流基准测试。我们开源了所有模型、代码及数据，以推动语言模型预测研究的广泛发展。 |
| FineTec：基于骨架分解与序列补全的时序扰动下细粒度动作识别 | Dian Shao | [PDF](https://arxiv.org/pdf/2512.25067v1) | 从时间受损的骨架序列中识别细粒度动作仍是一个重大挑战，尤其是在现实场景中，在线姿态估计常产生大量数据缺失。现有方法往往难以准确恢复时间动态与细粒度空间结构，导致丢失区分相似动作的关键细微运动线索。为此，我们提出FineTec——一个面向时间损坏条件下细粒度动作识别的统一框架。FineTec首先通过多样化时间掩码的上下文感知补全，从受损输入中恢复基础骨架序列；随后通过基于骨架的空间分解模块，将骨架划分为五个语义区域，并根据运动方差进一步划分为动态与静态子组，通过定向扰动生成两个增强骨架序列。这些序列与基础序列共同输入物理驱动估计模块，该模块利用拉格朗日动力学估算关节加速度。最后，融合骨架位置序列与融合加速度序列被共同输入基于图卷积网络的动作识别头。在粗粒度（NTU-60、NTU-120）与细粒度（Gym99、Gym288）基准上的大量实验表明，FineTec在不同程度时间损坏条件下均显著优于现有方法。具体而言，在极具挑战性的Gym99-severe和Gym288-severe设定下，FineTec分别达到89.1%与78.1%的Top-1准确率，证明了其鲁棒性与泛化能力。代码与数据集详见https://smartdianlab.github.io/projects-FineTec/。 |
| 从图像修复到编辑：一种面向上下文丰富视觉配音的自引导框架 | Xu He | [PDF](https://arxiv.org/pdf/2512.25066v1) | 音频驱动的视觉配音旨在将视频的口型动作与新的语音同步，但其根本挑战在于缺乏理想的训练数据：即需要成对的视频，其中仅人物的口型动作存在差异，而所有其他视觉条件完全一致。现有方法通过基于掩码的修复范式规避此问题，即利用不完整的视觉条件迫使模型同时生成缺失内容并同步口型，这会导致视觉伪影、身份特征漂移及同步效果不佳。本研究提出一种新颖的自引导框架，将视觉配音从一个不适定的修复任务重构为良条件的视频到视频编辑问题。我们的方法采用扩散变换器，首先作为数据生成器，合成理想的训练数据：为每个真实样本生成一个口型调整的伴生视频，从而形成视觉对齐的视频对。随后，基于扩散变换器的音频驱动编辑器在这些视频对上端到端地进行训练，利用完整且对齐的输入视频帧，专注于实现精确的音频驱动口型修改。这种完整且帧对齐的输入条件为编辑器提供了丰富的视觉上下文，使其能够获取完整的身份特征线索、场景交互信息以及连续的时空动态。利用这一丰富上下文，我们的方法从根本上实现了高度准确的口型同步、忠实于原视频的身份保持，并在复杂真实场景中展现出卓越的鲁棒性。我们进一步引入时间步自适应的多阶段学习策略作为必要组件，以解耦扩散时间步中相互冲突的编辑目标，从而促进稳定训练，并提升口型同步效果与视觉保真度。此外，我们提出了ContextDubBench，这是一个用于在多样化且具有挑战性的实际应用场景中进行鲁棒评估的综合基准数据集。 |
| 火神：通过大语言模型驱动搜索实现实例最优系统启发式方法 | Rohit Dwivedula | [PDF](https://arxiv.org/pdf/2512.25065v1) | 在现代操作系统与分布式系统中，资源管理任务（如调度、缓存或主动队列管理等）仍主要依赖于人工设计的启发式算法。由于硬件、工作负载及运行环境持续变化，设计高性能启发式算法成为一个昂贵且耗时的过程，我们不得不反复经历这一循环。

我们提出一种全新替代方案：利用代码生成型大语言模型（LLMs）合成实例最优启发式算法——这些算法将专门针对实际部署的具体工作负载和硬件环境进行定制。为实现这一目标，Vulcan系统通过LLM友好型、任务无关的接口实现策略与机制的分离。借助这些接口，用户可以指定目标策略的输入参数与优化目标，而Vulcan则通过基于LLM生成代码的进化搜索来寻找高性能策略。该接口既具备足够表达能力以涵盖各类系统策略，又保持适度约束性，使得即使是小型、低成本的大语言模型也能生成正确且可执行的代码。

我们运用Vulcan合成了缓存淘汰与内存分层场景的高性能启发式算法，实验表明：相较于所有人工设计的最先进算法，这些合成算法在两项任务中的性能分别提升最高达69%和7.9%。 |
| 一模型衍生多智：面向群体智能的贝叶斯变换器 | Diji Yang | [PDF](https://arxiv.org/pdf/2512.25063v1) | 尽管规模庞大且成效显著，现代Transformer模型几乎普遍被训练为单一思维系统：优化过程仅产生一组确定性参数，代表着对数据的单一功能假设。受"智能源于多元思维"理念的启发，我们提出群体贝叶斯Transformer（B-Trans），将标准大语言模型转化为贝叶斯Transformer模型，使其能够从单组预训练权重中采样生成多样且连贯的模型实例。

B-Trans通过将归一化层中类偏置的偏移量视为具有高斯变分近似的随机变量，引入贝叶斯启发的后验代理，从而在不训练完整贝叶斯神经网络的情况下诱导出模型行为的概率分布。从该代理分布中采样可获得行为多样且保持通用能力的模型实例集合。为保持每次生成过程中的内在连贯性，我们在序列层级固定采样噪声，确保跨token的时间一致性。B-Trans支持群体级决策机制，通过聚合多个采样个体的预测结果显著增强探索能力。在零样本生成、可验证奖励的强化学习（RLVR）以及无显式标签的强化学习等实验场景中，B-Trans有效发挥了群体智慧优势，在提升任务性能的同时，相比确定性基线模型产生了更优越的语义多样性。 |
| 关于表示的几何与拓扑：模加法的流形结构 | Gabriela Moisescu-Pareja | [PDF](https://arxiv.org/pdf/2512.25060v1) | 与均匀注意力或可学习注意力架构相关的“时钟与披萨”解释，最初被提出是为了论证不同架构设计能够为模加运算构建不同的计算回路。本研究表明事实并非如此：无论是均匀注意力架构还是可训练注意力架构，都通过拓扑结构与几何性质完全等价的表征方式实现了相同的算法。我们的研究方法突破了对单个神经元及权重的解释层面，转而识别每个习得表征对应的全部神经元，并将这些神经元集合作为整体进行研究。这种方法揭示出每个习得表征本质上是一个流形，使我们能够运用拓扑学工具进行分析。基于这一发现，我们可以对数百个计算回路中的习得表征进行统计分析，从而证明在常见深度学习范式中自然涌现的模加运算回路具有高度相似性。 |
