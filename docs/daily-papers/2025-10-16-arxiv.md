# arxiv 2025-10-16

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| PhysMaster：通过强化学习掌握视频生成中的物理表征 | Sihui Ji | [PDF](http://arxiv.org/pdf/2510.13809v1) | 当前视频生成模型虽能生成视觉逼真的视频，却常违背物理规律，这限制了其生成符合物理规律视频的能力，也难以充当"世界模型"。为解决该问题，我们提出PhysMaster框架，通过提取物理知识作为表征来引导视频生成模型增强物理感知能力。该框架基于图像到视频生成任务，要求模型从输入图像中预测符合物理规律的运动动态。由于输入图像蕴含物体相对位置及潜在交互等物理先验，我们设计PhysEncoder编码器从中提取物理信息作为附加条件，将物理知识注入视频生成过程。针对模型仅关注外观而缺乏物理性能监督的问题，PhysEncoder采用基于人类反馈的强化学习进行物理表征学习，利用生成模型的反馈信号，通过直接偏好优化（DPO）以端到端方式优化物理表征。PhysMaster通过简单代理任务验证了其提升物理感知的有效性，并证明其可泛化至广泛物理场景。这表明我们提出的框架——通过强化学习范式下的表征学习统一多种物理过程解决方案——能够作为物理感知视频生成及更广泛应用的通用即插即用解决方案。 |
| VisCoP：面向视觉语言模型的视频领域自适应视觉探测方法 | Dominick Reilly | [PDF](http://arxiv.org/pdf/2510.13808v1) | 大规模视觉语言模型（VLMs）在通用视觉推理任务中表现卓越，但当应用于与预训练数据存在显著分布差异的新领域时，其性能会出现急剧下降。现有的领域自适应方法通过对不同VLM组件进行微调来实现适应，但这往往导致领域特异性特征学习受限或原有能力的灾难性遗忘。为解决这些问题，我们提出视觉情境化探测（VisCoP）方法，通过在视觉编码器中注入一组紧凑的可学习视觉探针来增强模型能力。这些探针能够以最小程度的参数调整实现高效的领域自适应。我们在三个具有挑战性的领域自适应场景中评估VisCoP——跨视角（从外中心视角到自我中心视角）、跨模态（从RGB到深度信息）以及跨任务（从人类理解到机器人控制）。实验表明，VisCoP始终优于现有自适应策略，在目标领域取得卓越性能的同时，有效保留源领域知识。 |
| 生成式通用验证器作为多模态元推理器 | Xinchen Zhang | [PDF](http://arxiv.org/pdf/2510.13804v1) | 我们提出生成式通用验证器这一创新概念及插件，专为视觉语言模型与统一多模态模型的下一代多模态推理而设计，在推理与生成过程中提供对视觉结果进行反思与优化的核心能力。本研究作出三项主要贡献：（1）构建ViVerBench综合评测基准，涵盖16类关键任务以评估多模态推理中的视觉输出质量。实验表明现有视觉语言模型在这些任务中持续表现不佳，凸显出与人类可靠视觉验证能力之间的显著差距。（2）设计两条自动化流水线构建大规模视觉验证数据，训练出首个全能型生成验证器OmniVerifier-7B。该模型在通用视觉验证任务中取得显著突破，在ViVerBench上提升8.3个指标点。通过训练过程，我们识别出视觉验证的三项原子能力，并揭示其协同泛化机制。（3）提出OmniVerifier-TTS序列化测试时缩放范式，利用通用验证器在统一模型内桥接图像生成与编辑，通过迭代式细粒度优化提升生成能力上限。除生成任务外，我们将通用验证器扩展至更广泛的世界模型交错推理场景。实证研究表明，OmniVerifier-TTS在T2I-ReasonBench（+3.7）和GenEval++（+4.3）上实现提升，优于Best-of-N等现有并行测试时缩放方法。通过为多模态推理注入可靠视觉验证能力，OmniVerifier同时推进了生成过程中的可靠反思与可扩展测试时优化，标志着向更可信、可控的下一代推理系统迈出关键一步。 |
| 追踪万物：通过轨迹场实现任意视频的四维表示 | Xinhang Liu | [PDF](http://arxiv.org/pdf/2510.13802v1) | 有效的时空表征是视频动态建模、理解与预测的基础。作为视频的基本原子单元，像素随时间推移形成连续的三维轨迹，构成动态特性的原始要素。基于这一原理，我们提出将任意视频表示为轨迹场：一种稠密映射，为每一帧中的每个像素分配随时间变化的连续三维轨迹函数。基于此表征，我们推出Trace Anything神经网络，通过单次前向传播即可预测完整轨迹场。具体而言，对于每帧中的每个像素，我们的模型预测一组参数化轨迹（即B样条曲线）的控制点，从而可在任意查询时刻获取其三维位置。我们在大规模4D数据（包括来自我们新平台的数据）上训练了Trace Anything模型，实验表明：（i）在我们的轨迹场估计新基准测试中达到最先进性能，并在现有点追踪基准测试中表现优异；（ii）凭借单次前向范式，无需迭代优化或辅助估计器即可实现显著效率提升；（iii）展现出包括目标条件操控、运动预测和时空融合在内的涌现能力。项目页面：https://trace-anything.github.io/ |
| 通过世界基础实现空间推理 | Yiming Chen | [PDF](http://arxiv.org/pdf/2510.13800v1) | 本文提出三维视觉定位是空间推理的基石，并介绍Grounded-Spatial Reasoner（GS-Reasoner）模型，旨在探索能有效弥合二者鸿沟的空间表征方法。现有三维大语言模型因缺乏能同时捕获语义与几何信息的统一表征而存在局限，这种缺陷既表现为定位性能不佳，又体现在过度依赖外部模块，最终阻碍定位与空间推理的无缝集成。为此，我们提出一种简洁高效的双路径池化机制，通过紧密对齐几何特征与语义位置线索，构建基于图像块的三维统一表征。该表征在不增加输入令牌数量的前提下封装所有关键信息。基于这一整体表征，GS-Reasoner成为首个完全无需外部模块即可实现自回归定位的三维大语言模型，其性能媲美最先进模型，建立了统一自洽的三维空间推理框架。为进一步联通定位与空间推理，我们提出Grounded Chain-of-Thought（GCoT）数据集。该数据集精心标注了推理问题所涉对象的三维边界框，并构建了将定位作为核心推理步骤的思维链。大量实验表明，GS-Reasoner在三维视觉定位任务中取得显著成果，并由此大幅提升空间推理能力，最终达成最先进的性能表现。 |
| BRIEF-Pro：基于短序列至长序列合成的通用上下文压缩技术——实现快速精准的多跳推理

（解析：标题翻译采用学术术语精准对应原则：
1. "Universal Context Compression"译为"通用上下文压缩技术"，明确技术属性
2. "Short-to-Long Synthesis"译为"短序列至长序列合成"，保持计算机领域术语规范
3. "Multi-Hop Reasoning"译为"多跳推理"，符合知识图谱与逻辑推理领域的标准译法
4. 通过破折号衔接技术特性与目标，符合中文标题表述习惯
5. 保留原标题中"BRIEF-Pro"专有名词不译，维持技术识别度） | Jia-Chen Gu | [PDF](http://arxiv.org/pdf/2510.13799v1) | 随着检索增强生成（RAG）处理复杂任务时，持续扩展的上下文虽能提供更丰富信息，却会带来更高延迟并加重模型认知负荷。为缓解这一瓶颈——尤其针对复杂的多跳问题，我们推出BRIEF-Pro通用轻量化压缩器。该工具能够从检索文档中提炼出与查询相关的证据，生成精要摘要并无缝集成至上下文RAG中。通过使用较短上下文（少于1000词）构成的种子数据，BRIEF-Pro经训练后可在广泛场景中对超过1万词的扩展上下文执行抽象压缩。此外，用户可通过指定所需句子数量灵活控制摘要长度。在四个开放域多跳问答数据集上的实验表明，BRIEF-Pro生成的摘要更简洁相关，显著提升了小型、大型及专有语言模型的性能。当配合700亿参数阅读器模型时，BRIEF-Pro实现32倍压缩率，相较LongLLMLingua的9倍压缩率，其问答性能平均提升4.67%，而计算开销仅需后者的23%。 |
| 面包屑推理：基于压缩信标的高效内存推理 | Giovanni Monea | [PDF](http://arxiv.org/pdf/2510.13797v1) | 大型语言模型在长上下文推理中的可扩展性，正受到Transformer键值缓存线性增长的严重制约——这种线性增长会带来巨大的内存与计算开销。我们提出一个核心观点：随着模型生成推理标记，已生成的历史标记信息价值会逐渐衰减，这为压缩创造了条件。本研究提出采用经过训练的特殊标记对生成过程中的键值缓存进行周期性压缩，并清除已完成压缩的条目。我们通过改进的联合蒸馏与强化学习框架训练模型执行压缩操作，该训练方法利用强化学习的输出进行蒸馏，从而将传统强化学习过程的额外开销降至最低。实验结果表明，与未采用缓存压缩的模型及免训练的压缩技术相比，我们的方法在内存效率与准确率的帕累托边界上实现了更优表现。 |
| 语言模型中符号赋义的机制性涌现 | Shuyu Wu | [PDF](http://arxiv.org/pdf/2510.13796v1) | 符号接地（Harnad, 1990）阐述了词语等符号如何通过与现实世界的感知运动经验相连接而获得意义。近期研究初步表明，在没有显式设定接地目标的情况下，大规模训练的（视觉）语言模型可能自发产生接地现象。然而，这种涌现现象的具体发生位置及其驱动机制仍亟待探索。为解决该问题，我们提出受控评估框架，通过机制性与因果性分析系统追踪符号接地在内部计算过程中的形成路径。研究发现：接地现象集中显现在中层计算中，通过注意力头的聚合机制实现——即注意力头整合环境基础以支撑语言形式的预测。该现象在多模态对话及不同架构（Transformer与状态空间模型）中均能复现，但在单向LSTM中并未出现。我们的研究从行为表现与机制原理两个维度证明，符号接地可在语言模型中自发涌现，这对预测并潜在控制生成结果的可靠性具有实践意义。 |
| 蜜蜂：一个高质量语料库与全栈套件，助力解锁先进的完全开源多模态大语言模型

（翻译说明：
1. 保留核心术语"Corpus"译为"语料库"、"Full-Stack Suite"译为"全栈套件"、"MLLMs"采用通用译法"多模态大语言模型"
2. "Fully Open"译为"完全开源"以体现开放获取特性
3. "Unlock Advanced"译为"助力解锁先进"既保持技术含义又符合中文表达习惯
4. 整体采用学术翻译的严谨风格，通过冒号分层清晰呈现项目名称与核心功能） | Yi Zhang | [PDF](http://arxiv.org/pdf/2510.13795v1) | 当前完全开源的多模态大语言模型（MLLMs）在性能上仍落后于专有模型，其主要原因在于监督微调（SFT）阶段存在显著的数据质量差距。现有开源数据集普遍存在大量噪声，且严重缺乏复杂推理数据（如思维链CoT），这制约了高级模型能力的开发。针对这些挑战，本研究提出三大核心贡献：首先，我们推出Honey-Data-15M——一个经过多重清洗技术处理、并采用新颖双层级（短链与长链）CoT增强策略的SFT数据集，包含约1500万问答对；其次，我们提出数据治理流程HoneyPipe及其底层框架DataStudio，为学界提供超越静态数据集发布的透明可适配数据治理方案；最后，为验证数据集与流程效能，我们基于Honey-Data-15M训练了80亿参数的Bee-8B模型。实验表明，Bee-8B在完全开源MLLMs中确立了最新标杆，其性能不仅媲美InternVL3.5-8B等半开源模型，部分场景更实现反超。本研究向社区贡献了完整基础资源套件，包括：Honey-Data-15M语料库、HoneyPipe与DataStudio全栈工具集、训练方案、评估体系及模型权重。这项工作证明，通过系统化提升数据质量，完全开源MLLMs完全具备与半开源模型抗衡的核心竞争力。 |
| MimicKit：面向运动模仿与控制任务的强化学习框架

（注：采用学术翻译的严谨性原则，保留核心术语"MimicKit"的英文原名，通过冒号分隔主副标题。"Reinforcement Learning Framework"译为"强化学习框架"以符合计算机学科规范，"Motion Imitation and Control"采用四字格结构译为"运动模仿与控制"，并通过增译"任务"二字完善专业语境下的语义完整性。） | Xue Bin Peng | [PDF](http://arxiv.org/pdf/2510.13794v1) | MimicKit是一个基于动作模仿与强化学习的运动控制器训练开源框架。该代码库提供了常用的动作模仿技术与强化学习算法的实现。该框架通过提供统一的训练架构，以及标准化的环境、智能体与数据结构，旨在支持计算机图形学与机器人领域的研究与应用。代码库采用模块化设计且易于配置，能够便捷地修改和扩展至新角色与新任务。开源代码库地址：https://github.com/xbpeng/MimicKit。 |
