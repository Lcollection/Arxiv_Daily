# arxiv 2025-10-30

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| VFXMaster：基于情境学习的动态视觉特效生成技术突破

（解析：1. "Unlocking"译为"突破"更符合中文技术文献表述习惯，体现技术攻关含义；2. "Dynamic Visual Effect Generation"采用影视工业标准译法"动态视觉特效生成"；3. "In-Context Learning"遵循机器学习领域规范译法"情境学习"；4. 整体采用"主标题：副标题"的学术论文标题结构，通过冒号衔接实现术语准确性与表达流畅性的平衡） | Baolu Li | [PDF](http://arxiv.org/pdf/2510.25772v1) | 视觉特效（VFX）对数字媒体的表现力至关重要，但其创作仍是生成式人工智能面临的重大挑战。主流方法通常遵循"单一特效对应单一LoRA"的范式，这种模式不仅资源消耗大，且本质上无法泛化至未见过的新特效，从而限制了可扩展性与创作自由度。为应对这一挑战，我们提出VFXMaster——首个基于参考视频的统一化视觉特效生成框架。该框架将特效生成重构为上下文学习任务，能够将参考视频中的多样化动态特效迁移至目标内容。尤为突出的是，该方法对未见过的特效类别展现出卓越的泛化能力。

具体而言，我们设计了上下文条件策略，通过参考样本对模型进行提示。创新性地引入上下文注意力掩码机制，精准解耦并注入核心特效属性，使单一统一模型在避免信息泄露的前提下掌握特效模仿能力。此外，我们提出高效的单样本特效自适应机制，可基于用户提供的单段视频快速提升对复杂未知特效的泛化能力。大量实验表明，我们的方法能有效模仿多类别特效信息，并对领域外特效表现出卓越的泛化性能。为促进后续研究，我们将向社区公开代码、模型及完整数据集。 |
| Gaperon：一款多语言英法生成式语言模型套件

（解析："Gaperon"作为专有名词采用音译保留原称；"Peppered"在计算语言学语境中引申为"多语言特征交织"之意，通过"多语言"实现概念转译；"Generative Language Model Suite"采用计算机领域标准译法"生成式语言模型套件"，完整保留技术术语的准确性。） | Nathan Godey | [PDF](http://arxiv.org/pdf/2510.25771v1) | 我们正式发布Gaperon——一套完全开源的法语-英语-代码语言模型系列，旨在推动大规模模型训练的透明度与可复现性。该系列包含15亿、80亿及240亿参数规模的模型，基于2-4万亿训练标记进行训练，并完整开放训练流程的所有要素：通过神经质量分类器筛选的法英双语数据集、高效的数据整理与训练框架，以及数百个训练中间检查点。通过本项研究，我们系统探讨了数据过滤与测试集污染如何共同影响模型在基准测试和文本生成中的表现。研究发现：基于语言质量的过滤能提升文本流畅度与连贯性，但会导致基准测试成绩欠佳；而后期刻意污染（在包含测试集的混合数据上继续训练）既可恢复具有竞争力的基准分数，又仅会适度损害生成质量。我们同时论证了常规神经过滤方法可能无意间加剧基准泄露的问题。为支持深度研究，我们在预训练阶段引入了无害数据投毒机制，为安全研究提供真实测试场景。通过全面开放模型、数据集、代码及检查点，Gaperon为探索多语言模型开发中数据整理、性能评估、安全防护与开源共享之间的平衡关系建立了可复现的研究基础。 |
| 生成模型输出（不）正确性评估的E分数体系

（注：括号处理采用中文"（不）"结构实现双义表达，既保留原文"(In)Correctness"的辩证含义，又符合中文技术文献表述规范。专业术语"E-Scores"译为"E分数体系"以体现系统性评估特征，"Generative Model Outputs"采用行业通用译法"生成模型输出"。） | Guneet S. Dhillon | [PDF](http://arxiv.org/pdf/2510.25770v1) | 尽管生成模型（尤其是大语言模型）在当今世界已无处不在，但评估其正确性的规范机制仍十分有限。基于共形预测框架，已有研究通过构建大语言模型响应集合，将包含错误响应的概率控制在用户预设的容错水平内。然而，由于这些方法基于p值，易受p值操纵影响——即事后选择容错水平可能导致统计保证失效。为此，我们引入e值作为补充生成模型输出的工具，以e分数量化错误程度。在保持原有统计保证的同时，e分数赋予用户观察分数后自适应调整容错水平的灵活性，通过上界控制被称为规模扭曲的事后错误度量。我们通过实验验证了该方法在评估大语言模型输出正确性方面的有效性，涵盖数学事实准确性与属性约束满足度两种正确性类型。 |
| 神经随机流：SDE求解器的无建模与推断方法

（注：该翻译在保持专业术语准确性的基础上，采用"神经随机流"对应"Neural Stochastic Flows"，"SDE"作为随机微分方程标准缩写保留；通过"无建模与推断方法"准确传达"Solver-Free Modelling and Inference"的技术内涵，体现无需传统数值求解器的核心创新点；整体句式符合中文科技文献表达规范。） | Naoki Kiyohara | [PDF](http://arxiv.org/pdf/2510.25769v1) | 随机微分方程（SDE）非常适合模拟金融、物理和机器学习领域中存在的噪声干扰且采样不规则的时序数据。传统方法需要依赖昂贵的数值求解器在任意时间点之间进行采样。我们提出的神经随机流（NSF）及其潜在变量变体，通过采用具有架构约束的条件归一化流，直接学习（潜在）随机微分方程的转移规律，同时保持从随机流继承的特性。这种方法支持在任意状态间进行单次采样，并在较大时间间隔下实现高达两个数量级的加速。在合成随机微分方程模拟及现实世界追踪与视频数据上的实验表明，神经随机流在保持与数值方法相当的分布精度的同时，显著降低了任意时间点采样的计算成本。 |
| 语言模型事后归因的分解增强训练

（注：该翻译采用学术文献标准译法：
- "Decomposition-Enhanced Training" 译为"分解增强训练"，体现通过分解方法强化训练过程的技术特征
- "Post-Hoc Attributions" 译为"事后归因"，符合机器学习可解释性研究领域的术语规范
- 整体结构保持"方法+应用对象"的学术标题特征） | Sriram Balasubramaniam | [PDF](http://arxiv.org/pdf/2510.25766v1) | 大语言模型（LLMs）在长文档问答任务中的应用日益广泛，此时对信息来源的可靠归因成为建立信任的关键。现有的事后归因方法在抽取式问答场景表现良好，但在多跳推理、抽象概括及半抽取式场景中表现欠佳——这些场景的答案需要跨段落信息融合。为解决这一难题，我们提出将事后归因重新定义为推理问题：将答案解构为若干构成单元，每个单元与特定上下文绑定。我们首先通过实验证明，引导模型在生成归因时同步输出解构内容能有效提升性能。基于此，我们提出DecompTune后训练方法，指导模型将答案解构作为中间推理步骤。通过构建包含复杂问答任务的多样化数据集（由强LLM标注解构信息），我们采用两阶段SFT+GRPO流程与定制化奖励机制，对Qwen2.5（7B/14B）进行后训练。大量实验与消融研究表明，DecompTune显著提升归因质量，不仅超越现有方法，更达到或优于前沿巨型模型的表现水平。 |
| FreeArt3D：基于3D扩散模型的无训练铰接物体生成方法

（注：翻译采用学术文献标题的典型结构，通过冒号分隔主副标题。"Training-Free"译为"无训练"以突出无需训练过程的技术特性，"Articulated Object Generation"译为"铰接物体生成"准确传达可活动部件的三维物体生成概念，"3D Diffusion"保留技术核心术语译为"3D扩散模型"以保持专业性。） | Chuhao Chen | [PDF](http://arxiv.org/pdf/2510.25765v1) | 关节化三维物体是机器人技术、增强现实/虚拟现实及动画领域的核心要素。当前建模方法主要存在两类局限：基于优化的重建流程需依赖密集视角监督数据，而前馈生成模型则仅能输出粗糙几何近似且常忽略表面纹理。相比之下，静态物体的开放世界三维生成已取得显著突破，特别是随着Trellis等原生三维扩散模型的出现。然而，通过训练原生三维扩散模型将其扩展至关节化物体建模仍面临重大挑战。本研究提出FreeArt3D——一个无需训练的关节化三维物体生成框架。该框架无需在有限关节数据上训练新模型，而是将预训练的静态三维扩散模型（如Trellis）重构为强大的形状先验，通过将关节运动视为新增生成维度，将分数蒸馏采样（SDS）扩展至三维到四维领域。给定不同关节状态下拍摄的少量图像，FreeArt3D可联合优化物体的几何形态、表面纹理及关节参数，且无需任务特定训练或大规模关节数据集支持。本方法不仅能生成高保真几何结构与纹理，精准预测底层运动学结构，还能在多样化物体类别中展现良好泛化能力。尽管采用单实例优化范式，FreeArt3D可在数分钟内完成计算，在生成质量与多功能性方面均显著超越现有最优方法。 |
| DiagramEval：基于图结构的LLM生成图表评估方法

（注：采用"图表"对应"Diagram"以符合中文计算机领域术语习惯；"Graphs"译为"图结构"以体现通过拓扑关系进行评估的技术特征；副标题结构保持学术论文标题的简洁性，同时准确传达通过图论方法评估大语言模型生成图表的核心思想） | Chumeng Liang | [PDF](http://arxiv.org/pdf/2510.25761v1) | 图表在研究论文中承担着传递思想的核心作用，但其创建过程素以复杂繁琐著称。尽管图表以图像形式呈现，标准图像生成模型却难以生成结构清晰的图表。我们认为具有前景的方向是直接以SVG文本格式生成演示图表，这能够充分利用大语言模型的最新进展。然而，由于图表组件的复杂性和多模态特性，目前仍缺乏足够判别力与可解释性的指标来评估大语言模型生成的图表质量。本文提出DiagramEval——一种专为评估大语言模型生成演示图表而设计的新型评估指标。该方案将图表抽象为图结构，将文本元素视作节点，其关联关系作为有向边，并通过两组创新指标进行评估：节点对齐度与路径对齐度。我们首次对前沿大语言模型基于最新研究文献生成的图表进行系统评估，定量验证了所提指标的有效性。此外，我们还展示了增强的可解释性如何为理解大语言模型生成图表的特性提供重要洞见。代码地址：https://github.com/ulab-uiuc/diagram-eval。 |
| 大模型时代的多模态空间推理：综述与基准评测 | Xu Zheng | [PDF](http://arxiv.org/pdf/2510.25760v1) | 人类拥有通过视觉、听觉等多模态观察来理解空间的空间推理能力。大型多模态推理模型通过学习感知与推理扩展了这些能力，在各类空间任务中展现出卓越性能。然而针对这些模型的系统性综述与公开基准测试仍较为有限。本综述从多模态大语言模型（MLLMs）的最新进展切入，系统梳理多模态空间推理任务，并建立开放式评估基准。首先概述通用空间推理方法，重点分析后训练技术、可解释性及模型架构；突破传统二维任务范畴，深入探讨空间关系推理、场景与布局理解、三维空间中的视觉问答与定位；进一步评述具身智能领域的进展，包括视觉语言导航与行动模型；同时关注音频、第一视角视频等新兴模态如何通过新型传感器推动空间理解创新。本综述为蓬勃发展的多模态空间推理领域奠定了坚实基础并提供了前瞻视角。相关最新资料、代码及开放基准测试实现可通过以下链接获取：https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning。 |
| 合成数据揭示相关多示例学习中的泛化差距 | Ethan Harvey | [PDF](http://arxiv.org/pdf/2510.25759v1) | 多示例学习（MIL）在医学影像分析中常被用于处理高分辨率二维图像的局部区块或三维体数据的切片进行分类。然而，传统MIL方法将各实例独立处理，忽略了实际应用中至关重要的上下文关联——例如相邻图像区块或切片之间的视觉特征关联。我们设计了一项合成分类任务，其中准确预测必须考虑相邻实例的特征。通过将现有通用MIL方法的性能与可通过解析形式获得的最优贝叶斯估计器进行量化对比，揭示了这些方法的局限性。实验结果表明，即使在数万个实例上从头开始训练，较新的相关MIL方法仍难以实现理论上的最佳泛化效果。 |
| TheraMind：面向长期心理咨询的战略性与适应性智能体

说明：
1. 保留专有名词"TheraMind"不译，符合学术术语处理规范
2. "Strategic and Adaptive"译为"战略性与适应性"，准确传达系统兼具战略规划与动态调整的双重特性
3. "Agent"译为"智能体"，符合人工智能领域术语标准
4. "Longitudinal Psychological Counseling"译为"长期心理咨询"，精准对应临床心理学中的纵向追踪研究概念
5. 整体采用破折号连接主副标题，符合中文社科类论文标题规范
6. 通过"面向"一词明确系统应用领域，增强标题的学术指向性 | He Hu | [PDF](http://arxiv.org/pdf/2510.25758v1) | 在心理咨询领域，大型语言模型（LLMs）正受到日益广泛的关注。然而现有方法往往缺乏情感理解能力、适应性策略以及基于长期记忆的多轮次治疗方法运用，导致其与真实临床实践存在显著差距。为突破这些关键瓶颈，我们提出TheraMind——一种具备战略适应能力的纵向心理咨询智能体。该系统的核心创新在于双循环架构：通过会话内循环实现战术级对话管理，通过跨会话循环执行战略级治疗规划，从而解构复杂的心理咨询过程。会话内循环通过感知患者情绪状态动态选择响应策略，同时借助跨会话记忆确保治疗连续性；跨会话循环则通过评估每次咨询的治疗效果，动态调整后续干预方法，赋予系统长期适应能力。我们在基于真实临床案例构建的高保真模拟环境中验证该方法，综合评估表明TheraMind在连贯性、灵活性和治疗协调性等多轮次指标上显著优于现有方法，验证了双循环设计在模拟战略性、适应性和纵向治疗行为方面的有效性。代码已开源：https://0mwwm0.github.io/TheraMind/。 |
