# arxiv 2025-11-16

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 基于自洽性采样的多模态大语言模型结果奖励强化学习训练优化 | Jiahao Wang | [PDF](https://arxiv.org/pdf/2511.10648v1) | 结果奖励强化学习（RL）已成为优化多模态大语言模型（MLLMs）逐步推理能力的常用且日益重要的方法。在多选题场景下——这是多模态推理基准测试的主要形式——该范式面临着一个重要却常被忽视的障碍：存在缺陷的思维链后猜测出正确答案的虚假轨迹，会获得与真实推理相同的奖励，这是不容忽视的缺陷。我们提出自洽采样（SCS）来纠正这一问题。针对每个问题，SCS通过（i）引入细微视觉扰动和（ii）对初始轨迹进行重复截断与重采样；最终轨迹间的一致性将生成可微分的自洽分数，在策略更新时降低不可靠轨迹的权重。基于Qwen2.5-VL-7B-Instruct模型，将SCS嵌入RLOO、GRPO和REINFORCE++系列方法后，在六个多模态基准测试中准确率最高提升7.7个百分点，且额外计算成本可忽略不计。SCS在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也取得显著效果，为MLLMs的结果奖励强化学习提供了简单通用的改进方案。 |
| 深度万物 3：从任意视角重建视觉空间

（注：该翻译在保持学术严谨性的基础上实现了以下处理：
1. "Depth Anything"译为"深度万物"，延续该系列前作命名逻辑，既体现技术普适性又保留品牌识别度
2. "Recovering"译为"重建"准确传达三维场景重构的技术内涵
3. "Visual Space"译为"视觉空间"符合计算机视觉领域术语规范
4. 整体句式采用中文科技论文标题常见的动宾结构，确保专业性与可读性统一） | Haotong Lin | [PDF](https://arxiv.org/pdf/2511.10647v1) | 我们提出Depth Anything 3（DA3），该模型能够从任意数量的视觉输入中预测空间一致的几何结构，无论是否已知相机位姿。为实现极简建模，DA3带来两项关键发现：单一标准Transformer（如原始DINO编码器）足以作为主干网络而无需架构特化；单一深度射线预测目标消除了复杂多任务学习的必要性。通过师生训练范式，该模型在细节还原与泛化能力上达到与Depth Anything 2（DA2）相当的水准。我们建立了涵盖相机位姿估计、任意视角几何重建与视觉渲染的新视觉几何基准测试集。在此基准测试中，DA3在所有任务上均创下最新纪录，相机位姿精度相较前最优方法VGGT平均提升44.3%，几何精度提升25.1%。此外，其在单目深度估计任务上也优于DA2。所有模型均仅使用公开学术数据集完成训练。 |
| ParoQuant：面向高效推理大语言模型的双向旋转量化方法

（解析：1. "Pairwise Rotation"译为"双向旋转"体现量化过程中双向交互特性；2. "Efficient Reasoning"译为"高效推理"准确传达优化目标；3. 采用"方法"作为隐性后缀，符合中文论文命名习惯；4. 保留首字母缩写的专业术语"ParoQuant"；5. 整体结构采用"主标题：技术特征+应用领域"的学术命名范式） | Yesheng Liang | [PDF](https://arxiv.org/pdf/2511.10645v1) | 仅权重量化（Weight-only PTQ）通过将大语言模型的权重压缩为低精度表示，以降低内存占用并加速推理。然而，权重和激活值中异常值的存在往往会导致较大的量化误差和严重的精度损失，尤其在当前具有长思维链的推理大语言模型中，误差会随推理步骤累积。现有PTQ方法要么未能充分抑制异常值，要么在推理过程中引入显著开销。本文提出成对旋转量化（ParoQuant），这是一种仅权重量化方法，通过结合硬件友好的可优化独立Givens旋转与通道级缩放，均衡通道间数值量级并压缩各量化组内的动态范围。我们进一步协同设计推理内核，充分利用GPU并行能力，确保旋转与缩放操作在运行时保持轻量化。在推理任务中，ParoQuant相较AWQ平均提升2.4%的准确率，且额外开销低于10%。这为推理大语言模型的高效精准部署开辟了新路径。 |
| 大型语言模型的黑盒在线策略蒸馏

这个翻译保留了以下关键要素：
1. "Black-Box" 译为"黑盒" - 保持机器学习领域的专业术语
2. "On-Policy" 译为"在线策略" - 符合强化学习领域的标准译法
3. "Distillation" 译为"蒸馏" - 沿用知识蒸馏技术的标准翻译
4. "Large Language Models" 译为"大型语言模型" - 保持自然语言处理领域的通用译法

该翻译准确传达了原文的技术含义，符合计算机科学和人工智能领域的专业表达规范。 | Tianzhu Ye | [PDF](https://arxiv.org/pdf/2511.10643v1) | 黑盒蒸馏技术仅通过从专有教师模型的文本输出中学习，即可创建学生大语言模型（LLM），而无需访问其内部逻辑值或参数。本研究提出生成对抗蒸馏（GAD）方法，实现了在线策略的黑盒蒸馏。GAD将学生LLM构建为生成器，并训练判别器来区分其响应与教师LLM的响应，从而形成极小极大博弈框架。该判别器作为随学生模型协同演进的在线策略奖励模型，能够提供稳定自适应的反馈。实验结果表明，GAD持续超越常用的序列级知识蒸馏方法。特别值得注意的是，采用GAD训练的Qwen2.5-14B-Instruct（学生模型）在LMSYS-Chat自动评估中达到了与教师模型GPT-5-Chat相当的水平。这些成果确立了GAD作为黑盒LLM蒸馏领域具有前景且有效的范式。 |
| 机器人速成课：学习柔性化与风格化摔倒动作 | Pascal Strauch | [PDF](https://arxiv.org/pdf/2511.10635v1) | 尽管鲁棒运动控制领域近期取得进展，现实环境中运行的双足机器人仍面临跌倒风险。现有研究多聚焦于防跌倒策略，而本研究另辟蹊径，专注于跌倒现象本身。具体而言，我们的目标是在赋予用户控制机器人终止姿态能力的同时，最大限度减少机体物理损伤。为此，我们提出一种与机器人构型无关的奖励函数，该函数在强化学习过程中能平衡三个目标：达成期望终止姿态、冲击力最小化以及关键部件保护。为增强策略对多样化初始跌倒条件的适应性，并实现推理阶段对任意未知终止姿态的指定，我们引入基于仿真的初始姿态与终止姿态采样策略。通过仿真与实体实验，本研究证实即使是双足机器人也能实现受控的柔性跌倒。 |
| 潜在空间一小步，像素世界大飞跃：面向扩散模型的快速潜在升维适配器

（注：该翻译采用学术论文标题常见的对仗式译法：
1. 保留原文对阿姆斯特朗名言的化用修辞，通过"潜在空间"与"像素世界"形成概念对应
2. "升维适配器"准确传达"Upscale Adapter"的技术内涵
3. 使用"面向..."的学术表达替代直译"for Your"，更符合中文论文标题规范
4. 通过"快速"明确强调模型性能优势，保持技术术语的精确性） | Aleksandr Razin | [PDF](https://arxiv.org/pdf/2511.10629v1) | 扩散模型难以突破训练分辨率的限制：直接进行高分辨率采样速度缓慢且成本高昂，而事后图像超分辨率（ISR）方法因在解码后执行操作，会引入伪影并增加额外延迟。我们提出潜在空间升维适配器（LUA），这是一种轻量级模块，可在最终VAE解码步骤之前直接在生成器的潜在代码上执行超分辨率。LUA作为即插即用组件集成，无需修改基础模型或增加额外扩散阶段，通过潜在空间中的单次前向传播即可实现高分辨率合成。该模块采用共享的Swin风格主干网络配合尺度特定的像素重组头，支持2倍和4倍缩放因子，并保持与图像空间超分辨率基线的兼容性，在实现相当感知质量的同时将解码与放大时间降低近3倍（从512px生成1024px仅增加0.42秒，而使用相同SwinIR架构的像素空间超分辨率需1.87秒）。此外，LUA在不同VAE的潜在空间中展现出强大泛化能力，无需为每个新解码器从头训练即可快速部署。大量实验表明，LUA在保真度上可媲美原生高分辨率生成，同时为现代扩散流程提供了实用高效的可扩展高保真图像合成路径。 |
| Instella：性能卓越的完全开放语言模型

（注：译文采用"性能卓越"对应"Stellar Performance"的文学性表达，既保留天体隐喻又符合中文技术文献表述习惯；"完全开放"精准对应"Fully Open"的技术定义，体现模型开源特性；专业术语"语言模型"保持直译确保学术准确性） | Jiang Liu | [PDF](https://arxiv.org/pdf/2511.10628v1) | 大语言模型（LLMs）已在广泛任务中展现出卓越性能，但多数高性能模型仍保持闭源或部分开放，限制了研究的透明度与可复现性。本研究推出Instella系列——完全开放的三百亿参数语言模型，其训练全程基于公开可获取的数据与代码库。依托AMD Instinct MI300X GPU的算力支持，Instella通过大规模预训练、通用指令微调以及与人类偏好的对齐完成开发。尽管使用的预训练标记数量显著少于同期多数模型，Instella在完全开放模型中取得了顶尖性能，并与同类规模的领先开放权重模型表现相当。我们同时发布两个专业变体：支持128K标记上下文长度的Instella-Long，以及通过监督微调和数学任务强化学习增强的推理专用模型Instella-Math。这些成果共同确立了Instella作为透明、高效、多功能的开源替代方案，推动语言建模研究向开放可复现的目标迈进。 |
| 使用场景程序查询带标签的时间序列数据 | Edward Kim | [PDF](https://arxiv.org/pdf/2511.10627v1) | 基于仿真的测试已成为保障信息物理系统（CPS）安全性的关键补充手段，由此大量研究聚焦于在仿真环境中识别故障场景。然而核心问题依然存在：仿真中发现的自动驾驶故障场景能否在现实物理系统中复现？由于仿真与真实传感器数据差异导致的"仿真-现实差异"，使得仿真识别的故障可能源于合成传感器数据的伪影，也可能是真实传感器数据中同样存在的实际问题。为解决该问题，验证仿真故障场景的有效方法是在真实数据集中定位这些场景，并检验故障是否在数据集中持续存在。为此，我们提出形式化定义，阐述标注时序传感器数据如何与抽象场景相匹配——该场景通过Scenic概率编程语言以场景程序表征。我们提出一种查询算法，在给定场景程序和标注数据集的前提下，可识别出符合指定场景的数据子集。实验表明：相较于最先进的商用视觉大语言模型，本算法在场景查询中准确率更高、速度提升数个数量级，且能随查询时序数据时长实现线性扩展。 |
| 具有隐藏凸性的非凸函数约束问题的全局解 | Ilyas Fatkhullin | [PDF](https://arxiv.org/pdf/2511.10626v1) | 约束非凸优化问题具有根本性挑战，因为全局解通常难以求得且约束规范条件未必成立。然而在控制与强化学习中的安全策略优化等众多应用中，此类问题具有隐藏凸性，即可通过非线性可逆变换重构为凸规划问题。这类变换通常具有隐含性或未知性，导致无法直接建立与凸规划的关联。另一方面，关于原始变量的（次）梯度往往可直接获取或易于估计，这促使我们采用标准（次）梯度预言机直接在原始（非凸）问题空间设计算法。本研究首次构建了可证明收敛至全局极小值的非凸问题求解算法：首先通过改进的不精确邻近点方法，在非光滑情形下建立了$\widetilde{\mathcal{O}}(\varepsilon^{-3})$的全局末点收敛复杂度保证；针对光滑问题，我们基于线性约束二次子问题提出新型束水平方法，将复杂度提升至$\widetilde{\mathcal{O}}(\varepsilon^{-1})$。令人惊讶的是，尽管问题具有非凸性，我们的方法既不要求任何约束规范条件，又能处理隐藏凸等式约束，且达到的复杂度与求解无约束隐藏凸优化问题相当。 |
| SSR：面向大语言模型推理的苏格拉底式自我优化 | Haizhou Shi | [PDF](https://arxiv.org/pdf/2511.10621v1) | 大型语言模型（LLMs）已展现出卓越的推理能力，但现有测试时框架通常依赖粗略的自验证与自修正机制，限制了其在复杂任务中的有效性。本文提出苏格拉底式自我精炼（SSR）框架，通过细粒度评估与精准优化来提升LLM推理能力。该框架将模型响应分解为可验证的（子问题，子答案）对，通过受控重解与自洽性检查实现步骤级置信度估计。通过定位不可靠推理步骤并迭代优化，SSR能生成更准确且可解释的推理链。在五个推理基准测试和三种LLM上的实验结果表明，SSR持续优于当前最先进的迭代式自我精炼基线方法。除性能提升外，SSR为评估和理解LLM内部推理过程提供了原则性的黑盒分析方法。代码已开源：https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning。 |
