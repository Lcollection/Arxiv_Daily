# arxiv 2025-12-16

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 扩散浏览器：通过多分支解码器实现交互式扩散预览 | Susung Hong | [PDF](https://arxiv.org/pdf/2512.13690v1) | 视频扩散模型已彻底革新了生成式视频合成技术，但其生成过程存在精度不足、速度缓慢且透明度低的问题——用户在生成期间往往需要长时间处于"黑箱"等待状态。本研究提出DiffusionBrowser，这是一个与模型无关的轻量级解码框架，允许用户在去噪过程中的任意节点（时间步或Transformer模块）交互式生成预览。该模型能以超实时4倍速（4秒视频生成耗时不足1秒）生成包含RGB与场景本征信息的多模态预览表征，这些预览与最终视频保持外观与运动的一致性。通过训练后的解码器，我们证明了在中间噪声步骤中可通过随机性重注入与模态引导实现交互式生成调控，从而解锁了全新的控制能力。此外，我们利用习得的解码器对模型进行系统性探查，揭示了在原本黑箱化的去噪过程中，场景、物体及其他细节是如何逐步组合构建的。 |
| LitePT：更轻量且更强大的点云Transformer模型 | Yuanwen Yue | [PDF](https://arxiv.org/pdf/2512.13689v1) | 用于三维点云处理的现代神经架构同时包含卷积层与注意力模块，但如何最佳组合它们仍不明确。我们分析了三维点云网络中不同计算模块的作用，发现一种直观规律：卷积适合在早期高分辨率层提取低层次几何特征，此时注意力机制计算成本高昂却未带来增益；而注意力机制能更高效地在低分辨率深层捕获高层次语义与上下文信息。基于这一设计原则，我们提出一种改进的新型三维点云主干网络，在浅层采用卷积运算，在深层切换至注意力机制。为避免丢弃冗余卷积层时损失空间布局信息，我们引入了一种无需训练的新型三维位置编码方法PointROPE。由此构建的LitePT模型与最先进的Point Transformer V3相比，参数量减少3.6倍，运行速度提升2倍，内存占用降低2倍，但在多项任务与数据集上仍能保持相当甚至更优的性能。代码与模型已开源：https://github.com/prs-eth/LitePT。 |
| 迈向可扩展的视觉生成分词器预训练 | Jingfeng Yao | [PDF](https://arxiv.org/pdf/2512.13687v1) | 视觉分词器（如VAE）的潜在空间质量对现代生成模型至关重要。然而，基于标准重建的训练范式产生的潜在空间偏向于低层次信息，导致一个根本性缺陷：更好的像素级精度并不能带来更高质量的生成效果。这意味着将大量计算资源投入视觉分词器预训练对生成性能的提升效果甚微。我们将此问题定义为"预训练扩展困境"，并提出关键转变思路：为有效支持生成任务，潜在空间必须简洁地表征高层次语义信息。本文提出VTP——一个统一的视觉分词器预训练框架，率先实现图像-文本对比损失、自监督损失与重建损失的联合优化。我们的大规模实验揭示两个核心发现：（1）理解能力是驱动生成效果的关键因素；（2）显著改善的扩展特性，生成性能随预训练投入的计算量、参数量和数据量实现有效提升。经过大规模预训练后，我们的分词器展现出卓越性能（ImageNet数据集上零样本准确率78.2%，rFID指标0.36），在生成任务上比先进蒸馏方法收敛速度快4.1倍。更重要的是，该框架具备高效扩展性：在不改变标准DiT训练配置的情况下，仅通过增加VTP预训练的浮点运算量，下游生成任务的FID指标即可提升65.8%，而传统自编码器在仅使用1/10计算量时性能便过早停滞。预训练模型已开源：https://github.com/MiniMax-AI/VTP。 |
| 超越表层形式：基于自发语音的阿尔茨海默病检测语义分析流程 | Dylan Phelps | [PDF](https://arxiv.org/pdf/2512.13685v1) | 阿尔茨海默病（AD）是一种进行性神经退行性疾病，会对认知能力产生负面影响。通过分析语言评估任务（如图片描述）的输出结果，可以自动识别与语言相关的变化。语言模型作为AD筛查工具的基础显示出潜力，但其有限的可解释性在区分认知衰退的真实语言标记与表层文本模式方面构成挑战。为解决这一问题，我们研究了表层形式变化如何影响分类性能，旨在评估语言模型表征潜在语义指标的能力。我们引入了一种创新方法：通过改变句法和词汇来转换文本表层形式，同时保留语义内容。这种转换显著改变了文本结构和词汇内容（表现为较低的BLEU和chrF分数），但保留了底层语义（反映为较高的语义相似度分数），从而隔离了语义信息的影响。研究发现，模型在处理转换文本时的表现与使用原始文本时相似，仅在宏观F1分数上存在微小偏差。我们还探究了图片描述中的语言是否保留足够细节，能够通过生成模型重建原始图像。研究发现，基于图像的转换会引入大量噪声，从而降低分类准确性。我们的方法为探究影响模型预测的特征提供了新视角，并能够消除可能的虚假相关性。研究表明，仅依靠语义信息，基于语言模型的分类器仍能检测AD。这项工作表明，难以察觉的语义损伤可以被识别，这解决了语言退化中一个被忽视的特征，并为早期检测系统开辟了新途径。 |
| 循环视频掩码自编码器 | Daniel Zoran | [PDF](https://arxiv.org/pdf/2512.13684v1) | 我们提出循环视频掩码自编码器（RVM）：这是一种新颖的视频表征学习方法，通过基于Transformer的循环神经网络聚合随时间变化的密集图像特征，有效捕捉自然视频数据的时空结构。RVM通过非对称掩码预测任务进行学习，仅需标准像素重建目标。该设计实现了高效的“通用型”编码器：在动作识别、点/目标跟踪等视频级任务上，RVM与前沿视频模型（如VideoMAE、V-JEPA）性能相当；在测试几何与密集空间理解的任务中，其表现也优于图像模型（如DINOv2）。值得注意的是，RVM在轻量化模型架构中无需知识蒸馏即可实现强劲性能，其参数效率比同类视频掩码自编码器提升高达30倍。此外，我们证明RVM的循环特性能够以线性计算成本实现长时序的稳定特征传播，克服了标准时空注意力架构的某些局限。最后，我们通过定性可视化展示RVM能够学习场景语义、结构与运动的丰富表征。 |
| I-Scene：三维实例模型作为隐式通用空间学习器 | Lu Ling | [PDF](https://arxiv.org/pdf/2512.13683v1) | 泛化能力仍是交互式三维场景生成的核心挑战。现有基于学习的方法将空间理解建立在有限场景数据集上，导致对新布局的泛化能力受限。我们通过重新编程预训练的三维实例生成器，使其成为场景级学习器，以模型为中心的空间监督替代数据集受限的监督。这种重新编程释放了生成器的可迁移空间知识，使其能够泛化至未见过的布局和全新物体组合。值得注意的是，即使训练场景由随机组合的物体构成，空间推理能力依然能够涌现。这表明生成器的可迁移场景先验为从纯几何线索推断邻近性、支撑关系和对称性提供了丰富的学习信号。我们摒弃广泛使用的规范空间，采用以视角为中心的场景空间构建方法，实现了一个完全前馈、可泛化的场景生成器，能够直接从实例模型中学习空间关系。定量与定性结果表明，三维实例生成器是隐式的空间学习与推理器，为构建交互式三维场景理解与生成的基础模型指明了方向。项目页面：https://luling06.github.io/I-Scene-project/ |
| 激光：面向免训练流式4D重建的层级尺度对齐方法 | Tianye Ding | [PDF](https://arxiv.org/pdf/2512.13680v1) | 近期如VGGT和$π^3$等前馈重建模型虽能实现出色的重建质量，但由于其二次方内存复杂度无法处理流式视频，限制了实际部署。现有流式处理方法虽通过可学习记忆机制或因果注意力机制解决此问题，但需要大量重新训练，且可能无法充分利用先进离线模型的强几何先验。我们提出LASER框架，该免训练框架通过对齐连续时间窗口的预测结果，将离线重建模型转换为流式系统。我们发现简单的相似变换（$\mathrm{Sim}(3)$）对齐会因层深错位而失效：单目尺度歧义导致不同场景层的相对深度尺度在窗口间出现不一致变化。为此，我们提出分层尺度对齐方法：将深度预测分割为离散层级，计算每层尺度因子，并在相邻窗口和时间戳间进行传播。大量实验表明，LASER在相机位姿估计与点云地图重建方面达到最先进性能，同时在RTX A6000 GPU上以14 FPS运行且峰值内存仅6 GB，实现了公里级流式视频的实际部署。项目网站：$\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$ |
| 通过文本引导图像到3D的前馈式三维编辑 | Ziqi Ma | [PDF](https://arxiv.org/pdf/2512.13678v1) | 图像到三维生成技术的最新进展为设计、增强现实/虚拟现实（AR/VR）及机器人领域开辟了广阔前景。然而，要将人工智能生成的三维资源应用于实际场景，一个关键需求是能够便捷地对其进行编辑。我们提出了一种前馈式方法——Steer3D，旨在为图像到三维模型增加文本可操控性，从而实现对生成三维资源的语言化编辑。该方法受ControlNet启发，我们将其适配于图像到三维生成任务，实现了在前向传播过程中直接进行文本引导。我们构建了可扩展的自动数据生成引擎，并开发了基于流匹配训练与直接偏好优化（DPO）的两阶段训练方案。相较于现有方法，Steer3D能更精准地遵循语言指令，同时保持与原始三维资源更高的一致性，且处理速度提升2.4至28.5倍。Steer3D证明，通过十万量级数据训练，可以为预训练的图像到三维生成模型增加新模态（文本）的引导能力。项目网站：https://glab-caltech.github.io/steer3d/ |
| JoVA：面向联合视频-音频生成的统一多模态学习框架 | Xiaohu Huang | [PDF](https://arxiv.org/pdf/2512.13677v1) | 本文提出JoVA——一个统一的视频-音频联合生成框架。尽管近期研究取得了鼓舞人心的进展，现有方法仍面临两个关键局限：其一，多数现有方案仅能生成环境音效，缺乏生成与唇形同步的人类语音的能力；其二，当前统一的视频-音频生成方法通常依赖显式融合或特定模态对齐模块，这不仅增加了架构设计的复杂性，也削弱了原始Transformer模型的简洁性。为解决这些问题，JoVA在每层Transformer中采用视频与音频token的联合自注意力机制，无需额外对齐模块即可实现直接高效的跨模态交互。此外，为实现高质量的唇语同步，我们基于面部关键点检测提出了一种简洁有效的嘴部区域损失函数，该函数能在保持架构简洁性的同时，增强训练过程中对关键嘴部区域的监督。在多个基准测试上的大量实验表明，JoVA在唇形同步精度、语音质量及整体音视频生成保真度方面，均优于或可与当前最先进的统一生成方法与音频驱动方法相媲美。我们的研究成果确立了JoVA作为高质量多模态生成框架的优越性。 |
| 迈向高效模型编辑：实现大型语言模型个性化定制 | Baixiang Huang | [PDF](https://arxiv.org/pdf/2512.13676v1) | 个性化正日益成为大型语言模型（LLM）适应用户个体偏好与需求的关键能力。然而现有方法通常存在计算成本高、数据依赖性强、易受灾难性遗忘影响，以及在多轮交互或处理隐式查询时性能下降等问题。为应对这些挑战，我们将个性化重新定义为模型编辑任务，并提出"个性化编辑"框架——该框架通过聚类偏好表征引导的局部编辑来实现精准更新。这一设计能在保持模型整体能力的同时，完成与用户偏好对齐的定向调整。

当前个性化评估基准多依赖LLM间拟人对话而非真实人机交互数据，或偏重风格模仿而忽视需要准确回忆用户特定偏好的信息检索任务。为此，我们构建了用户偏好问答数据集（UPQA），该短答案问答数据集基于真实场景中不同难度的用户查询构建。与既有基准不同，UPQA直接评估模型回忆并应用特定用户偏好的能力。

在多组实验环境中，个性化编辑框架相比微调方法实现了更高的编辑精度与计算效率，同时在多轮对话和隐式偏好查询场景中显著优于基于提示的基线模型。 |
