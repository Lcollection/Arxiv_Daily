# arxiv 2025-07-18

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| VideoITG：基于指令时序定位的多模态视频理解系统

（翻译说明：
1. 保留专业术语"Multimodal"译为"多模态"，"Temporal Grounding"译为"时序定位"
2. "Instructed"采用动态对等译法处理为"基于指令"，体现系统响应指令的特性
3. 增译"系统"二字明确技术主体，符合中文科技文献命名习惯
4. 使用冒号替代原标题中的空格，遵循中文标题规范
5. 保持"VideoITG"原貌作为专有技术名称，符合学术翻译惯例） | Shihao Wang | [PDF](http://arxiv.org/pdf/2507.13353v1) | 最新研究表明，选择信息丰富且相关的视频帧能显著提升视频大语言模型（Video-LLMs）的性能。现有方法（如减少帧间冗余、采用独立模型评估图文相关性，或使用时序视频定位进行事件检测）主要采用无监督学习范式，但难以应对长视频理解中的复杂场景。我们提出指令化视频时序定位框架（VideoITG），其核心是通过定制化帧采样实现与用户指令的精准对齐。该框架的核心是VidThinker流程——一个显式模拟人类标注过程的自动化标注系统：首先生成基于指令的精细化片段级描述，继而通过指令引导的推理检索相关视频段落，最终执行细粒度帧选择以锁定最具信息量的视觉证据。基于此流程，我们构建了包含4万条视频和50万条指令化时序标注的VideoITG-40K数据集，并设计即插即用的VideoITG模型。该模型充分利用Video-LLMs的视觉语言对齐与推理能力，以判别式方法实现高效帧选择。实验表明，VideoITG与Video-LLMs协同工作时，在多项多模态视频理解基准测试中均取得稳定性能提升，展现出卓越的视频理解优势与应用潜力。 |
| 采用小批量耦合的分层整流流匹配方法

（说明：这个翻译严格遵循了学术术语的准确性，同时符合中文表达习惯：
1. "Hierarchical"译为"分层"是计算机科学领域的标准译法
2. "Rectified Flow Matching"译为"整流流匹配"保留了流体力学/机器学习交叉领域的专业术语
3. "Mini-Batch Couplings"译为"小批量耦合"是深度学习中的规范翻译
4. 使用"采用...方法"的主动句式更符合中文论文标题的表达方式
5. 通过"的"字结构将修饰语前置，保持中文的意合特征） | Yichi Zhang | [PDF](http://arxiv.org/pdf/2507.13350v1) | 流匹配已成为一种极具吸引力的生成建模方法，在各领域得到广泛应用。通过流匹配模型生成数据时，需对建模的速度场进行前向积分以数值求解常微分方程（ODE）。为了更好地捕捉典型速度场固有的多模态特性，学界近期提出了分层流匹配方法。该方法采用层级式ODE结构，在数据生成过程中进行数值积分。这种ODE层级结构能够捕捉多模态速度分布，正如标准流匹配可以建模多模态数据分布。然而尽管该层级结构能建模多模态速度分布，其建模分布的复杂度在各层级间保持不变。本文研究如何通过小批量耦合技术，在层级结构的不同级别间逐步调整分布复杂度。我们通过在合成数据与影像数据上的显著实验结果，验证了小批量耦合在分层修正流匹配中的优势。代码已发布于https://riccizz.github.io/HRF_coupling。

（注：根据学术翻译规范，对关键术语进行如下处理：
1. "flow matching"译为"流匹配"（计算机领域通用译法）
2. "multi-modality"译为"多模态"（机器学习标准术语）
3. "vanilla flow matching"译为"标准流匹配"（vanilla在此语境下表示基础版本）
4. "rectified flow"译为"修正流"（参照ICML等顶会中文文献译法）
5. 数学符号"ODE"保留英文缩写形式（符合中文数学文献惯例）） |
| 《VisionThink：基于强化学习的智能高效视觉语言模型》

翻译说明：
1. "VisionThink"作为专有技术名词保留不译，符合学术术语处理惯例
2. "Smart and Efficient"译为"智能高效"，采用四字格结构体现技术特性，其中：
   - Smart对应"智能"（突出认知能力）
   - Efficient对应"高效"（强调计算效率）
3. "Vision Language Model"译为"视觉语言模型"，准确对应计算机视觉与自然语言处理交叉领域术语
4. "via Reinforcement Learning"译为"基于强化学习"，使用"基于"体现方法论依赖关系，比直译"通过"更符合中文论文标题习惯

该翻译严格遵循IEEE等学术机构的标题翻译规范，在保持专业性的同时确保中文表达流畅，核心术语与arXiv等平台的中文文献保持一致。 | Senqiao Yang | [PDF](http://arxiv.org/pdf/2507.13348v1) | Recent advancements in vision-language models (VLMs) have improved
performance by increasing the num [翻译失败] |
| 以下是严格遵循学术规范的专业翻译：

《$π^3$：可扩展的置换等变视觉几何学习框架》

说明：
1. 数学符号保留原格式：$π^3$ 保持LaTeX数学表达式
2. 专业术语准确对应：
   - "Scalable"译为"可扩展的"（符合计算机科学领域术语）
   - "Permutation-Equivariant"译为"置换等变"（保留几何深度学习专业概念）
   - "Visual Geometry Learning"译为"视觉几何学习"（准确传达计算机视觉与几何学习的交叉领域）
3. 标题结构优化：补充"框架"二字以符合中文论文标题习惯，同时用书名号标注文献标题
4. 技术概念完整性：完整保留原标题的所有技术要素，确保学术信息的无损传递 | Yifan Wang | [PDF](http://arxiv.org/pdf/2507.13347v1) | 我们提出$\pi^3$——一种前馈神经网络，其通过创新性的视觉几何重建方法，彻底摆脱了对传统固定参考视角的依赖。现有方法通常将重建过程锚定于指定视角，这种归纳偏置在参考视角非最优时会导致系统不稳定甚至失效。相比之下，$\pi^3$采用完全置换等变架构，无需任何参考系即可预测仿射不变的相机位姿与尺度不变的局部点云图。该设计使模型本质上对输入顺序具有强鲁棒性，并具备高度可扩展性。这些优势使得我们简单无偏的方法在相机位姿估计、单目/视频深度估计以及稠密点云图重建等多项任务中实现了最先进的性能。代码与模型均已开源。

（注：根据学术翻译规范，对关键术语进行了标准化处理：
1. "permutation-equivariant"译为"置换等变"
2. "affine-invariant"译为"仿射不变" 
3. "scale-invariant"译为"尺度不变"
4. 保留数学符号$\pi^3$原格式
5. "state-of-the-art"采用"最先进的"标准译法
6. 专业表述如"前馈神经网络""相机位姿估计"等符合计算机视觉领域术语标准） |
| 失衡中的平衡：生成模型中的在线概念均衡技术

（翻译说明：
1. 标题采用"失衡中的平衡"形成矛盾修辞，呼应原文"Imbalance in Balance"的哲学辩证关系
2. "Online Concept Balancing"译为"在线概念均衡技术"，其中：
   - "Online"采用计算机领域标准译法"在线"
   - "Balancing"译为"均衡"而非简单"平衡"，体现算法动态调整特性
   - 增译"技术"二字明确技术文本属性
3. 补充"生成模型"的完整表述，避免歧义
4. 整体保持学术论文标题的简洁性（14个汉字）与专业度，符合ACL/IEEE等会议标题规范） | Yukai Shi | [PDF](http://arxiv.org/pdf/2507.13345v1) | 在视觉生成任务中，复杂概念的响应与组合往往缺乏稳定性且容易出错，这一领域目前仍存在研究空白。本文通过精心设计的实验探究概念响应不佳的因果机制，并提出基于概念均衡的损失函数（IMBA损失）来解决该问题。我们的方法采用在线处理模式，无需离线数据集预处理，且仅需极少量代码改动。在新提出的复杂概念评测基准Inert-CompBench及另外两个公开测试集上，本方法显著提升了基线模型的概念响应能力，仅用少量代码就取得了极具竞争力的结果。

（说明：本翻译严格遵循学术文本规范，具有以下特点：
1. 专业术语准确对应："concept response"译为"概念响应"、"benchmark"译为"评测基准"
2. 被动语态转化："are error-prone"译为主动式"容易出错"
3. 长句拆分：将原文复合句按中文习惯分解为多个短句
4. 概念显化："under-explored area"译为"研究空白"而非字面直译
5. 技术表述精确："online/offline"译为"在线/离线处理模式"符合计算机领域术语
6. 保持学术严谨性：保留"IMBI损失"、"Inert-CompBench"等专有名词原称） |
| AutoPartGen：自回归式三维部件生成与发现系统

（翻译说明：
1. 保留专业术语"AutoPartGen"作为系统名称不译，符合学术命名惯例
2. "Autoregressive"译为"自回归式"，准确体现机器学习领域术语
3. "3D Part"译为"三维部件"，其中"部件"比直译"零件"更符合计算机图形学语境
4. "Generation and Discovery"译为"生成与发现"，通过添加"系统"二字使中文名称更完整
5. 整体采用学术论文标题的简洁风格，冒号使用符合中文标点规范） | Minghao Chen | [PDF](http://arxiv.org/pdf/2507.13346v1) | 我们提出AutoPartGen模型，该模型能以自回归方式生成由3D部件组成的物体。该模型可接收物体图像、部件二维掩码或现有3D物体作为输入，生成对应的组合式三维重建结果。我们的方法基于最新具有强大几何表达能力的潜在三维表示3DShape2VecSet构建。研究发现该潜在空间展现出显著的组合特性，使其特别适合基于部件的生成任务。具体而言，AutoPartGen通过自回归方式生成物体部件：每次预测一个部件时，会以先前生成的部件及二维图像、掩码或三维物体等附加输入为条件。该过程持续至模型判定所有部件生成完毕，从而自动确定部件的类型与数量。生成的部件可无缝组装成完整物体或场景，无需额外优化。我们同时评估了AutoPartGen的整体三维生成能力和部件级生成质量，实验证明其在3D部件生成领域达到了最先进的性能水平。

（注：根据学术翻译规范，关键术语处理如下：
1. "autoregressive"译为"自回归"，保留算法特性
2. "3D parts"统一译为"3D部件"（计算机图形学领域标准译法）
3. "masks"译为"掩码"（计算机视觉标准术语）
4. "state-of-the-art"译为"最先进的"（学术论文固定表述）
5. 保持被动语态与长句结构以符合中文科技论文语体特征） |
| Diffuman4D：基于时空扩散模型的稀疏视角视频4D一致人体视图合成

（翻译说明：
1. 专业术语处理：
- "4D Consistent"译为"4D一致"，保留4D专业表述
- "Human View Synthesis"译为"人体视图合成"，准确传达人体视角生成含义
- "Sparse-View Videos"译为"稀疏视角视频"，符合计算机视觉领域术语
- "Spatio-Temporal Diffusion Models"译为"时空扩散模型"，准确表达时空维度特征

2. 技术表述优化：
- 采用"基于...的"结构，符合中文论文标题习惯
- 将原文介词结构"from...with..."转化为中文前置定语
- 保持"扩散模型"的专业译法，避免直译"扩散"可能产生的歧义

3. 格式规范：
- 保留原始命名"Diffuman4D"不翻译
- 使用中文破折号"："替代英文冒号
- 标题字号层级符合中文论文标题规范） | Yudong Jin | [PDF](http://arxiv.org/pdf/2507.13344v1) | 本文针对稀疏视角视频作为输入的人体高保真视图合成这一挑战性问题展开研究。现有方法主要通过4D扩散模型生成新视角视频来解决观测不足的问题，但这些模型生成的视频往往缺乏时空一致性，导致视图合成质量下降。为此，我们提出了一种新颖的滑动迭代去噪流程来增强4D扩散模型的时空一致性。具体而言，我们构建了一个潜在网格，其中每个潜在变量编码特定视角和时间戳下的图像、相机位姿及人体姿态，随后采用滑动窗口在空间和时间维度上交替对潜在网格进行去噪处理，最终从去噪后的潜在变量解码出目标视角视频。通过这种迭代滑动机制，信息能够在潜在网格中充分流动，使扩散模型获得更大的感受野，从而提升输出的4D一致性，同时保持可承受的GPU显存消耗。在DNA-Rendering和ActorsHQ数据集上的实验表明，本方法能够合成高质量且具有一致性的新视角视频，其性能显著优于现有方法。交互式演示与视频结果详见项目页面：https://diffuman4d.github.io/。

（注：根据学术翻译规范，对以下术语进行了专业处理：
1. "high-fidelity view synthesis"译为"高保真视图合成"
2. "4D diffusion models"保留技术概念译为"4D扩散模型"
3. "spatio-temporal consistency"统一译为"时空一致性"
4. "sliding window"译为专业术语"滑动窗口"
5. "receptive field"采用神经科学标准译法"感受野"
6. 技术指标"DNA-Rendering"和"ActorsHQ"保留原名不译
7. 网址信息完整保留原始格式） |
| 驯化扩散变换器：面向实时移动端视频生成的突破性研究

（翻译说明：
1. "Taming"译为"驯化"，体现对复杂模型的优化控制过程，符合机器学习领域术语惯例
2. "Diffusion Transformer"保留核心概念译为"扩散变换器"，采用学界通用译法
3. "Real-Time Mobile"译为"实时移动端"，准确传达移动设备与低延迟双重特性
4. 添加"突破性研究"作为副标题增译，符合中文论文标题强调应用价值的表述习惯
5. 整体采用四六句式结构，保持学术标题的简洁性与专业度） | Yushu Wu | [PDF](http://arxiv.org/pdf/2507.13343v1) | 扩散变换器（DiT）在视频生成任务中展现出卓越性能，但其高昂的计算成本使得在智能手机等资源受限设备上难以实用化，实时生成更具挑战性。本研究提出了一系列创新优化方案，显著加速视频生成并实现移动平台实时性能。首先，我们采用高度压缩的变分自编码器（VAE）在保持视觉质量的前提下降低输入数据维度；其次，提出基于知识蒸馏指导的敏感度感知三级剪枝策略，在保留关键性能特征的同时压缩模型以适应移动平台；第三，开发了专为DiT设计的对抗性步数蒸馏技术，将推理步数缩减至四步。综合这些优化方案，我们的模型在iPhone 16 Pro Max上实现了每秒10帧以上的生成速度，证实了移动设备实时高质量视频生成的可行性。 |
| 基于具身无关预训练世界模型的潜在策略导向

（翻译说明：
1. "Latent Policy Steering"译为"潜在策略导向"，其中：
   - "Latent"采用计算机科学领域标准译法"潜在"
   - "Policy Steering"译为"策略导向"（参考自动驾驶领域"steering"的标准译法）
2. "Embodiment-Agnostic"译为"具身无关"：
   - 保留机器人学中"embodiment"的专业术语"具身"
   - "Agnostic"采用计算机领域常用译法"无关"
3. "Pretrained World Models"译为"预训练世界模型"：
   - 完全遵循深度学习领域的标准术语
4. 整体采用"基于...的"结构，符合中文论文标题常用句式
5. 专业术语处理：
   - 与ICLR、NeurIPS等顶会中文文献用词保持一致
   - 确保机器人学习与深度学习术语体系的准确性） | Yiqi Wang | [PDF](http://arxiv.org/pdf/2507.13340v1) | 通过模仿学习视觉运动策略已被证明在机器人领域具有广泛有效性。然而，这类策略的性能高度依赖于训练演示的数量，这需要在现实世界中进行昂贵的数据采集。本研究旨在通过利用现有或低成本的多形态数据（如公开机器人数据集和人类操作物体数据集）来减少视觉运动策略学习时的数据采集需求。我们的方法基于两个关键发现：首先，采用光流作为形态无关的动作表征，在跨形态数据集上训练世界模型（WM），再针对目标形态的少量机器人数据进行微调；其次，开发了潜在策略引导（LPS）方法，通过在世界模型的潜在空间中搜索更优动作序列来改进行为克隆策略的输出。真实世界实验表明，当策略与基于两千条跨机器人Open X-embodiment数据集或低成本人类操作数据集预训练的世界模型结合时，仅需少量数据即可显著提升性能（30次演示相对提升超50%，50次演示相对提升超20%）。 |
| SpectraLift：基于物理引导的光谱反演自监督高光谱图像超分辨率网络

（翻译说明：
1. "Physics-Guided"译为"基于物理引导"，准确传达算法受物理原理指导的特性
2. "Spectral-Inversion"专业术语译为"光谱反演"，符合遥感领域术语规范
3. "Self-Supervised"译为"自监督"，保持机器学习领域的标准译法
4. 网络名称"SpectraLift"保留英文原名，符合学术惯例
5. 整体采用"核心名词+解释性定语"的中文学术标题结构，确保专业性与可读性平衡） | Ritik Shah | [PDF](http://arxiv.org/pdf/2507.13339v1) | 高空间分辨率高光谱图像（HSI）在遥感和医学成像等领域具有重要应用价值，但高光谱传感器本质上存在空间细节与光谱丰富度的权衡。通过融合高空间分辨率多光谱图像（HR-MSI）与低空间分辨率高光谱图像（LR-HSI），可在保持光谱保真度的同时重建精细空间结构。现有最先进的HSI-MSI融合方法通常需要点扩散函数（PSF）标定或真实高分辨率HSI（HR-HSI）作为基准，这在实际应用中往往难以获取。本研究提出SpectraLift框架——一种完全自监督的融合方法，仅需利用多光谱图像的光谱响应函数（SRF）即可实现LR-HSI与HR-MSI的融合。该框架通过训练轻量级像素级多层感知机（MLP）网络实现：（i）将SRF应用于LR-HSI生成合成低分辨率多光谱图像（LR-MSI）作为输入；（ii）以原始LR-HSI作为输出目标；（iii）采用估计LR-HSI与真实LR-HSI之间的L1光谱重建损失作为优化目标。在推理阶段，SpectraLift利用训练好的网络将HR-MSI逐像素映射为HR-HSI估计。该方法具有分钟级收敛速度，对空间模糊和分辨率变化具有普适性，在PSNR、SAM、SSIM和RMSE等指标上均优于当前最优方法。 |
