# arxiv 2025-12-23

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 棱镜假说：通过统一自编码实现语义与像素表征的和谐统一 | Weichen Fan | [PDF](https://arxiv.org/pdf/2512.19693v1) | 跨模态的深度表征本质上是相互交织的。本文系统分析了多种语义编码器与像素编码器的频谱特性。研究发现了一个极具启发性却鲜被探讨的对应关系：编码器的特征频谱与其功能角色存在深刻关联——语义编码器主要捕获编码抽象含义的低频分量，而像素编码器额外保留了传达细粒度细节的高频信息。这一启发式发现为理解编码器行为与其底层频谱结构的关系提供了统一视角，我们将其定义为"棱镜假说"：每种数据模态都可视为现实世界在共享特征频谱上的投影，恰如棱镜分光现象。基于此洞见，我们提出了统一自编码模型，该模型通过创新的频段调制器协调语义结构与像素细节，使其在潜在空间中无缝共存。在ImageNet和MS-COCO基准测试上的大量实验表明，我们的模型以先进性能将语义抽象与像素级保真度统一于单一潜在空间。 |
| Interact2Ar：基于自回归扩散模型的全身体人-人交互生成 | Pablo Ruiz-Ponce | [PDF](https://arxiv.org/pdf/2512.19692v1) | 生成逼真的人与人交互是一项极具挑战性的任务，它不仅需要高质量的单人身体与手部动作，还要求所有交互者之间保持协调一致的运动关联。由于现有数据集的局限性及学习复杂度的增加，先前的研究方法往往忽略手部动作，从而限制了交互的真实感与表现力。此外，当前基于扩散模型的方法通常同时生成完整动作序列，难以捕捉人类交互中反应性与适应性的本质特征。为突破这些局限，我们提出了Interact2Ar——首个端到端的文本条件自回归扩散模型，专门用于生成全身尺度的人与人交互动作。该模型通过并行分支结构精细整合手部运动学特征，实现了高保真度的全身动作生成。进一步地，我们引入结合新型记忆技术的自回归生成流程，利用高效的大上下文窗口适应人类交互固有的动态变化特性。本模型的适应能力支持一系列下游应用，包括时序动作合成、实时干扰适应，以及从双人交互到多人场景的拓展。为验证生成动作的质量，我们开发了一套专为全身交互评估设计的鲁棒评估体系与扩展指标。通过定量与定性实验，我们证明了Interact2Ar在交互生成领域达到了最先进的性能水平。 |
| 在医师监督下可扩展地提升任务基准的临床有效性 | Junze Ye | [PDF](https://arxiv.org/pdf/2512.19691v1) | 临床风险评分的自动化计算为减轻医生行政负担、提升患者护理质量提供了重要机遇。当前评估该能力的标准是MedCalc-Bench——一个通过基于大语言模型的特征提取与规则化聚合构建的大规模数据集。然而，将此类模型生成的基准视为静态"真理"存在风险，可能使历史模型错误固化为评估金标准，当这些数据集作为强化学习的奖励信号时，该问题会被危险地放大。本研究提出将临床评分计算等复杂任务的基准视为"持续更新的动态文档"，应随其创建流程的改进而定期重评估。我们引入了一套系统化的、医生参与循环的流程，利用先进的智能验证器对MedCalc-Bench进行审计与重标注，通过自动化分诊机制将稀缺的临床医生注意力保留给最具争议的案例。审计结果显示，因提取错误、计算器逻辑失配及临床模糊性等问题，相当比例的原标注与医学事实存在偏差。为探究此类标注噪声是否显著影响下游强化学习训练，我们通过组相对策略优化对Qwen3-8B模型进行微调，实验表明使用修正标注训练可使准确率较原始基线绝对提升8.7%，证实了标注噪声确实实质影响模型评估。这些发现强调：在安全关键领域，严格的基准维护是实现真正模型对齐的前提条件。 |
| 通过大规模多模态对应学习，拓展视听感知的前沿领域 | Apoorv Vyas | [PDF](https://arxiv.org/pdf/2512.19687v1) | 我们推出感知编码器视听版（Perception Encoder Audiovisual, PE-AV），这是一个通过规模化对比学习训练而成的新型音视频理解编码器系列。基于PE架构，PE-AV在多个关键维度实现突破：将表征能力扩展至音频领域，并原生支持跨音频-视频、音频-文本及视频-文本模态的联合嵌入。PE-AV统一的跨模态嵌入能力赋能语音检索等创新任务，并在标准音视频基准测试中创下性能新纪录。

这一突破得益于我们构建的强大视听数据引擎，该引擎为约亿级（O(100M)）音视频对生成高质量描述文本，实现了跨模态的大规模监督训练。我们的音频数据涵盖语音、音乐及通用音效，突破了以往研究常受限于单一领域的瓶颈。通过设计十组对比学习目标对，我们证明扩展跨模态与描述类型的配对能显著增强表征对齐能力，提升零样本任务性能。

在此基础上，我们进一步开发PE-A-Frame模型：通过对PE-AV进行帧级对比目标微调，实现了音频帧与文本的细粒度对齐，为声音事件检测等任务提供精准支持。 |
| 视觉感知链式思维：在统一模型中实现高保真视觉一致性 | Zixuan Ye | [PDF](https://arxiv.org/pdf/2512.19686v1) | 近期，思维链（CoT）的引入显著提升了统一模型的生成能力。然而，研究发现当前生成过程中的思维路径主要关注与文本提示的文本一致性，而忽视了多模态生成（例如多参考生成）中与视觉参考图像的**视觉上下文一致性**。这种一致性的缺失导致模型难以保持关键视觉特征（如人物身份、物体属性、风格等）。为此，我们将视觉上下文一致性融入统一模型的推理过程，通过以下方式显式激励模型维持这种一致性：1）**自适应视觉规划**：生成结构化视觉检查清单，以明确需要保持一致的视觉要素；2）**迭代视觉修正**：在检查清单指导下进行自我反思，并以迭代方式优化生成结果。为实现这一目标，我们采用监督微调方法，指导模型学习如何进行视觉检查规划、执行自我反思与优化，并利用流式GRPO算法通过定制化的视觉检查奖励机制进一步增强视觉一致性。实验表明，我们的方法在多模态生成任务中优于零样本统一模型及采用文本思维链的模型，展现出更高的视觉上下文一致性。 |
| 从视频中零样本重建场景内物体操作 | Dixuan Lin | [PDF](https://arxiv.org/pdf/2512.19684v1) | 我们构建了首个系统，旨在解决从单目RGB视频中重建场景内物体操控的问题。该任务面临多重挑战：场景重建本身具有不适定性，手部与物体的深度关系存在模糊性，且需保证交互过程符合物理规律。现有方法多基于手部中心坐标系进行操作，忽略了场景信息，导致度量精度受限且实用性不足。在我们的方法中，首先利用数据驱动的基础模型对核心组件进行初始化，包括物体网格与位姿、场景点云以及手部姿态。随后采用两阶段优化策略，完整重建从抓取到交互的手-物体运动轨迹，并确保其与输入视频中观察到的场景信息保持一致性。 |
| 从室内到开放世界：揭示多模态大语言模型的空间推理差距 | Mingrui Wu | [PDF](https://arxiv.org/pdf/2512.19683v1) | 尽管多模态大语言模型（MLLMs）在语义任务上已取得显著成果，但其空间智能——对于构建强健且具现实基础的AI系统至关重要——仍处于欠发达状态。现有基准测试在诊断这一局限方面存在不足：它们要么聚焦于过度简化的定性推理，要么依赖特定领域的室内数据，而缺乏具有可验证度量真值的户外数据集进一步制约了评估能力。为弥补这一空白，我们提出了一个基于行人视角视频构建的大规模基准测试，该视频数据通过同步立体相机、激光雷达及IMU/GPS传感器采集。该数据集提供精确度量的三维信息，支持自动生成涵盖层次化谱系的空间推理问题——从定性关系到定量度量乃至运动学理解。评估结果表明，在结构化室内基准测试中观察到的性能优势，在开放世界场景中消失殆尽。通过合成异常场景与遮蔽测试的进一步分析证实，当前MLLMs严重依赖语言先验而非基于现实的视觉推理。因此，我们的基准测试为诊断这些局限并推进具有物理基础的空间智能发展提供了系统性研究平台。 |
| GenEnv：大型语言模型代理与环境模拟器间的难度对齐协同进化 | Jiacheng Guo | [PDF](https://arxiv.org/pdf/2512.19682v1) | 训练高效的大型语言模型（LLM）智能体，目前面临的核心瓶颈在于现实交互数据的高成本与静态特性。为解决这一问题，我们提出GenEnv框架，通过在智能体与可扩展的生成式环境模拟器之间建立难度对齐的协同进化博弈机制。与传统基于静态数据集进化的方法不同，GenEnv实现了数据动态演化：模拟器作为动态课程策略，持续生成精准匹配智能体“最近发展区”的任务。这一过程由简洁高效的α-课程奖励机制引导，确保任务难度与智能体当前能力相匹配。我们在API-Bank、ALFWorld、BFCL、Bamboogle和TravelPlanner五个基准测试中评估GenEnv，结果显示：相较于70亿参数基线模型，智能体性能最高提升达**+40.3%**，并达到或超越更大规模模型的平均性能。与基于Gemini 2.5 Pro的离线数据增强方法相比，GenEnv在使用数据量减少3.3倍的情况下仍取得更优性能。通过从静态监督转向自适应模拟，GenEnv为扩展智能体能力提供了一条数据高效的进化路径。 |
| VA-$π$：面向像素感知自回归生成的变分策略对齐 | Xinyao Liao | [PDF](https://arxiv.org/pdf/2512.19680v1) | 自回归（AR）视觉生成模型依赖分词器将图像映射至离散序列并进行反向重建。然而，分词器的训练目标是从真实标记重建清晰图像，而AR生成器仅针对标记似然性进行优化。这种目标错位导致生成的标记序列可能解码为低质量图像，且缺乏像素空间的直接监督。我们提出VA-π——一种轻量级训练后优化框架，通过基于像素空间的原理性目标直接优化AR模型。

VA-π将生成器与分词器的对齐问题构建为变分优化，推导出统一像素重建与自回归建模的证据下界（ELBO）。为在离散标记空间中进行优化，VA-π引入基于强化的对齐策略：将AR生成器视为策略网络，以像素空间重建质量作为内在奖励。该奖励通过预测标记序列在教师强制模式下重建原始图像的质量进行度量，使模型获得直接的像素级指导，而无需昂贵的自由运行采样。ELBO的正则化项作为天然约束器，保持标记的分布一致性。

VA-π能够快速适配现有AR生成器，既无需重新训练分词器，也不依赖外部奖励模型。仅使用1%的ImageNet-1K数据及25分钟微调，即可在LlamaGen-XXL上将FID从14.36降至7.65，IS从86.55提升至116.70；在GenEval文本到图像任务中，视觉生成模型（LlamaGen：从0.306提升至0.339）与统一多模态模型（Janus-Pro：从0.725提升至0.744）均获得显著提升。代码已开源：https://github.com/Lil-Shake/VA-Pi。 |
| WorldWarp：基于异步视频扩散的三维几何传播 | Hanyang Kong | [PDF](https://arxiv.org/pdf/2512.19678v1) | 生成几何一致的长视频面临一个根本性困境：一致性要求在像素空间中严格遵循三维几何规律，而当前最先进的生成模型在相机条件化的潜空间中运行效果最佳。这种脱节导致现有方法在处理遮挡区域和复杂相机轨迹时存在困难。为弥合这一鸿沟，我们提出WorldWarp框架，该框架将三维结构锚点与二维生成优化器相结合。为实现几何基础，WorldWarp通过高斯泼溅（3DGS）技术维护在线三维几何缓存。通过将历史内容显式变形映射到新视角，该缓存充当结构支架，确保每个新帧都遵循先验几何关系。然而，静态变形映射不可避免地会因遮挡产生空洞和伪影。我们采用专为"填充-修正"目标设计的时空扩散（ST-Diff）模型解决此问题。我们的核心创新是时空动态噪声调度机制：空白区域接收完整噪声以触发生成，而变形映射区域接收部分噪声以实现精细化调整。通过每一步动态更新三维缓存，WorldWarp在视频片段间保持连贯性。最终，该框架通过三维逻辑引导结构、扩散逻辑完善纹理的方式，实现了业界领先的生成保真度。项目页面：\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}。 |
