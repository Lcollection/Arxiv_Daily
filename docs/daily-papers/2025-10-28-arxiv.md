# arxiv 2025-10-28

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 协奏曲：联合2D-3D自监督学习涌现空间表征

（注：译文采用学术术语对译原则，其中"Concerto"保留音乐隐喻译为"协奏曲"，体现多模态协同理念；"Joint 2D-3D Self-Supervised Learning"准确对应"联合2D-3D自监督学习"，明确技术范式；"Emerges Spatial Representations"译为"涌现空间表征"，既符合复杂系统理论中的涌现特性表述，又精准传达空间表征的自生成特性。） | Yujia Zhang | [PDF](http://arxiv.org/pdf/2510.23607v1) | 人类通过多感官协同学习抽象概念，一旦形成概念表征，通常仅凭单一模态即可唤起。受此原理启发，我们提出Concerto——一个极简主义的人类空间认知概念学习模拟系统，融合三维模态内自蒸馏与二维-三维跨模态联合嵌入。尽管架构简洁，Concerto通过零样本可视化证明其可学习到更连贯、信息量更丰富的空间特征。在三维场景感知的线性探测任务中，其性能分别超越独立训练的二维和三维自监督最优模型14.2%与4.8%，同时优于它们的特征拼接方案。经全参数微调后，Concerto在多个场景理解基准测试中刷新纪录（例如在ScanNet上达到80.7% mIoU）。我们还开发了面向视频提取点云空间理解的Concerto变体，以及将Concerto表征线性投射至CLIP语言空间的转换器，从而实现开放世界感知。这些成果充分证明，Concerto孕育出的空间表征具有卓越的细粒度几何与语义一致性。 |
| 变分掩码扩散模型 | Yichi Zhang | [PDF](http://arxiv.org/pdf/2510.23606v1) | 掩码扩散模型近期已成为离散生成建模的灵活框架。然而，标准掩码扩散存在一个关键局限：当需要建模符号间依赖关系时，其无法有效捕捉并行预测符号间的相互关联，导致生成质量下降。为显式建模符号间依赖关系，我们提出变分掩码扩散（VMD），该框架通过引入潜变量改进掩码扩散过程。在合成数据集上的受控实验表明，VMD能成功学习传统掩码扩散无法捕捉的依赖关系。我们进一步在数独谜题和文本数据集上验证方法的有效性——通过学习符号间依赖关系，模型显著提升了生成内容的全局一致性。跨领域实验证明，VMD在提升生成质量与依赖关系建模能力方面均具有优势，这彰显了将变分推断融入掩码扩散的重要价值。代码已开源：https://riccizz.github.io/VMD。 |
| 轨迹追踪、智能修复、重映射渲染：基于渐进式纹理填充的主体驱动三维与四维生成技术

（注：采用"轨迹追踪"对应Track体现运动轨迹捕捉，"智能修复"对应Inpaint强调AI辅助修复特性，"重映射渲染"对应Resplat突显纹理重映射技术；通过"主体驱动"准确传达Subject-driven的核心理念，"渐进式纹理填充"精准对应Progressive Texture Infilling的技术实现路径，完整保留专业术语的同时确保中文表达符合学术规范） | Shuhong Zheng | [PDF](http://arxiv.org/pdf/2510.23605v1) | 当前的三维/四维生成方法通常以照片真实感、效率与美学表现为优化目标，但往往难以在不同视角下保持主体的语义一致性。基于特定主体的单张或少量图像对生成方法进行适配（即个性化或主体驱动生成），可实现与主体身份特征相符的视觉内容生成。然而，个性化三维/四维生成领域仍存在大量探索空间。本研究提出TIRE（轨迹追踪-修复-重映射）这一创新性主体驱动三维/四维生成方法：以前沿三维生成模型输出的初始三维资产为输入，通过视频追踪技术定位需修改区域；采用主体驱动的二维修复模型对目标区域进行渐进式填充；最终将优化后的二维多视角观测数据重映射至三维空间并保持全局一致性。大量实验表明，相较于现有最优方法，本方案在三维/四维生成的身份特征保持方面实现显著提升。项目网站详见：https://zsh2000.github.io/track-inpaint-resplat.github.io/。 |
| 像素参考：一种支持任意粒度的时空目标指代统一框架 | Yuqian Yuan | [PDF](http://arxiv.org/pdf/2510.23603v1) | 多模态大语言模型（MLLMs）在开放世界视觉理解中展现出强大的通用能力。然而，现有模型主要侧重于整体场景级理解，往往忽视细粒度、以对象为中心的推理需求。本文提出PixelRefer——一个统一的区域级多模态大语言模型框架，能够对图像和视频中用户指定区域实现先进的细粒度理解。基于大语言模型注意力主要聚焦于对象级标记的观察，我们提出尺度自适应对象标记器（SAOT），从自由形式区域生成紧凑且语义丰富的对象表征。分析表明全局视觉标记主要在大语言模型浅层发挥作用，由此设计出高效变体PixelRefer-Lite：通过对象中心注入模块将全局上下文预融合至对象标记，形成轻量化的纯对象框架，在保持高语义保真度的同时显著降低计算成本。为促进细粒度指令调优，我们构建了包含220万样本的高质量对象中心指令数据集PixelRefer-2.2M。在多个基准测试中的广泛实验表明，PixelRefer以更少训练样本实现领先性能，而PixelRefer-Lite在保持竞争力的准确度同时展现出显著的效率优势。 |
| 阿丽塔-G：用于智能体生成的自我进化生成式智能体 | Jiahao Qiu | [PDF](http://arxiv.org/pdf/2510.23601v1) | 研究表明，当大型语言模型（LLMs）被构建成具备记忆、工具和反馈机制的智能体时，其性能表现更优。在此基础上，自我进化智能体已崭露头角，但当前研究主要将适应能力局限于提示词重写或失败重试。为此，我们提出ALITA-G框架——通过系统化生成、抽象化处理与精编模型上下文协议（MCP）工具，将通用智能体转化为领域专家的自我进化系统。该框架使通用智能体执行目标领域任务集，并从成功轨迹中合成候选MCP工具，进而将其抽象为参数化基元并整合至MCP工具箱。在推理阶段，ALITA-G首先借助工具描述与用例进行检索增强的MCP选择，随后启动配备MCP执行器的智能体。在GAIA、PathVQA和Humanity's Last Exam等多个基准测试中，ALITA-G在降低计算成本的同时取得显著性能提升。在GAIA验证集上实现83.03%的pass@1和89.09%的pass@3，创下最新技术标杆，且单样本平均令牌消耗量较基线智能体降低约15%。ALITA-G由此建立了从通用能力到可复用领域专长的规范化路径，在提升复杂推理任务准确性的同时显著增强运算效率。 |
| 审慎思考：分支与反思推理奖励模型 | Yizhu Jiao | [PDF](http://arxiv.org/pdf/2510.23596v1) | 大型语言模型（LLM）日益依赖显化中间步骤并分配额外测试时间计算资源的思维模型，其中“三思策略”表明审慎的二次处理能够激发更强的推理能力。相比之下，大多数奖励模型（RM）仍将多重质量维度一次性压缩为单一标量值，这种设计会导致判断力扩散现象：注意力分散在多个评估标准上，造成焦点稀释和浅层分析。我们提出分支再思奖励模型（BR-RM），这种双轮次模型将“三思原则”迁移至奖励建模领域。首轮执行自适应分支，筛选出实例关键维度（如事实性与安全性）并构建简洁的循证假设；次轮实施分支条件再思考，通过定向重读检验这些假设，仅聚焦最关键要素。我们采用GRPO风格强化学习，基于结构化双轮次轨迹进行训练，配合带有严格格式校验的二元结果奖励，使该方法能与标准RLHF流程兼容。通过将一次性评分转化为聚焦的二次推理，BR-RM在保持实用性与可扩展性的同时，有效抑制判断力扩散，提升对细微但关键错误的检测灵敏度。实验结果表明，我们的模型在跨领域的三项高难度奖励模型基准测试中均达到最先进性能。代码与模型即将发布。 |
| 多智能体协同进化：基于共同演化的语言模型自我优化 | Yixing Chen | [PDF](http://arxiv.org/pdf/2510.23595v1) | 强化学习（RL）在提升大语言模型（LLMs）的推理能力方面展现出显著潜力。然而，现有基于强化学习的方法严重依赖人工标注数据集和可验证的奖励机制，这限制了其扩展性与泛化能力。受博弈场景中自我对弈范式成功的启发，近期提出的自我对弈强化学习方法试图在不依赖人工标注数据的情况下增强LLM的推理能力。但这类方法主要依赖具身环境反馈（如Python解释器或游戏引擎），将其扩展至通用领域仍面临挑战。

为解决这些问题，我们提出多智能体进化框架（MAE），该框架使LLMs能够在数学计算、逻辑推理和通用知识问答等多样化任务中实现自主进化。MAE的核心设计基于从单一LLM实例化的智能体三元组（提议者、求解者、评判者），通过强化学习优化其行为：提议者生成问题，求解者尝试解决，评判者则在协同进化过程中对双方进行评估。基于Qwen2.5-3B-Instruct模型的实验表明，MAE在多个基准测试中平均提升4.54%。这些结果证明，MAE是一种可扩展、数据高效的方法，能够在最小化人工监督依赖的前提下，持续增强LLMs的通用推理能力。 |
| PRISM-Bench：基于谜题的视觉任务基准与思维链错误检测

（该翻译采用学术术语直译与意译结合的方式："Puzzle-Based Visual Tasks"译为"基于谜题的视觉任务"以保持专业准确性；"CoT"作为认知科学领域常用缩写，扩展为完整术语"思维链"并保留其专业内涵；"Benchmark"译为"基准"符合计算机学科惯例；整体采用破折号连接主副标题，符合中文科技文献标题规范。） | Yusu Qian | [PDF](http://arxiv.org/pdf/2510.23594v1) | 我们推出\textbf{PRISM-Bench}基准测试——一套基于谜题的视觉挑战评估体系，其设计目标不仅是检验模型能否解决问题，更要揭示模型的推理过程。与仅衡量最终答案准确度的传统评估不同，PRISM-Bench引入诊断性任务：给定一个视觉谜题和包含恰好一处错误的逐步思维链（CoT），模型必须定位首个错误步骤。这种设置能对逻辑一致性、错误检测能力和视觉推理进行细粒度评估。该基准中的谜题需要多步骤符号推理、几何推理与类比推理，有效规避基于表面模式匹配的捷径解法。对前沿多模态大语言模型（MLLM）的评估表明：流畅生成与可靠推理之间存在持续差距——能生成合理思维链的模型往往无法定位简单的逻辑谬误。通过将答案生成与推理验证解耦，PRISM-Bench为评估多模态推理能力提供了更精准的观测视角，并凸显了在开发可信MLLMs过程中实施诊断性评估范式的必要性。 |
| 轻量级鲁棒直接偏好优化 | Cheol Woo Kim | [PDF](http://arxiv.org/pdf/2510.23590v1) | 直接偏好优化（DPO）因其稳定性和简洁性，已成为微调大语言模型（LLM）的常用方法。然而该方法也存在对数据噪声敏感且易过拟合的缺陷。近期研究提出采用分布鲁棒优化（DRO）来应对数据中潜在的噪声和分布偏移，但这类方法往往存在过度保守和计算成本过高的问题。我们提出DPO-PRO（具有偏好鲁棒性的DPO），这是一种基于DPO的鲁棒微调算法，通过轻量级DRO框架解决偏好分布的不确定性。与现有基于DRO的改进方法不同，DPO-PRO仅聚焦于偏好不确定性，既避免了不必要的保守倾向，又实现了可忽略的计算开销。我们进一步证明DPO-PRO等效于一个正则化的DPO目标函数，该函数能对弱偏好信号下模型的过度自信进行惩罚。我们在标准对齐基准测试和真实世界公共卫生任务上评估DPO-PRO，实验结果表明：相较于现有DPO改进方法，我们的方案能持续提升对噪声偏好信号的鲁棒性。 |
| InFlux：视频相机动态内参自校准基准测试集 | Erich Liang | [PDF](http://arxiv.org/pdf/2510.23589v1) | 精确追踪相机内参是实现从二维视频理解三维空间的关键。然而多数三维算法假设相机内参在视频中保持恒定，这一假设在现实场景的野生视频中往往不成立。该领域面临的主要障碍是缺乏动态相机内参基准数据集——现有基准数据集通常在场景内容和内参变化方面多样性有限，且均未提供连续视频帧的逐帧内参变化。本文提出动态内参基准数据集InFlux，为具有动态内参的视频提供逐帧真实内参标注。相较于现有基准，InFlux捕捉了更广泛的内参变化和场景多样性，包含386个具有动态相机内参的高清室内外视频，标注帧数超过14.3万帧。为确保逐帧内参的精确性，我们构建了完整的校准实验对照表，并扩展Kalibr工具箱以提升其精度与鲁棒性。基于本基准数据集，我们对现有相机内参预测基线方法进行评估，发现大多数方法难以在动态内参视频中实现准确预测。数据集、代码、视频及提交入口请访问：https://influx.cs.princeton.edu/。 |
