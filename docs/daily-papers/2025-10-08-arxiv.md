# arxiv 2025-10-08

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| Human3R：无处不在的全体人类 | Yue Chen | [PDF](http://arxiv.org/pdf/2510.06219v1) | 我们提出Human3R——一个基于世界坐标系、从随意拍摄的单目视频中实现在线4D人-场景重建的统一前馈框架。与依赖多阶段流程、人-场景迭代接触优化及强依赖性预处理（如人体检测、深度估计和SLAM）的现有方法不同，Human3R通过单次前向传播即可联合重建全局多人SMPL-X人体模型（“所有人”）、稠密3D场景（“全场景”）及相机轨迹（“一次性完成”）。本方法基于4D在线重建模型CUT3R构建，采用参数高效的视觉提示调优技术，在保持CUT3R丰富时空先验的同时，实现了对多个SMPL-X人体的直接读取。Human3R作为统一模型，摒弃了强依赖性与迭代优化流程：仅需在单GPU上使用小规模合成数据集BEDLAM训练一日，即可实现卓越性能——以前馈方式实时（15帧/秒）单阶段重建多人三维人体与场景，且内存占用仅8GB。大量实验表明，该统一模型在全局人体运动估计、局部人体网格重建、视频深度估计和相机位姿估计等任务中均达到领先或具有竞争力的性能。我们期待Human3R能成为简洁而强大的基准模型，并易于拓展至下游应用。代码已发布于https://fanegg.github.io/Human3R。 |
| 《EgoNight：基于挑战性基准的夜间第一人称视角视觉理解研究》

（注：译文通过"第一人称视角"准确传达"Egocentric"的学术内涵，"挑战性基准"对应"Challenging Benchmark"的技术概念，采用"研究"作为"Towards"的动态学术表述，完整保留原标题的技术层级与探索性质） | Deheng Zhang | [PDF](http://arxiv.org/pdf/2510.06218v1) | 现有大多数以自我为中心视觉理解的基准测试主要聚焦于白天场景，忽视了实际应用中不可避免的低光照条件。为探究这一空白，我们推出首个夜间以自我为中心视觉综合基准EgoNight，其核心任务为视觉问答（VQA）。该基准的关键特性是引入昼夜对齐视频——通过利用白天数据提升夜间标注质量，并清晰揭示不同光照条件下的性能差异。为实现这一目标，我们同时采集了Blender渲染的合成视频与现实世界录制视频，确保场景与动作在视觉和时序上严格对应。基于这些配对视频，我们构建了EgoNight-VQA数据集，其采用创新的日间增强夜间自动标注引擎，并经过大量人工校验优化。每个问答对均经过标注员双重核查以确保可靠性。EgoNight-VQA总计包含90段视频中的3658组问答对，涵盖12种多样化问答类型，累计投入超过300小时人工工作量。对当前最先进多模态大语言模型（MLLMs）的评估显示，从白天到夜晚的迁移会出现显著性能下降，这凸显了低光照条件下推理面临的挑战。除视觉问答外，EgoNight还引入昼夜对应检索与夜间以自我为中心深度估计两个辅助任务，进一步探索现有模型的性能边界。我们相信EgoNight-VQA将为推进应用驱动的以自我为中心视觉研究、开发跨光照域泛化模型奠定坚实基础。所有数据与代码将在论文录用后公开。 |
| TaTToo：基于工具化思维的测试时扩展参数化推理模型在表格推理中的应用 | Jiaru Zou | [PDF](http://arxiv.org/pdf/2510.06217v1) | 过程奖励模型（PRMs）近期已成为增强大型推理模型（LRMs）推理能力的重要框架，尤其在测试时扩展（TTS）背景下表现突出。然而，该类模型在表格推理领域监督LRMs的潜力尚未得到充分探索。通过详尽的实证分析，我们发现现有PRMs虽已广泛用于监督纯文本推理步骤，但在处理表格特有操作（如子表检索与模式交互）时存在明显不足，导致关键性能瓶颈。为突破此局限，我们提出TaTToo——一种新型表格锚定PRM框架，其具备两大特性：（i）对表格推理步骤进行显式推理；（ii）集成基于工具的验证以提供精确奖励监督。具体而言，我们首先设计了可扩展的数据构建流程，通过融合表格验证原理与工具化执行，创建了包含超6万条步骤级标注的高质量数据集。基于此数据，我们采用双阶段范式训练TaTToo：通过冷启动监督微调捕捉工具使用推理模式，继而采用工具锚定奖励塑形的强化学习，使模型与基于表格的验证对齐。我们针对新设计PRM引发的策略改进进行了全面评估。在涵盖数值推理、事实核查与数据分析的5个挑战性表格推理基准测试中，TaTToo使下游策略LRMs的推理性能提升30.9%，仅以80亿参数即超越Qwen-2.5-Math-PRM-720B等强基线PRM，并在多样化TTS策略中展现出强大泛化能力。 |
| 舍弃深度传感器：无需深度传感器的RGB-D SLAM技术

（注：标题翻译采用意译策略，在保留核心概念"RGB-D SLAM"和"无需深度传感器"的基础上，将"Dropping the D"译为"舍弃深度传感器"以准确传达技术内涵。RGB-D SLAM作为专业术语保持原格式，指代同时使用彩色图像和深度信息的同步定位与地图构建技术。） | Mert Kiray | [PDF](http://arxiv.org/pdf/2510.06216v1) | 我们提出DropD-SLAM——一种不依赖深度传感器即可实现RGB-D级别精度的实时单目SLAM系统。该系统通过三个预训练视觉模块替代主动深度输入：单目度量深度估计器、学习型关键点检测器及实例分割网络。采用扩张实例掩码抑制动态物体，同时为静态关键点分配预测深度值并反投影至三维空间形成度量尺度特征。这些特征经由未经修改的RGB-D SLAM后端进行处理以实现追踪与建图。在TUM RGB-D基准测试中，DropD-SLAM在静态序列上达到7.4厘米平均绝对轨迹误差，动态序列上达1.8厘米，在单GPU上以22帧/秒运行时性能匹配或超越当前最优RGB-D方法。这些结果表明，现代预训练视觉模型能够作为可靠实时的度量尺度源替代主动深度传感器，标志着SLAM系统向更简洁、更经济实用方向迈出重要一步。 |
| 生成式图像模型的精细离焦模糊控制 | Ayush Shrivastava | [PDF](http://arxiv.org/pdf/2510.06215v1) | 当前基于扩散模型的文生图技术虽能生成多样化高质量图像，但在融入细粒度相机元数据（如精确光圈参数）方面仍存在局限。本研究提出一种创新的文生图扩散框架，通过利用图像文件中常嵌入的相机元数据（EXIF数据），重点实现可控镜头虚化效果。我们的方法模拟物理成像过程：首先生成全焦点图像，估计其单眼深度信息，通过新型焦距变换器预测合理对焦距离，继而利用现有可微分镜头虚化模型生成散焦图像。梯度在整个过程中实现反向传播，使我们无需显式监督即可根据内容元素与EXIF数据学习生成虚化效果。在推理阶段，该方法可在保持场景内容的前提下实现精确的交互式虚化控制，这一特性是现有扩散模型无法实现的。实验结果表明，我们的模型在保持场景内容不变的前提下，实现了更优越的细粒度控制能力。 |
| 分层GRPO：处理大型语言模型搜索代理强化学习中的结构异质性问题

（注：GRPO指"Guided Reinforcement Policy Optimization"，译为"引导式强化策略优化"。该翻译在保持专业术语准确性的同时，采用"分层"对应"Stratified"，"结构异质性"对应"Structural Heterogeneity"，并通过增补"问题"二字使中文表达更符合学术语境。） | Mingkang Zhu | [PDF](http://arxiv.org/pdf/2510.06214v1) | 大型语言模型（LLM）智能体日益依赖搜索引擎等外部工具来解决复杂的多步骤问题，而强化学习（RL）已成为训练此类智能体的关键范式。然而，搜索智能体的轨迹在结构上具有异质性——搜索调用的数量、位置和结果差异会导致根本不同的答案方向与奖励分布。采用单一全局基线的标准策略梯度方法存在我们识别并形式化的“跨层级偏差”问题，即对异质轨迹进行“苹果与橘子”式的错误比较。这种跨层级偏差会扭曲信用分配，阻碍对复杂多步骤搜索策略的探索。

为解决该问题，我们提出分层GRPO方法，其核心组件“分层优势归一化”（SAN）根据轨迹的结构特性将其划分为同质子集，并在每个子集内局部计算优势值。这确保轨迹仅与其真实同类进行比较。理论分析证明：SAN能消除跨层级偏差，在各子集内产生条件无偏的单位方差估计，同时保持标准归一化方法具备的全局无偏性与单位方差特性，从而获得更纯净、尺度更稳定的学习信号。为提升有限样本场景下的实际稳定性，我们进一步将SAN与全局估计量进行线性融合。

在多样化的单跳与多跳问答基准测试中，大量实验表明分层GRPO始终以最高11.3个百分点的优势显著超越GRPO，实现了更高的训练奖励、更强的训练稳定性及更有效的搜索策略。这些结果确立了分层法作为解决LLM搜索智能体强化学习中结构异质性问题的原理性方案。 |
| 训练动态影响训练后量化鲁棒性 | Albert Catalan-Tatjer | [PDF](http://arxiv.org/pdf/2510.06213v1) | 尽管训练后量化技术被广泛采用以实现大语言模型的高效部署，但其量化鲁棒性的内在机制仍不明确。我们通过对开源语言模型训练轨迹（最大规模达320亿参数和15万亿训练词元）进行量化性能退化的系统性分析，精确评估训练动态与量化性能之间的关联。核心发现表明：大规模训练中的量化误差由学习率与其他训练超参数之间的复杂相互作用驱动。具体而言，当学习率开始衰减时，验证损失与量化误差会出现背离现象，且该现象与训练数据规模基本无关。为探究训练动态的干预方案并确定可优化量化鲁棒性的特定配置，我们通过受控实验训练了最高达1000亿词元的自建模型。研究结果挑战了“扩大数据规模必然损害量化效果”的固有认知，实证表明通过策略性调整训练超参数，能够在扩大规模时同步提升量化质量。 |
| 驱动与生成：端到端驾驶与视频生成模型的协同评估

该标题的翻译要点解析：
1. Drive&Gen采用意译法，拆解为“驱动”与“生成”两个核心动作
2. Co-Evaluating译为“协同评估”，体现模型间的交互验证关系
3. End-to-End保持专业术语一致性，译为“端到端”
4. 通过冒号结构保持原标题的学术表述特征，完整呈现“方法-对象”的逻辑关系
5. 使用四字格“驱动与生成”形成对仗结构，符合中文科技文献的标题美学要求 | Jiahao Wang | [PDF](http://arxiv.org/pdf/2510.06209v1) | 生成模型的近期突破为自动驾驶领域带来了令人振奋的新可能。具体而言，视频生成模型正被探索作为可控虚拟测试环境。与此同时，端到端驾驶模型作为传统模块化自动驾驶系统的精简替代方案，因其简洁性与可扩展性日益受到关注。然而这些技术在仿真与规划中的应用引发重要问题：首先，尽管视频生成模型能生成愈发逼真的视频，但这些生成内容能否严格遵循预设条件，并具备足够真实性以评估端到端自动驾驶规划器？其次，鉴于数据对理解和控制端到端规划器至关重要，我们该如何深入洞察其认知偏差，并提升其在分布外场景中的泛化能力？本研究通过构建驾驶模型与生成式世界模型之间的桥梁来解答这些问题。我们提出利用端到端驾驶模型评估生成视频真实性的新型统计度量方法，借助视频生成模型的可控性开展定向实验，探究影响端到端规划器性能的分布差距。最终我们证明，视频生成模型产生的合成数据可作为现实数据采集的经济替代方案，这种合成数据能有效增强端到端模型在既有运行设计域之外的泛化能力，为拓展自动驾驶服务至新型运营场景提供支持。 |
| ShapeGen4D：实现基于视频的高质量四维形状生成 | Jiraphon Yenphraphai | [PDF](http://arxiv.org/pdf/2510.06208v1) | 视频条件化4维形状生成旨在直接从输入视频中恢复时变三维几何结构与视角一致的外观表征。本研究提出了一种原生视频到4维形状生成框架，能够端到端地从视频中合成单一动态三维表征。基于大规模预训练三维模型，我们的框架引入了三个核心组件：（i）时序注意力机制，在生成过程中关联所有视频帧信息，同时产生时间索引的动态表征；（ii）时序感知点采样与四维潜在锚定技术，有效提升几何结构与纹理的时间一致性；（iii）跨帧噪声共享策略，显著增强时序稳定性。本方法无需逐帧优化即可精确捕捉非刚性运动、体积变化乃至拓扑结构转换。在多样化真实场景视频测试中，相较于基线方法，本方案在鲁棒性、感知保真度方面表现更优，同时显著减少了失效模式的发生。 |
| 基于可微分数字信号处理的调制发现 | Christopher Mitcheltree | [PDF](http://arxiv.org/pdf/2510.06204v1) | 调制是声音设计与音乐制作的核心环节，能够创造复杂且动态变化的音频。现代合成器配备包络发生器、低频振荡器（LFO）及更多参数自动化工具，使用户能够轻松调制输出信号。然而，辨识用于塑造声音的调制信号具有挑战性——现有的声音匹配/参数估计系统往往呈现为不可解读的黑箱，或仅预测高维度的帧级参数数值，却忽略了底层调制曲线的形态、结构与路由路径。我们提出一种神经声音匹配方法，通过调制提取、约束化控制信号参数化与可微分数字信号处理（DDSP）技术来解析声音中的调制信息。实验证明，该方法在强调制合成音频与真实音频样本中均表现优异，可适配不同DDSP合成器架构，并在可解释性与声音匹配精度之间实现平衡。我们公开了代码与音频样本，并通过VST插件形式提供训练完成的DDSP合成器。 |
