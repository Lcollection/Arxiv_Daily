# arxiv 2025-11-20

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 在有限字母表上的词元化是困难的 | Violeta Kastreva | [PDF](https://arxiv.org/pdf/2511.15709v1) | 近期研究表明，词元化问题具有NP完全性。然而这些研究均假设词元化处理的输入字符集规模无界——这一假设并不符合实际，因为实践中词元化器通常基于固定规模的字符集（如字节或Unicode字符）进行操作。我们通过分析有界$n$元字符集上的词元化问题来填补这一空白，重点考察两种自然变体：自底向上词元化与直接词元化。前者需要选择最优的合并操作序列，后者则需要选择能最优压缩数据集的词表。首先我们指出，对$n$元字符集证明的难度结论可直接推广至更大规模的字符集。随后我们证明即使对于二元字符集，两种变体不仅具有NP完全性，而且不存在多项式时间近似方案（除非P=NP）。我们进一步证明直接词元化在单元字符集上仍保持NP完全性。虽然单元字符集在实际中可能缺乏应用价值，但该结论表明词元化的计算难解性并非源于大字符集或复杂构造，而是其固有特性。总体而言，我们的研究解释了为何BPE和UnigramLM等实用算法均采用启发式策略，并指出近似算法将成为词元化研究的重要发展方向。 |
| RoMa v2：更强大、更精准、更快速、更密集的特征匹配 | Johan Edstedt | [PDF](https://arxiv.org/pdf/2511.15706v1) | 稠密特征匹配旨在估计三维场景中两幅图像之间的所有对应关系，因其高精度与强鲁棒性已成为当前黄金标准。然而现有稠密匹配器在诸多复杂现实场景中仍存在失效或性能不佳的问题，且高精度模型往往速度迟缓，限制了实际应用。本文通过一系列系统性改进多方位攻克这些缺陷，最终构建出性能显著提升的新模型。我们特别设计了新型匹配架构与损失函数，结合精心构建的多样化训练数据分布，使模型能够应对多种复杂匹配任务。通过解耦的“先匹配后优化”两阶段流程，我们既加速了训练过程，又借助定制CUDA内核显著降低了优化阶段的内存占用。此外，我们融合DINOv3基础模型及多项创新洞见，有效增强模型鲁棒性并减少偏差。大量实验表明，这一新型匹配器创造了最新技术标杆，其精度显著超越前人工作。代码已开源：https://github.com/Parskatt/romav2 |
| GeoVista：面向地理定位的Web增强型智能视觉推理系统

该翻译方案具有以下特点：
1. 专业术语精准对应：
   - "Agentic"译为"智能"体现自主决策能力
   - "Visual Reasoning"译为"视觉推理"符合计算机视觉领域术语
   - "Geolocalization"译为"地理定位"保持测绘学专业表述

2. 技术概念完整传达：
   - "Web-Augmented"译为"Web增强型"准确表达网络数据增强特性
   - 系统名称"GeoVista"保留原格式不翻译
   - 整体采用"系统"作为补充说明词，符合中文技术命名习惯

3. 学术表述规范：
   - 使用冒号分隔主副标题
   - 采用四字格"智能视觉"保持术语紧凑性
   - 动词"面向"准确体现技术应用方向

该译名既保持了学术翻译的准确性，又符合中文技术文献的命名规范，能有效传达原技术通过Web数据增强实现智能地理定位的核心创新点。 | Yikun Wang | [PDF](https://arxiv.org/pdf/2511.15705v1) | 当前关于具身视觉推理的研究虽能实现深度多模态理解，但主要聚焦于图像编辑工具，尚未拓展至通用型具身模型。本研究重新审视地理定位任务——该任务不仅需要精细的视觉定位能力，还需在推理过程中通过网页搜索验证或修正假设。鉴于现有地理定位基准数据集无法满足高分辨率图像需求及深度具身推理的定位挑战，我们构建了GeoBench基准数据集，包含全球范围的普通照片与全景图像，以及不同城市的卫星图像子集，以系统评估具身模型的地理定位能力。同时，我们提出GeoVista模型，该具身模型将工具调用无缝集成于推理循环中，包含用于放大关注区域的图像缩放工具和检索网络信息的网页搜索工具。我们为其开发了完整训练流程：首先通过冷启动监督微调阶段学习推理模式与工具使用先验，继而通过强化学习阶段进一步提升推理能力。通过采用分层奖励机制有效利用多层级地理信息，显著提升了整体地理定位性能。实验结果表明，GeoVista在地理定位任务上大幅超越其他开源具身模型，在多数指标上达到与Gemini-2.5-flash、GPT-5等闭源模型相当的性能水平。 |
| In-N-On：利用真实场景与任务数据扩展自我中心操作能力

该标题精准传达了以下学术内涵：
1. In-N-On 指代三重维度：
   - In：真实环境数据（in-the-wild）
   - N：连接符号，体现数据融合
   - On：任务导向数据（on-task）
2. "Scaling Egocentric Manipulation" 译为"扩展自我中心操作能力"，既保持计算机视觉领域术语规范，又体现技术突破性
3. 通过"利用"二字明确方法论，突出通过多源数据驱动技术发展的研究路径
4. "真实场景"与"任务数据"形成方法论互补，完整覆盖机器人操作的两大关键数据来源

该翻译严格遵循学术翻译的准确性、专业性与简洁性原则，符合人工智能与机器人领域的中文表达规范。 | Xiongyi Cai | [PDF](https://arxiv.org/pdf/2511.15704v1) | 以自我为中心拍摄的视频是学习操作策略的宝贵且可扩展的数据源。然而，由于显著的数据异质性，现有方法大多仅将人类数据用于简单预训练，未能充分发挥其潜力。本文首先提出一种可扩展的数据收集与使用方法，将人类数据分为"非任务场景"与"任务场景"两类，并系统分析数据使用策略。我们构建了包含1,000+小时多样化非任务场景自我中心数据与20+小时直接匹配目标操作任务数据的PHSD数据集。基于此训练出大规模自我中心语言条件流匹配策略Human0，通过领域自适应技术缩小人类与人形机器人之间的差距。实验证明，Human0通过扩展人类数据获得了多项新颖特性：仅凭人类数据即可实现指令跟随、具备小样本学习能力，并利用任务场景数据提升系统鲁棒性。项目网站：https://xiongyicai.github.io/In-N-On/ |
| 视觉思考，文本推理：ARC挑战中的视觉与语言协同 | Beichen Zhang | [PDF](https://arxiv.org/pdf/2511.15703v1) | 从极简示例中实现抽象推理，仍是GPT-5与Grok-4等前沿基础模型尚未解决的核心难题。现有模型仍难以从少量示例中推断结构化转换规则——这正是人类智能的关键特征。面向通用人工智能的抽象与推理语料库（ARC-AGI）为此能力提供了严格测试基准，要求实现概念规则归纳并向新任务迁移。当前多数方法将ARC-AGI视为纯文本推理任务，却忽略了人类在解决此类难题时高度依赖视觉抽象的特性。然而我们的初步实验揭示了一个悖论：简单地将ARC-AGI网格转换为图像会因规则执行不精确而导致性能下降。由此我们提出核心假设：视觉与语言在不同推理阶段具有互补优势——视觉擅长全局模式抽象与验证，语言专精符号化规则表述与精确执行。基于此洞见，我们引入两种协同策略：（1）视觉-语言协同推理（VLSR），将ARC-AGI分解为模态对齐的子任务；（2）模态切换自校正（MSSC），利用视觉验证文本推理以实现内在误差修正。大量实验表明，该方法在多种旗舰模型和多项ARC-AGI任务中相较纯文本基线最高提升4.33%。我们的研究结果表明，将视觉抽象与语言推理相统一，是未来基础模型实现可泛化、类人智能的关键步骤。源代码即将发布。 |
| 首选帧：视频内容定制的关键所在 | Jingxi Chen | [PDF](https://arxiv.org/pdf/2511.15700v1) | 首帧在视频生成模型中扮演何种角色？传统观点将其视为视频的时空起点，仅仅是后续动画的种子帧。本研究揭示了截然不同的视角：视频模型隐式地将首帧作为概念记忆缓冲区，存储视觉实体以供生成过程中重复调用。基于这一洞见，我们证明仅需20-50个训练样本，无需调整模型架构或进行大规模微调，即可在多样化场景中实现鲁棒且泛化的视频内容定制。这一发现揭示了视频生成模型基于参考内容进行视频定制的强大而长期被忽视的能力。 |
| 面向符号通信的联合语义-信道编码与调制 | Jingkai Ying | [PDF](https://arxiv.org/pdf/2511.15699v1) | 近年来，Transformer架构在众多任务与模态中取得了卓越性能。Token作为基于Transformer模型的统一输入输出表示，已成为基础信息单元。本文研究token通信问题，探索如何高效可靠地传输token。我们选择点云作为信源——这种主流三维数据格式相较于图像或视频具有更复杂的空间结构。通过集合抽象方法获取点云token后，为构建更具信息量且利于传输的token表征，我们提出联合语义信道与调制（JSCCM）方案用于token编码器，将点云token映射为标准数字星座点（调制token）。具体而言，JSCCM包含两个并行的基于Point Transformer的编码器，以及结合Gumbel-softmax与软量化方法的差分调制器。此外，开发了速率分配器与信道适配器，可根据语义信息和信道状态自适应生成高质量调制token。大量实验表明，所提方法在重建质量上较联合语义信道编码与传统分离编码提升超过1dB，调制符号压缩比达到6倍以上。 |
| 救援透镜：基于大语言模型的志愿者反馈分类与行动系统在食物救援中的应用 | Naveen Raman | [PDF](https://arxiv.org/pdf/2511.15698v1) | 食品救援组织通过协调志愿者，将捐赠方过剩的食品重新分配给需要的人群，从而同步应对食品不安全与浪费问题。志愿者反馈机制能帮助组织及时发现问题并确保志愿者满意度。然而当前组织仍采用人工方式处理反馈，这种方式既繁琐又耗时，导致难以有效识别需优先解决的关键问题。本研究探索如何利用大语言模型帮助食品救援组织者理解志愿者体验并采取相应行动。我们与位于宾夕法尼亚州匹兹堡的大型食品救援组织412 Food Rescue合作，开发了基于大语言模型的RescueLens工具。该工具能自动分类志愿者反馈、建议需跟进联系的捐赠方与受助方，并根据反馈更新志愿者指引。通过在标注数据集上的评估，RescueLens可召回96%的志愿者问题，精确率达71%。更重要的是，通过按问题发生率对相关方进行排序，该工具可帮助组织者聚焦0.5%的捐赠方——这些捐赠方引发了超过30%的志愿者问题。目前RescueLens已在412 Food Rescue投入实际应用，根据对组织者的半结构化访谈，该工具有效优化了反馈处理流程，使组织者能更合理地分配工作时间。 |
| 量化对大型推理模型强化学习的影响 | Medha Kumar | [PDF](https://arxiv.org/pdf/2511.15694v1) | 目前，大规模强化学习（RL）无需任何监督微调即可实现强大的推理能力。尽管在微调场景下对训练后量化（PTQ）和量化感知训练（QAT）已有深入研究，但量化如何影响大型推理模型（LRMs）中的强化学习仍是一个悬而未决的问题。为解答这一问题，我们通过系统实验发现：在数学基准测试中，经过强化学习后量化的模型与采用量化感知强化学习优化的模型之间存在显著的推理性能差距。研究结果表明，量化感知强化学习会对学习过程产生负面影响，而训练后量化与QLoRA方法则能带来更优的性能表现。 |
| 使用光谱-空间混合网络的高光谱图像分类 | Mohammed Q. Alkhatib | [PDF](https://arxiv.org/pdf/2511.15692v1) | 本文提出SS-MixNet——一种用于高光谱图像分类的轻量化高效深度学习模型。该架构融合了用于局部光谱-空间特征提取的3D卷积层，以及两个并行MLP风格混合器模块，以捕获光谱与空间维度的长程依赖关系。通过引入基于深度卷积的注意力机制，在最小计算开销下增强特征判别能力。在QUH-唐岛湾和QUH-青云数据集上仅使用1%标注数据进行训练验证，SS-MixNet在2D-CNN、3D-CNN、IP-SWIN、SimPoolFormer及HybridKAN等对比方法中取得最优性能，在两个数据集上的总体分类精度分别达到95.68%和93.86%。定量指标与分类图谱共同证实，该模型在有限监督条件下能实现精准鲁棒的预测。代码已公开于：https://github.com/mqalkhatib/SS-MixNet |
