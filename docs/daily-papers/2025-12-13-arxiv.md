# arxiv 2025-12-13

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 立体空间：在规范空间中通过端到端扩散实现无深度立体几何合成 | Tjark Behrens | [PDF](https://arxiv.org/pdf/2512.10959v1) | 我们提出StereoSpace，一种基于扩散的单目转立体合成框架，该框架仅通过视角条件建模几何关系，无需显式深度或变形操作。通过构建规范矫正空间并结合条件引导，生成器能够端到端地推断对应关系并填补遮挡区域。为确保公平且无数据泄露的评估，我们设计了端到端的测试协议，在测试阶段完全排除真实几何数据或代理几何估计的干扰。该协议重点关注反映下游应用价值的评价指标：感知舒适度指标iSQoE与几何一致性指标MEt3R。StereoSpace在变形修复、潜在空间变形及变形条件引导等各类方法中表现卓越，在分层场景与非朗伯场景中均能实现锐利的视差效果与强大的鲁棒性。这确立了视角条件扩散模型作为无需深度信息的可扩展立体生成解决方案的有效性。 |
| WorldLens：现实世界驾驶世界模型的全方位评估 | Ao Liang | [PDF](https://arxiv.org/pdf/2512.10958v1) | 生成式世界模型正在重塑具身人工智能，使智能体能够合成逼真的四维驾驶环境——这些场景视觉上令人信服，却常在物理规律或行为逻辑上存在缺陷。尽管该领域发展迅速，学界仍缺乏统一标准来评估生成世界是否保持几何一致性、遵循物理法则或支持可靠控制。我们推出WorldLens全谱基准测试，从生成、理解与行为三个维度系统评估模型在自建世界中的综合能力。该框架涵盖五大层面——环境生成、场景重建、指令跟随、下游任务与人类偏好——共同检验视觉真实感、几何一致性、物理合理性与功能可靠性。

研究发现，现有世界模型在不同维度表现失衡：纹理逼真的模型常违背物理规律，而几何稳定的模型又缺乏行为可信度。为实现客观指标与人类判断的对齐，我们进一步构建WorldLens-26K数据集，包含2.6万段带数值评分与文本解释的人工标注视频，并开发出WorldLens-Agent评估模型——通过从标注数据中蒸馏学习，实现可扩展、可解释的自动化评分。这套基准测试、数据集与评估模型共同构成衡量世界保真度的统一生态系统，为未来模型评估建立新标准：不仅要看生成世界有多真实，更要看它们如何真实地运行。 |
| 场景构建器：基于解耦去遮挡与姿态估计模型的开放集三维场景生成技术 | Yukai Shi | [PDF](https://arxiv.org/pdf/2512.10957v1) | 在本研究中，我们提出了一种名为SceneMaker的解耦式三维场景生成框架。由于缺乏足够的开放集去遮挡与姿态估计先验知识，现有方法在严重遮挡和开放集场景下难以同时生成高质量几何结构与精确姿态。为解决这些问题，我们首先将去遮挡模型从三维物体生成过程中解耦，并通过融合图像数据集与收集的去遮挡数据集来增强模型对多样化开放集遮挡模式的适应能力。随后，我们提出一种融合全局与局部机制的统一姿态估计模型，该模型在自注意力与交叉注意力机制中均实现双重优化以提升精度。此外，我们构建了开放集三维场景数据集，进一步拓展了姿态估计模型的泛化能力。综合实验表明，我们的解耦框架在室内场景与开放集场景中均展现出优越性能。相关代码与数据集已发布于https://idea-research.github.io/SceneMaker/。 |
| 借助立体视觉与中层视觉技术，赋能动态城市导航系统 | Wentao Zhou | [PDF](https://arxiv.org/pdf/2512.10956v1) | 基础模型在语言和视觉领域的成功，推动了全端到端机器人导航基础模型的研究。这类模型直接将单目视觉输入映射为控制动作，完全忽略了中层视觉模块（如跟踪、深度估计等）。尽管视觉能力会隐式涌现的假设颇具吸引力，但这需要大量从像素到动作的监督数据，而这些数据难以获取。在动态和非结构化环境中，这一挑战尤为突出：鲁棒的导航需要精确的几何与动态理解，而单目视图中的深度尺度模糊性进一步限制了准确的空间推理。本文论证了仅依赖单目视觉而忽略中层视觉先验是低效的。

我们提出了StereoWalker模型，通过立体视觉输入及显式中层视觉信息（如深度估计与密集像素跟踪）来增强导航基础模型。我们的思路很直接：立体输入解决了深度尺度模糊问题，而现代中层视觉模型能为动态场景提供可靠的几何与运动结构。我们还构建了一个大规模立体导航数据集，通过互联网立体视频自动标注动作，以支持StereoWalker的训练并促进未来研究。实验表明，中层视觉使StereoWalker仅用1.5%的训练数据即可达到与当前最优模型相当的性能，使用全量数据时则能超越现有最佳水平。同时我们发现，立体视觉相比单目输入能带来更高的导航性能。 |
| 全属性：面向视觉概念个性化的开放词汇属性编码器 | Tsai-Shien Chen | [PDF](https://arxiv.org/pdf/2512.10955v1) | 视觉概念个性化旨在仅将特定图像属性（如身份、表情、光照和风格）迁移至未见过的场景中。然而，现有方法依赖于通用图像编码器提取的整体嵌入表示，这些表示往往纠缠了多种视觉因素，难以分离单一属性，常导致信息泄露与合成结果不连贯。为突破这一局限，我们提出了Omni-Attribute——首个开放词汇的图像属性编码器，专门用于学习高保真、属性特定的表征。我们的方法从数据和模型两个维度进行协同设计：（一）通过构建语义关联的图像对并标注正负属性，显式指导编码器学习保留或抑制特定信息；（二）采用双目标训练范式，在生成保真度与对比解耦之间取得平衡。实验表明，所得嵌入表示在开放词汇属性检索、个性化定制及组合生成任务中均表现优异，在多项基准测试中达到了最先进的性能水平。 |
| 双向归一化流：从数据到噪声及其逆向过程 | Yiyang Lu | [PDF](https://arxiv.org/pdf/2512.10953v1) | 归一化流（Normalizing Flows, NFs）已成为生成建模领域一种具有理论基础的框架。标准NF包含前向过程与反向过程：前向过程将数据映射至噪声，反向过程则通过逆向映射生成样本。典型的NF前向变换受显式可逆性约束，确保反向过程可作为其精确解析逆变换。近期TARFlow及其变体通过结合Transformer与自回归流技术，为NF方法注入了新的活力，但也暴露出因果解码成为主要瓶颈的问题。本文提出双向归一化流（$\textbf{BiFlow}$）框架，该框架无需依赖精确解析逆变换。BiFlow通过学习近似潜在噪声到数据逆映射的反向模型，实现了更灵活的损失函数与架构设计。在ImageNet数据集上的实验表明，相较于因果解码方案，BiFlow在提升生成质量的同时，将采样速度加快达两个数量级。该框架在基于NF的方法中取得了最先进的成果，并在单次评估（"1-NFE"）方法中展现出具有竞争力的性能。随着NF领域近期取得令人鼓舞的进展，我们希望本研究能进一步引发对这一经典范式的关注。 |
| 高质量数据共享的分层数据集选择 | Xiaona Zhou | [PDF](https://arxiv.org/pdf/2512.10952v1) | 现代机器学习的成功关键在于获取高质量的训练数据。在许多现实场景中，例如从公共存储库获取数据或跨机构共享数据时，数据天然以离散数据集的形式存在，这些数据集在相关性、质量和效用上存在差异。因此，选择搜索哪些存储库或机构以获取有用数据集，以及将哪些数据集纳入模型训练，是至关重要的决策。然而，现有方法大多选择单个样本，并将所有数据视为同等相关，忽略了数据集及其来源之间的差异。本研究将数据集选择任务形式化：从大规模异构数据池中选择完整数据集，以在资源约束下提升下游性能。我们提出基于层次结构的数据集选择方法（DaSH），该方法在数据集和群组（如集合、机构）两个层面建模效用，能够从有限观察中实现高效泛化。在两个公共基准测试（Digit-Five和DomainNet）中，DaSH在准确率上比现有最优数据选择基线方法提升高达26.2%，同时所需探索步骤显著减少。消融实验表明，DaSH对低资源环境和相关数据集缺失具有鲁棒性，适用于实际多源学习工作流程中可扩展且自适应的数据集选择。 |
| 群体扩散：通过解锁跨样本协作增强图像生成 | Sicheng Mo | [PDF](https://arxiv.org/pdf/2512.10954v1) | 在本研究中，我们探索了扩散模型推理中一个尚未开发的信号机制。现有方法在推理阶段均独立生成图像，而我们提出新的研究方向：样本能否通过协作方式生成？我们提出分组扩散方法，突破传统注意力机制仅作用于图像内部区块的限制，实现跨图像注意力共享。这种机制使得多幅图像在推理过程中能够进行联合去噪，同时学习图像内部与图像间的关联特征。

我们观察到明显的规模效应——更大的分组规模会产生更强的跨样本注意力，从而提升生成质量。此外，我们引入定性度量指标来捕捉这种关联特性，并证明其强度与FID指标高度相关。基于标准扩散变换器架构构建的GroupDiff模型，在ImageNet-256x256数据集上实现了最高32.2%的FID提升。本研究揭示了跨样本推理作为一种高效且未被探索的生成建模机制，为生成模型发展提供了新的方向。 |
| E-RayZer：作为空间视觉预训练的自监督三维重建 | Qitao Zhao | [PDF](https://arxiv.org/pdf/2512.10950v1) | 自监督预训练已在语言、单张二维图像及视频的基础模型领域引发革命，但在从多视角图像中学习三维感知表征方面仍鲜有探索。本文提出E-RayZer——一种直接从无标注图像中学习真正三维感知表征的自监督大规模三维视觉模型。不同于RayZer等通过潜在空间视角合成间接推断三维结构的现有自监督方法，E-RayZer直接在三维空间中运行，通过显式几何实现自监督三维重建。这种设计消除了捷径解，并产生几何基础坚实的表征。为确保收敛性与可扩展性，我们提出一种新颖的细粒度学习课程，以完全无监督的方式组织从易到难的训练样本，并协调异构数据源。实验表明，E-RayZer在姿态估计任务上显著优于RayZer，在三维重建任务上达到甚至有时超越VGGT等全监督重建模型的性能。此外，当迁移至三维下游任务时，其学习到的表征性能优于主流视觉预训练模型（如DINOv3、CroCo v2、VideoMAE V2和RayZer），确立了E-RayZer作为三维感知视觉预训练的新范式。 |
| 我们是否已为文本到3D生成中的强化学习做好准备？一项渐进式研究 | Yiwen Tang | [PDF](https://arxiv.org/pdf/2512.10949v1) | 强化学习（RL）此前已被证明在大语言模型与多模态模型中具有显著效果，近期更成功拓展至增强二维图像生成领域。然而，由于三维物体具有更高的空间复杂性——既需要全局一致的几何结构，又需精细的局部纹理细节，将强化学习应用于三维生成的研究仍处于探索初期。这种复杂性使得三维生成对奖励函数设计和强化学习算法极为敏感。为应对这些挑战，我们在多个维度上首次系统性地开展了面向文本到三维自回归生成的强化学习研究：（1）奖励设计：通过评估奖励维度与模型选择，我们发现与人类偏好对齐至关重要，且通用多模态模型能为三维属性提供稳健的信号反馈；（2）强化学习算法：我们研究了GRPO算法的多种变体，证实了词元级优化的有效性，并进一步探索了训练数据规模与迭代次数的扩展规律；（3）文本到三维基准测试：针对现有基准无法有效衡量三维生成模型隐含推理能力的问题，我们提出了MME-3DR评测体系；（4）先进强化学习范式：受三维生成天然层次结构的启发，我们提出Hi-GRPO方法，通过定制化奖励组合优化从全局到局部的层次化三维生成。基于这些发现，我们开发了首个强化学习增强的文本到三维模型AR3D-R1，该模型实现了从粗粒度形状到纹理细化的全流程生成。本研究旨在为强化学习驱动的三维生成推理机制提供新的见解。代码已发布于https://github.com/Ivan-Tang-3D/3DGen-R1。 |
