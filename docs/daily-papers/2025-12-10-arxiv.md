# arxiv 2025-12-10

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 阿斯特拉：基于自回归去噪的通用交互世界模型 | Yixuan Zhu | [PDF](https://arxiv.org/pdf/2512.08931v1) | 扩散变换器的最新进展使得视频生成模型能够从文本或图像生成高质量视频片段。然而，能够根据历史观测与动作预测长时程未来的世界模型仍处于探索不足的状态，尤其是在通用场景与多样化动作形式方面。为填补这一空白，我们提出了Astra——一个交互式通用世界模型，能够为多样化场景（如自动驾驶、机器人抓取）生成具有精确动作交互（如相机运动、机器人动作）的真实世界未来状态。我们设计了一种自回归去噪架构，并使用时序因果注意力机制聚合历史观测数据以支持流式输出。通过引入噪声增强的历史记忆模块，模型避免了对过往帧的过度依赖，从而在响应速度与时序连贯性之间取得平衡。为实现精准动作控制，我们开发了动作感知适配器，将动作信号直接注入去噪过程。此外，我们构建了动作专家混合模块，动态路由异构动作模态，显著提升了模型在探索、操控、相机控制等多样化现实任务中的泛化能力。Astra实现了交互式、连贯且通用的长时程视频预测，并支持多种形式的交互。在多个数据集上的实验表明，Astra在生成保真度、长程预测能力及动作对齐效果上均优于现有最先进的世界模型。 |
| Selfi：基于三维几何特征对齐的自优化重建引擎 | Youming Deng | [PDF](https://arxiv.org/pdf/2512.08930v1) | 新颖视角合成（NVS）传统上依赖于具有显式三维归纳偏置的模型，并需预先结合运动恢复结构（SfM）提供的已知相机参数。近期如VGGT等视觉基础模型采取了截然不同的路径——通过训练数据和损失目标隐式获取三维知识，从而能够直接从未标定图像集中前馈预测相机参数与三维表征。尽管VGGT特征具备灵活性，但其缺乏显式的多视角几何一致性。我们发现，提升此类三维特征一致性对NVS与姿态估计任务均有裨益。本文提出Selfi——一种通过特征对齐实现自我优化的三维重建流程，通过将VGGT主干网络的输出作为伪真值，将其转化为高保真三维重建引擎。具体而言，我们采用基于重投影的一致性损失训练轻量级特征适配器，将VGGT输出提炼至能捕捉三维空间邻近性的几何对齐新特征空间。该方法在NVS与相机姿态估计任务中均实现了最先进性能，证明特征对齐对于下游三维推理是极具价值的关键步骤。 |
| 高效重建动态场景：一次一个D4RT单元 | Chuhan Zhang | [PDF](https://arxiv.org/pdf/2512.08924v1) | 从视频中理解并重建动态场景的复杂几何结构与运动，一直是计算机视觉领域的一项艰巨挑战。本文提出D4RT模型——一种简洁而高效的前馈模型，旨在有效解决这一难题。D4RT采用统一的Transformer架构，能够从单段视频中联合推断深度信息、时空对应关系及完整的相机参数。其核心创新在于引入了一种新颖的查询机制，该机制既避免了密集逐帧解码带来的巨大计算负担，也规避了管理多个任务特定解码器的复杂性。我们的解码接口使模型能够独立、灵活地探查任意时空点的三维位置。由此产生的轻量级高扩展性方法，实现了显著高效的训练与推理过程。实验表明，该方法在各类4D重建任务中均超越现有技术，确立了新的性能标杆。动态效果演示请参见项目页面：https://d4rt-paper.github.io/。 |
| 相同内容，不同答案：多模态大语言模型中的跨模态不一致性 | Angela van Sprang | [PDF](https://arxiv.org/pdf/2512.08923v1) | 我们引入两个新的基准测试REST与REST+（渲染等价性压力测试），旨在系统评估多模态大语言模型（MLLMs）中的跨模态不一致性问题。尽管MLLMs经过训练可将视觉与语言表征于同一嵌入空间，但它们无法在两种模态中执行相同任务。我们的基准测试包含三种模态（图像、文本、混合模态）中具有相同语义信息的样本，实验表明当前最先进的MLLMs无法在不同模态间保持一致的推理能力。通过对15个MLLMs的评估，我们发现即使排除文字识别（OCR）问题的影响，各模型的模态不一致程度仍存在显著差异。无论是将文本渲染为图像，还是将图像转换为文本描述，均无法解决这种不一致性。研究进一步发现，即使OCR识别准确，视觉特征（文字颜色与分辨率，但字体除外）及视觉标记数量仍会影响模型表现。最终，我们提出的模态一致性评分与文本-图像间的模态差距存在相关性，这为理解跨模态不一致的MLLMs提供了机制性解释。 |
| 统一扩散变换器：实现高保真文本感知图像复原 | Jin Hyeon Kim | [PDF](https://arxiv.org/pdf/2512.08922v1) | 文本感知图像修复（TAIR）旨在从包含退化文本内容的低质量输入中恢复高质量图像。尽管扩散模型为通用图像修复提供了强大的生成先验，但由于缺乏显式语言知识，它们在以文本为中心的任务中常产生文本幻觉。为解决这一问题，我们提出UniT——一个统一的文本修复框架，通过迭代方式整合扩散变换器（DiT）、视觉语言模型（VLM）和文本定位模块（TSM），实现高保真文本修复。在UniT中，VLM从退化图像中提取文本内容以提供显式文本指导；同时，基于扩散特征训练的TSM在每一步去噪过程中生成中间OCR预测，使VLM能在去噪过程中迭代优化其指导信息；最终，DiT骨干网络凭借其强大的表征能力，利用这些线索恢复细粒度文本内容，并有效抑制文本幻觉。在SA-Text和Real-Text基准测试上的实验表明，UniT能准确重建退化文本，显著减少幻觉现象，并在TAIR任务中实现了最先进的端到端F1分数性能。 |
| OSMO：用于人机技能传递的开源触觉手套 | Jessica Yin | [PDF](https://arxiv.org/pdf/2512.08920v1) | 人类视频演示为学习机器人策略提供了丰富的训练数据，但仅凭视频无法捕捉掌握操作所需的关键接触信号。我们推出OSMO——一款专为人机技能迁移设计的开源可穿戴触觉手套。该手套在指尖和手掌区域配置了12个三轴触觉传感器，其设计兼容先进的手部追踪方法，支持在自然场景中进行数据采集。实验表明，仅使用通过OSMO采集的人类演示数据（无需任何真实机器人数据）训练的机器人策略，能够完成具有挑战性的高接触操作任务。通过为人类和机器人配备相同的手套，OSMO最大程度缩小了视觉与触觉的实体差异，实现了连续剪切力与法向力反馈的迁移，同时避免了图像修复或其他基于视觉的力推断需求。在需要持续接触压力的真实世界擦拭任务中，我们的触觉感知策略实现了72%的成功率，通过消除接触相关的故障模式，超越了纯视觉基线方法。我们公开完整的硬件设计、固件及组装指南，以促进学界对该技术的应用。 |
| SAQ：稳定子感知量子纠错解码器 | David Zenati | [PDF](https://arxiv.org/pdf/2512.08914v1) | 量子纠错（QEC）解码面临一个根本性的精度与效率权衡问题。经典方法如最小权重完美匹配（MWPM）在不同噪声模型中表现参差不齐，且存在多项式复杂度问题；而张量网络解码器虽能达到高精度，但计算成本过高。近期出现的神经解码器虽降低了复杂度，但其精度尚不足以与计算成本高昂的经典方法竞争。我们提出SAQ-Decoder——一个将基于Transformer的学习与约束感知后处理相结合的统一框架，该框架既能实现接近最大似然（ML）的精度，又能在校验子规模上保持线性计算可扩展性。我们的方法融合了双流Transformer架构（通过非对称注意力模式处理校验子与逻辑信息）以及一种新颖的可微逻辑损失函数（通过有限域上的平滑逼近直接优化逻辑错误率）。SAQ-Decoder在环面码上实现了接近最优的性能：在独立噪声和退极化噪声模型下分别达到10.99%和18.6%的错误阈值，逼近11.0%和18.9%的ML理论界限，同时在精度、复杂度和参数效率上超越现有神经与经典基准方法。我们的研究证明，基于学习的解码器能够同时实现具有竞争力的解码精度与计算效率，这为实用化容错量子计算系统的关键需求提供了解决方案。 |
| LiDAS：面向夜间感知的光照驱动动态主动感知系统 | Simon de Moreau | [PDF](https://arxiv.org/pdf/2512.08912v1) | 夜间环境对基于摄像头的感知系统构成显著挑战，因为现有方法被动依赖场景光照。我们提出照明驱动动态主动感知系统（LiDAS），这是一种将商用视觉感知模型与高清前照灯相结合的闭环主动照明系统。LiDAS并非均匀增强场景亮度，而是动态预测最优照明场以最大化下游感知性能——即减少对空旷区域的照明，将光能重新分配至目标物体区域。通过自适应照明控制，LiDAS实现了日间训练模型在夜间的零样本泛化能力。该系统在合成数据上完成训练后，以零样本方式部署于真实世界闭环驾驶场景，在同等能耗下较标准近光灯方案实现平均精度（mAP50）提升18.7%，平均交并比（mIoU）提升5.0%。在降低40%能耗的同时仍能保持感知性能。LiDAS可与领域泛化方法形成互补，无需重新训练即可进一步增强系统鲁棒性。通过将现有前照灯转化为主动视觉执行器，LiDAS为鲁棒的夜间感知提供了经济高效的解决方案。 |
| 从单张图像实现自我演化的三维场景生成 | Kaizhi Zheng | [PDF](https://arxiv.org/pdf/2512.08905v1) | 从单张图像生成高质量、带纹理的三维场景，始终是视觉与图形学领域的核心挑战。现有的图像到三维生成器虽能从单一视角恢复合理几何结构，但其以物体为中心的训练范式限制了模型在复杂大尺度场景中的泛化能力，难以同时保证场景结构的准确性与纹理的真实性。本文提出EvoScene——一种自演进、免训练的三维场景重建框架，能够从单张图像逐步重建完整三维场景。其核心思想在于融合现有模型的互补优势：三维生成模型的几何推理能力与视频生成模型的视觉知识。通过"空间先验初始化""视觉引导三维场景网格生成""空间引导新视角生成"三个迭代阶段，EvoScene在二维与三维域间交替优化，持续提升场景结构与外观质量。多样化场景实验表明，相较于现有强基线模型，EvoScene在几何稳定性、视角一致性纹理和未观测区域补全方面均表现优异，可直接生成适用于实际应用的三维网格模型。 |
| UniLayDiff：面向内容感知布局生成的一体化扩散变换器模型 | Zeyang Liu | [PDF](https://arxiv.org/pdf/2512.08897v1) | 内容感知布局生成是图形设计自动化中的关键任务，其核心在于创建与给定背景图像自然融合的视觉元素排布方案。由于实际应用场景的多样性，开发能够统一处理各类输入约束生成子任务（如基于元素类型、尺寸或关系等条件）的单一模型极具挑战性。现有方法往往仅能处理部分子任务，或需为不同约束条件配置独立模型参数，尚未实现真正统一的解决方案。本文提出UniLayDiff：一种基于扩散变换器的统一框架，首次通过单一端到端可训练模型处理多样化的内容感知布局生成任务。具体而言，我们将布局约束视为独立模态，采用多模态扩散变换器框架来捕捉背景图像、布局元素与多样化约束之间的复杂交互关系。此外，通过在预训练模型基础上使用LoRA进行关系约束的微调，该方案不仅实现了统一的条件生成，还显著提升了整体布局质量。大量实验表明，UniLayDiff在无条件生成到各类条件生成任务中均达到最先进性能，据我们所知，这是首个能够全面统一内容感知布局生成任务的模型。 |
