# arxiv 2025-12-09

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| Voxify3D：像素艺术与体素渲染的融合 | Yi-Chuan Huang | [PDF](https://arxiv.org/pdf/2512.07834v1) | 体素艺术是一种广泛应用于游戏和数字媒体的独特风格化形式，但从三维网格自动生成体素艺术仍面临几何抽象、语义保持与离散色彩协调性等多重要求相互冲突的挑战。现有方法往往过度简化几何结构，或难以实现体素艺术特有的像素级精度与调色板约束美学。本文提出Voxify3D——一个连接三维网格优化与二维像素艺术监督的可微分双阶段框架。我们的核心创新在于三大组件的协同整合：（1）正交像素艺术监督机制，通过消除透视畸变实现体素-像素精确对齐；（2）基于分块的CLIP对齐模块，在离散化过程中保持跨层级语义一致性；（3）调色板约束的Gumbel-Softmax量化器，支持在离散色彩空间进行可微分优化，并提供可控的调色板策略。该集成方案系统解决了三大基础难题：极端离散化下的语义保持、通过体渲染实现的像素艺术美学、端到端的离散优化。实验表明，本方法在多样化角色模型和可控抽象程度（2-8色、20倍-50倍分辨率）下均表现出优越性能（CLIP-IQA评分37.12，用户偏好率77.90%）。项目主页：https://yichuanh.github.io/Voxify-3D/ |
| 关系视觉相似性 | Thao Nguyen | [PDF](https://arxiv.org/pdf/2512.07833v1) | 人类不仅能看到属性相似性——我们还能识别关系相似性。苹果像桃子是因为两者都是红色的水果，但地球也像桃子：其地壳、地幔和地核分别对应桃子的表皮、果肉和果核。认知科学家认为，这种感知和识别关系相似性的能力，正是人类区别于其他物种的关键特征。然而，当前所有广泛使用的视觉相似性度量方法（如LPIPS、CLIP、DINO）都仅关注感知属性相似性，无法捕捉人类所感知的丰富且常令人惊奇的关系相似性。

我们该如何超越图像的可见内容，捕捉其关系属性？又该如何在表征空间中让具有相同关系逻辑的图像彼此靠近？为回答这些问题，我们首先将关系图像相似性定义为可量化问题：当两幅图像的视觉元素之间的内在关系或功能相互对应时，即使其视觉属性不同，它们也具有关系相似性。

基于此，我们构建了一个包含11.4万张图像-文本对的数据集，其中文本描述经过匿名化处理——着重描述场景底层的关系逻辑而非表面内容。利用该数据集，我们对视觉-语言模型进行微调，以测量图像间的关系相似性。该模型成为通过底层关系结构（而非表面视觉外观）连接图像的第一步。

研究表明，虽然关系相似性在现实世界中具有广泛应用价值，但现有图像相似性模型均未能有效捕捉这一特性——这揭示了视觉计算领域存在的重要空白。 |
| 泛化结果是否具有普遍适用性？ | Matteo Boglioni | [PDF](https://arxiv.org/pdf/2512.07832v1) | 大型语言模型（LLM）的分布外（OOD）泛化能力对其实际部署至关重要。然而，以往评估LLM泛化性能的研究通常仅关注单一分布外数据集。这种方法可能无法精确评估模型能力，因为模型部署后遇到的数据偏移具有高度多样性。本研究旨在探究分布外泛化结果是否具有可推广性。具体而言，我们在微调过程中评估模型在多个分布外测试集上的表现，并通过回归剔除领域内性能的影响，计算这些测试集性能之间的偏相关性。这种方法使我们能够评估在控制领域内性能后，不同泛化表现之间的关联程度。通过对OLMo2和OPT模型的分析，我们发现泛化结果并未呈现统一规律：任意两个分布外测试集之间是否存在正相关或负相关，很大程度上取决于所分析的具体模型选择。 |
| UnityVideo：统一多模态多任务学习框架，用于增强世界感知的视频生成 | Jiehui Huang | [PDF](https://arxiv.org/pdf/2512.07831v1) | 当前视频生成模型虽展现出卓越的合成能力，但仍受限于单模态条件约束，制约了其对整体世界的全面理解。这一局限源于跨模态交互的不足以及模态多样性对世界知识表征的有限覆盖。为突破这些限制，我们提出UnityVideo——一个面向世界感知视频生成的统一框架，该框架通过联合学习多种模态（分割掩码、人体骨架、密集姿态、光流与深度图）及训练范式实现突破。我们的方法包含两大核心组件：（1）动态噪声注入技术以统一异构训练范式；（2）搭载上下文学习器的模态切换器，通过模块化参数与情境学习实现统一处理。我们构建了包含130万样本的大规模统一数据集。通过联合优化，UnityVideo显著加速模型收敛，并极大增强了对未见数据的零样本泛化能力。实验表明，UnityVideo在视频质量、时序一致性及物理世界约束对齐方面均达到更优水平。代码与数据详见：https://github.com/dvlab-research/UnityVideo |
| 单层足矣：适配预训练视觉编码器用于图像生成 | Yuan Gao | [PDF](https://arxiv.org/pdf/2512.07829v1) | 视觉生成模型（如扩散模型）通常运行在压缩的潜在空间中，以平衡训练效率与生成样本质量。与此同时，利用高质量预训练视觉表征的研究日益受到关注，常见做法包括将其与变分自编码器（VAE）对齐或直接整合到生成模型中。然而，由于理解导向的特征与生成友好的潜在空间之间存在根本性错配，适配这类表征仍具挑战性。表征编码器受益于高维潜在空间以捕捉掩码区域的多样化假设，而生成模型则倾向于低维潜在空间以忠实保留注入的噪声。这种差异导致先前研究不得不依赖复杂的目标函数与架构设计。

本研究提出特征自编码器（FAE），这是一个简洁而有效的框架，可将预训练的视觉表征适配至适用于生成任务的低维潜在空间，仅需单个注意力层即可实现，同时保留足够的信息以支持重建与理解任务。其核心在于耦合两个独立的深度解码器：一个负责重建原始特征空间，另一个则以重建特征为输入进行图像生成。FAE具有通用性，可与多种自监督编码器（如DINO、SigLIP）结合，并适配于扩散模型与归一化流这两类不同的生成模型体系。

在类别条件生成与文生图基准测试中，FAE展现出卓越性能。例如在ImageNet 256×256数据集上，采用分类器引导（CFG）的扩散模型取得了接近最优的FID分数：1.29（800训练周期）与1.70（80训练周期）。在无CFG条件下，FAE更达到当前最优的FID分数1.48（800周期）与2.08（80周期），既实现了高质量生成，又展现出快速学习能力。 |
| 人工智能代理的采纳与应用：来自Perplexity平台的早期实证研究 | Jeremy Yang | [PDF](https://arxiv.org/pdf/2512.07828v1) | 本文首次对开放网络环境中通用人工智能代理的采纳率、使用强度及应用场景进行了大规模实地研究。我们的分析聚焦于Perplexity公司开发的AI驱动浏览器Comet及其内置智能体Comet Assistant。基于数亿次匿名用户交互数据，我们致力于解答三个核心问题：谁在使用AI智能体？使用强度如何？以及具体用途是什么？研究发现，不同用户群体在采纳与使用模式上存在显著异质性。早期采纳者、人均GDP与教育水平较高国家的用户，以及从事数字或知识密集型行业（如数字科技、学术界、金融、市场营销和创业领域）的个体，更倾向于采纳或高频使用智能体。

为系统化解析智能体使用的实质内容，我们构建了分层智能体分类体系，将应用场景划分为主题、子主题和任务三个层级。其中"生产力与工作流"及"学习与研究"两大主题占智能体查询总量的57%，而"课程学习"与"商品选购"两个子主题占比达22%。在90项具体任务中，前10项任务已覆盖55%的查询量。从使用场景看，个人用途占查询总量的55%，专业工作与教育学习场景分别占30%和16%。短期数据显示应用场景具有较强粘性，但长期观察发现用户会逐渐转向认知需求更高的主题。

日益强大的人工智能代理的普及对研究者、企业、政策制定者和教育工作者具有重要启示，这一快速兴起的AI能力类别亟待学界展开新的探索路径。 |
| 基于深度学习的自适应多层蜜网架构用于威胁行为分析 | Lukas Johannes Möller | [PDF](https://arxiv.org/pdf/2512.07827v1) | 网络威胁的复杂性与多样性日益升级，使得静态蜜罐已无法满足需求，亟需采用自适应、情报驱动的欺骗技术。本研究提出ADLAH系统——一种自适应深度学习异常检测蜜网，通过基础设施的自主编排，在最小化成本的同时实现高保真威胁情报的最大化捕获。核心贡献在于提出了一套端到端的人工智能驱动欺骗平台架构蓝图与设计理念。该系统的核心决策机制已通过功能原型验证可行性：强化学习智能体能够实时判断何时将会话从低交互传感器节点升级至动态部署的高交互蜜罐。由于缺乏充足的实时攻击数据，本研究暂未进行实际规模验证，但详细阐述了设计权衡与局限性，并制定了规模化实证评估的严谨路线图。除选择性会话升级与异常检测外，该架构致力于实现僵尸网络攻击链的自动化提取、聚类与版本管理——这一核心能力源于对暴露服务主要受自动化流量支配的实证观察。这些要素共同勾勒出一条切实可行的技术路径，能够以经济高效的方式捕获高价值对抗行为、实现系统化的僵尸网络版本管理，并生成可操作的威胁情报。 |
| OpenVE-3M：面向指令引导视频编辑的大规模高质量数据集 | Haoyang He | [PDF](https://arxiv.org/pdf/2512.07826v1) | 基于指令的图像编辑数据集在质量和多样性上持续提升，然而面向指令式视频编辑的大规模高质量数据集仍然稀缺。为填补这一空白，我们推出了OpenVE-3M——一个开源、大规模、高质量的指令式视频编辑数据集。该数据集包含两大类别：空间对齐编辑（全局风格、背景替换、局部修改、局部移除、局部添加及字幕编辑）与非空间对齐编辑（镜头多视角编辑与创意编辑）。所有编辑类型均通过精心设计的数据流水线生成，并经过严格的质量筛选。OpenVE-3M在数据规模、编辑类型多样性、指令长度及整体质量上均超越现有开源数据集。此外，针对该领域缺乏统一评估基准的问题，我们构建了OpenVE-Bench基准测试集，包含431个视频-编辑配对样本，涵盖多样化编辑任务，其三项核心评估指标与人类评判高度契合。基于本数据集训练的50亿参数模型OpenVE-Edit在OpenVE-Bench上实现了突破性的性能表现，不仅刷新了该基准的最高纪录，更以显著优势超越包括140亿参数基线模型在内的所有现有开源模型，展现出卓越的编辑效率与效果。项目主页详见https://github.com/lewandofskee/OpenVE。 |
| WorldReel：基于一致几何与运动建模的四维视频生成技术 | Shaoheng Fang | [PDF](https://arxiv.org/pdf/2512.07821v1) | 当前视频生成器虽能实现惊人的照片级真实感，但其三维本质仍存在根本性不一致。我们提出WorldReel——一个原生时空一致的4D视频生成系统。该模型能同步生成RGB视频帧与4D场景表征，包括点云图、相机轨迹和稠密光流映射，从而实现对几何结构与外观随时间演变的连贯建模。我们通过显式的4D表征强制构建统一的基础场景，使其在不同视角与动态内容中保持延续性，即使在大幅度非刚性运动与显著相机移动条件下仍能生成一致性视频。

我们采用合成数据与真实数据相结合的创新训练策略：合成数据提供精确的4D监督信号（几何、运动与相机参数），真实视频则贡献视觉多样性与真实感。这种混合训练机制使WorldReel既能保持强几何保真度，又能泛化至真实场景素材。大量实验表明，WorldReel在动态场景与运动相机条件下的视频生成任务中确立了新的技术标杆，在几何一致性、运动连贯性等指标上显著超越现有方法，并有效减少了时空伪影。

我们相信WorldReel将视频生成推向4D一致的世界建模新阶段，使智能体能够通过统一稳定的时空表征对场景进行渲染、交互与推理。 |
| 基于图学习的脑电图谱地形表示与梯度对齐在脑机接口中的应用 | Prithila Angkan | [PDF](https://arxiv.org/pdf/2512.07820v1) | 我们提出一种基于图结构的梯度对齐脑电图表征学习方法，该方法通过融合多域信息来学习适用于脑机接口的脑电图表征。该模型利用图卷积网络融合基于频率的拓扑图与时频谱图的嵌入表示，从而捕捉跨域关联关系。针对脑电信号具有时序动态性和个体敏感特性所导致的高类间分离性挑战，本方法通过引入中心损失与成对差异损失加以解决。此外，该方法采用梯度对齐策略来协调来自不同域及融合嵌入表示之间的梯度冲突，确保当梯度指向相互矛盾的方向时，能够调整至统一的优化方向。我们在三个公开脑电数据集（BCI-2a、CL-Drive和CLARE）上进行了大量实验验证本方法的有效性，并通过系统性的消融实验进一步揭示了模型各组成部分的作用机制。 |
