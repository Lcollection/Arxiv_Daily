# arxiv 2025-10-13

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| StreamingVLM：面向无限视频流的实时理解系统

（解析：采用"面向"体现技术应用场景的指向性，保留"StreamingVLM"专业术语不翻译确保技术准确性。"实时理解系统"完整对应"Real-Time Understanding"的技术内涵，通过添加"系统"二字明确该技术的工程实现属性，符合中文技术文献表述规范。"无限视频流"准确传达"infinite video streams"的核心概念，整体结构保持学术翻译的严谨性与技术术语的统一性。） | Ruyi Xu | [PDF](http://arxiv.org/pdf/2510.09608v1) | 视觉语言模型（VLMs）具备驱动实时助手与自主智能体的潜力，但其面临关键挑战：如何在处理近乎无限的视频流时，避免延迟与内存使用的指数级增长。若对完整视频施加全局注意力机制，将导致计算复杂度呈平方级增长，且在长视频场景下性能显著下降。而简单的滑动窗口方法同样存在缺陷——它们要么破坏时序连贯性，要么因冗余重计算引发高延迟问题。本文提出StreamingVLM，专为实现对无限视觉输入的实时稳定理解而设计。我们构建了统一框架，使训练过程与流式推理机制对齐。在推理阶段，通过复用注意力锚点状态、近期视觉标记的短窗口以及近期文本标记的长窗口，维持紧凑的键值缓存。这种流式处理能力通过简单的监督微调策略实现：对短片段重叠视频块施加全局注意力，有效模拟推理阶段的注意力模式，而无需在超长上下文上进行训练。为进行评估，我们构建了Inf-Streams-Eval新基准数据集，其视频平均时长超两小时，要求实现帧与文本的逐秒密集对齐。在该基准测试中，StreamingVLM相较GPT-4O mini取得66.18%的胜率，并在单张NVIDIA H100上保持最高8 FPS的稳定实时性能。值得注意的是，我们的监督微调策略在未经过特定视觉问答调优的情况下，仍能增强通用视觉问答能力：在LongVideoBench上提升4.30分，OVOBench Realtime上提升5.96分。代码已开源：https://github.com/mit-han-lab/streaming-vlm。 |
| VITA-VLA：基于动作专家蒸馏的高效视觉语言模型行为教学框架

该研究提出一种通过动作专家蒸馏技术，高效指导视觉语言模型执行具体任务的方法框架。其核心在于将专业动作决策系统的知识迁移至通用视觉语言模型中，使模型在保持语言理解能力的同时获得精准的动作执行能力。该方法通过蒸馏过程显著提升了模型行为决策的准确性与效率，为具身智能系统的开发提供了新思路。 | Shaoqi Dong | [PDF](http://arxiv.org/pdf/2510.09607v1) | 视觉语言动作（VLA）模型通过利用预训练视觉语言模型（VLM）强大的感知能力，显著推动了机器人操作技术的发展。通过将动作模块集成到这些预训练模型中，VLA方法展现出更强的泛化能力。然而，从头开始训练这些模型成本高昂。本研究提出了一种简洁高效的基于蒸馏的框架，通过迁移预训练小型动作模型的知识，使VLM具备动作执行能力。我们的架构保留了原始VLM结构，仅添加动作标记和状态编码器以整合物理输入。

为实现动作知识蒸馏，我们采用两阶段训练策略：首先通过将VLM隐藏状态映射到小型动作模型的动作空间，实现轻量级对齐，从而有效复用其预训练动作解码器并避免昂贵的预训练过程；随后选择性微调语言模型、状态编码器和动作模块，使系统能够整合多模态输入并生成精确动作。具体而言，动作标记为VLM提供了预测未来动作的直接接口，而状态编码器使模型能够整合仅靠视觉无法捕捉的机器人动态信息。这种设计相比从头训练大型VLA模型实现了显著的效率提升。

与先前最先进方法相比，我们的方法在LIBERO基准上达到97.3%的平均成功率（提升11.8%），在LIBERO-LONG上达到93.5%（提升24.5%）。在五项真实世界操作任务中，我们的方法持续超越教师模型，达成82.0%的成功率（提升17%），这证明动作蒸馏能有效帮助VLM生成精确动作，同时大幅降低训练成本。 |
| 空间视界：从毫米到千米的全尺度视觉空间推理 | Peiwen Sun | [PDF](http://arxiv.org/pdf/2510.09606v1) | 随着空间推理研究热潮的兴起，研究者在室内场景理解方面已取得显著进展，但在机器人、自动驾驶等多样化应用场景中仍面临挑战。本文旨在通过解决两大核心问题推动跨场景全尺度空间推理的发展：1）数据集构建过度依赖室内3维扫描与人工标注；2）缺乏有效的全尺度场景建模方法，导致模型易对单一场景过拟合。我们提出集成结构化空间推理知识体系、尺度感知建模与渐进式训练范式的整体解决方案，据我们所知，这是首次尝试拓展多模态大语言模型的全尺度空间智能。通过专业驱动的任务定制化自动流程，我们构建包含5种空间尺度的3.8万组视频场景，创建涵盖19类任务、约100万空间问答对的SpaceVista-1M数据集。尽管专家模型能注入领域知识，但其评估可靠性有限。为此我们通过人工录制、检索与组装视频数据，构建具有精确标注的全尺度基准测试集。然而，直接训练SpaceVista-1M常因潜在知识冲突导致次优结果，据此我们提出SpaceVista-7B空间推理模型——该模型可接受超越语义的密集输入，以尺度为锚点实现尺度感知专家模块与渐进式奖励机制。最终在包含SpaceVista-Bench的5个基准测试中，我们的模型展现出卓越性能，实现了全尺度与全场景的强泛化能力。数据集、模型与基准测试将发布于https://peiwensun2000.github.io/mm2km。 |
| 提示性测试时扩展是一种强大的LLM推理数据增强方法 | Sondos Mahmoud Bsharat | [PDF](http://arxiv.org/pdf/2510.09599v1) | 大语言模型在获得思维链示例时展现出卓越的推理能力，但构建大规模推理数据集仍耗费大量人力与资源。本研究提出提示测试时扩展（P-TTS），这是一种通过微调增强大语言模型推理能力的简易高效推理阶段数据增强策略。该方法无需收集成千上万的样本，仅利用90个精选推理实例，通过系统化调整指令提示强度，在测试时合成多样化的推理轨迹上下文。我们在P-TTS生成数据上对不同规模的Qwen-2.5模型进行微调。在数学推理基准测试（AIME2024&25、MATH500和GPQA-Diamond）中，P-TTS-7B和32B模型显著超越S1、S1.1（千样本）等基线：在AIME'24（7B）上分别实现26.66%和30.00%的绝对准确率提升，在AIME'25（7B）上分别提升13.34%和6.67%；P-TTS-32B在AIME'24上分别获得23.33%和16.63%提升，在AIME'25上分别提升26.63%和3.33%（相较于S1与S1.1），并在MATH500与GPQA-Diamond上保持相当或更优表现。进一步实验表明，P-TTS在高考、考研、OlympiadBench、AMC23、小学奥数及Minerva等跨领域推理基准上显著提升零样本泛化能力。分析显示，测试时扩展能有效探索推理模式的潜在空间，以最小标注成本放大语言模型的问题解决能力，进一步释放其推理潜力。该技术为资源受限或快速演进领域提供了一种实用低成本的推理能力激发方案。 |
| BaNEL：仅使用负奖励的生成建模探索后验分布 | Sangyun Lee | [PDF](http://arxiv.org/pdf/2510.09596v1) | 当今生成式模型在海量监督数据和表征生成质量的信息化奖励函数加持下蓬勃发展。其运行基于两个核心假设：监督数据为模型预训练提供知识基础，而奖励函数则密集反馈如何进一步提升生成质量与正确性。然而在重要难题的极端案例中，会显现两大困境：（1）基础生成模型获得的奖励信号趋近于零；（2）调用奖励评估器的成本高昂。这种情境构成了与标准基于奖励的后训练截然不同的学习挑战。为此，我们提出贝叶斯负例学习（BaNEL）算法，该算法仅通过失败尝试进行模型后训练，同时最小化奖励评估次数（NREs）。我们的方法论基于以下洞见：从失败中学习规律可转化为另一个环内生成建模问题。随后利用该模型判别新数据是否与已知失败模式相似，从而引导生成过程规避此类错误。实验表明，在多个稀疏奖励任务中，BaNEL无需观察任何成功样本即可提升模型性能，其成功率较现有新颖性奖励方法高出数个数量级，且消耗的奖励评估次数更少。 |
| LiveOIBench：大型语言模型能否在信息学奥赛中超越人类选手？ | Kaijian Zou | [PDF](http://arxiv.org/pdf/2510.09595v1) | 随着算法竞赛题目因其复杂性和易验证性特点，正日益成为评估大语言模型编码能力的重要基准。然而现有编程评测基准存在诸多局限：缺乏高难度题目、测试用例覆盖不足、依赖在线平台接口导致可访问性受限。为应对这些问题，我们推出LiveOIBench综合评测体系，包含403道经专家遴选的信息学奥赛级别竞赛题目，每道题目平均配备60个专业设计的测试用例。这些题目直接源自2023至2025年间举办的72场各地区官方信息学竞赛，具备四大核心特征：（1）精心设计的高质量任务配备详细子任务评分标准和大量隐藏测试用例；（2）直接整合精英选手表现数据，支持与人类顶尖表现者的对标分析；（3）通过新增奥赛题目持续更新机制确保数据无污染；（4）自包含评估系统支持离线可复现的评测。在对32个主流通用与推理大模型进行测试后，我们发现GPT-5达到81.76百分位值的优异表现，但仍未突破通常位于90百分位值以上的人类顶尖选手水平。相比之下，开源推理模型GPT-OSS-120B仅达60百分位值，凸显其与前沿闭源模型的能力差距。深度分析表明，稳健的推理模型更注重精准的问题解析而非过度探索，提示未来模型应强化结构化分析能力并减少无效探索。所有数据、代码及排行榜结果将通过项目网站公开发布。 |
| 模式：利用动态专家混合学习复杂系统的组合表示 | Nathan Quiblier | [PDF](http://arxiv.org/pdf/2510.09594v1) | 生命科学中的动力系统通常由复杂且相互重叠的行为机制混合构成。细胞亚群可能从周期性动态转向平衡动态，或分化出不同的发育路径。这些机制间的转换往往表现出噪声干扰和不规则性，这对基于流形的传统建模技术构成了严峻挑战——该类技术默认局部动力学具有平滑特性。为应对这一挑战，我们提出MODE（动态专家混合模型），该图模型框架通过神经门控机制将复杂动力学分解为稀疏可解释的组分，既能实现行为机制的无监督发现，又能精准完成跨机制转换的长期预测。关键在于，由于框架中的智能体可切换至不同控制法则，MODE特别适用于前述含噪声的转换场景。我们在计算生物学领域的合成与真实数据集上系统评估了该方法：首先利用合成动态快照数据（包括噪声环境与小样本场景）对MODE进行无监督分类任务的系统性基准测试；继而证明该方法在模拟细胞生物学关键循环与分支过程的挑战性预测任务中的卓越表现；最后将模型部署于人类单细胞RNA测序数据，结果表明其不仅能区分增殖与分化动态，更能预测细胞定型终末命运的关键时间节点——这正是计算生物学领域亟待突破的核心难题。 |
| STaTS：基于统计窗口合并的结构感知时序序列摘要方法

（解析说明：该翻译遵循以下原则：
1. 保留专业术语："Structure-Aware"译为"结构感知"，"Temporal Sequence"译为"时序序列"，"Statistical Window Merging"译为"统计窗口合并"
2. 采用学术论文标题惯用表达："via"译为"基于"体现方法论特征
3. 保持技术准确性："Summarization"在数据挖掘领域规范译为"摘要"而非"概括"
4. 符合中文语序：将英文后置定语"via..."通过前置状语"基于..."实现自然语序转换
5. 保留首字母缩略词"STaTS"不译，维持专业文献惯例） | Disharee Bhowmick | [PDF](http://arxiv.org/pdf/2510.09593v1) | 时间序列数据通常包含潜在的时间结构、局部平稳状态间的转换、重复模式及突发性变异，但这些特征在标准表示学习流程中很少被有效利用。现有模型通常直接处理原始数据或固定窗口序列，将所有时间步视为同等重要，导致长序列或含噪序列中存在效率低下、鲁棒性差和可扩展性有限的问题。我们提出STaTS——一种轻量级的无监督结构感知时序摘要框架，能够自适应地将单变量与多变量时间序列压缩为紧凑且保留信息特征的标记序列。该框架通过基于BIC的统计差异准则检测多时间分辨率下的变点，随后使用均值等简单函数或高斯混合模型等生成模型对各片段进行摘要表示。该方法可实现高达30倍的序列压缩，同时保留核心时序动态特征。STaTS作为模型无关的预处理器，无需重新训练即可与现有无监督时序编码器集成。在150+数据集上的大量实验（包括UCR-85、UCR-128和UEA-30档案的分类任务，以及ETTh1、ETTh2、ETTm1和Electricity的预测任务）表明，STaTS在保持85-90%全模型性能的同时，显著降低了计算成本。此外，STaTS在噪声环境下表现出更强的鲁棒性，并保持了判别性结构特征，其性能优于均匀压缩和基于聚类的基线方法。这些结果确立了STaTS作为一种原理性通用解决方案，能够实现高效且结构感知的时间序列建模。 |
| 思维同步言语：一种实现口语模型实时推理的双脑方法 | Donghang Wu | [PDF](http://arxiv.org/pdf/2510.09592v1) | 实时口语模型（SLMs）因顺序生成完整思维过程所产生的高延迟而难以有效利用思维链（CoT）推理。让口语模型实现类人"边想边说"的能力正受到越来越多关注。我们首次提出脑启发式框架——思维引导式发言（MPS），该框架能实现高保真度的实时推理。仿照人类使用不同脑区进行思考与应答的机制，我们创新性地采用双脑架构：由"构思脑"负责高层推理，用以引导和控制专门负责流畅语音生成的"表达脑"。这种分工模式消除了思维模式切换，完整保留了推理过程的连贯性。实验表明，MPS在"边想边说"任务中显著优于现有方法，其推理性能与预先计算完整思维链的模型相当，同时大幅降低延迟。在零延迟配置下，本方法在数学推理任务Spoken-MQA中达到92.8%的准确率，在语音对话任务URO-Bench中获得82.5分。我们的研究有效弥合了高质量推理与实时交互之间的鸿沟。 |
| 视觉语言模型：基于2.6万篇文献的系统综述

（注：此处采用"系统综述"对应"Survey"的学术含义，既体现文献分析的全面性，又符合中文学术表达规范；数字单位转换为中文习惯的"万"级计量；冒号结构保留原标题的从属关系，确保学术翻译的精确性与可读性） | Fengming Lin | [PDF](http://arxiv.org/pdf/2510.09586v1) | 我们提出一项透明可复现的研究趋势测量，涵盖2023至2025年间CVPR、ICLR和NeurIPS三大会议的26,104篇录用论文。通过标准化处理标题与摘要、实施短语保护机制，并基于人工构建的词典进行匹配，我们为每篇论文标注最多35个主题标签，同时挖掘关于任务、架构、训练范式、目标函数、数据集及共现模态的细粒度线索。分析结果揭示三大宏观趋势：(1) 多模态视觉-语言-大语言模型研究急剧增长，日益将传统感知任务重构为指令跟随与多步推理；(2) 生成式方法持续扩张，扩散研究聚焦于可控性、蒸馏与加速；(3) 三维与视频领域保持活跃，场景构建从神经辐射场转向高斯泼溅，且日益强调以人与智能体为中心的认知理解。在多模态视觉语言模型领域，参数高效适配技术（如提示学习/适配器/LoRA）与轻量级视觉语言桥梁成为主流；训练实践从零构建编码器转向指令微调与强骨干网络精调；对比学习目标逐渐被交叉熵/排序损失及蒸馏方法取代。跨会议对比显示CVPR在三维领域更具优势，ICLR的多模态视觉语言模型占比最高，而效率、鲁棒性等可靠性主题则渗透各领域。我们公开词典与方法论以支持审计与拓展研究。当前局限包括词典召回率与仅基于摘要的分析，但纵向趋势在会议与年份间呈现一致性。 |
