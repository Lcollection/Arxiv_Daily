# arxiv 2025-12-02

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 高效流：面向具身AI的高效等变流策略学习 | Jianlei Chang | [PDF](https://arxiv.org/pdf/2512.02020v1) | 生成式建模在视觉运动策略学习领域近期展现出显著潜力，能够为多样化具身智能任务提供灵活且富有表现力的控制能力。然而，现有生成式策略常面临数据效率低下（需要大规模演示数据）与采样效率不足（推理过程中动作生成缓慢）的双重挑战。本文提出EfficientFlow——一个基于流匹配策略学习的高效具身智能统一框架。为提升数据效率，我们将等变性引入流匹配过程，从理论上证明：当采用各向同性高斯先验分布与等变速度预测网络时，生成的动作分布将保持等变性，从而显著提升泛化能力并大幅降低数据需求。针对采样加速问题，我们创新性地提出加速度正则化策略。由于边缘流轨迹的加速度直接计算难以实现，我们推导出新型替代损失函数，仅需条件轨迹即可实现稳定可扩展的训练。在广泛的机器人操作基准测试中，所提算法在有限数据条件下实现了具有竞争力乃至更优的性能，同时推理速度获得数量级提升。这些成果标志着EfficientFlow成为高性能具身智能领域强大而高效的新范式。 |
| 最大熵强化学习的扩散模型框架 | Sebastian Sanokowski | [PDF](https://arxiv.org/pdf/2512.02019v1) | 扩散模型在数据驱动学习以及从复杂、非归一化的目标分布中采样方面取得了显著成功。基于这一进展，我们将最大熵强化学习重新解释为一个基于扩散模型的采样问题。我们通过最小化扩散策略与最优策略分布之间的反向KL散度来解决该问题，并利用一个可处理的上界进行优化。通过将策略梯度定理应用于此目标，我们推导出一个改进的替代目标函数，该函数以原理性方式融入了扩散动力学。由此，我们提出了基于扩散的Soft Actor-Critic、Proximal Policy Optimization和Wasserstein Policy Optimization的简单变体，分别命名为DiffSAC、DiffPPO和DiffWPO。这些方法仅需对其基础算法进行微小的实现调整。实验表明，在标准连续控制基准测试中，DiffSAC、DiffPPO和DiffWPO相比SAC和PPO能够获得更高的回报和样本效率。 |
| 以数据为中心的自动驾驶实验室视觉开发 | Anbang Liu | [PDF](https://arxiv.org/pdf/2512.02018v1) | 自动驾驶实验室为减少生物科学中劳动密集、耗时且往往难以复现的工作流程提供了一条前景广阔的路径。然而，其严格的精度要求依赖于大量标注数据训练出的高鲁棒性模型。这类数据在常规实践中难以获取，尤其是负样本。本研究聚焦于移液操作——自动驾驶实验室中最关键且对精度最敏感的动作。为克服训练数据稀缺问题，我们构建了一个融合真实与虚拟数据生成的混合流程。真实数据流采用人机协同方案，将自动化采集与选择性人工验证相结合，以最小成本实现精度最大化。虚拟数据流通过参考条件提示引导的图像生成技术增强真实数据，并经过可靠性筛选与验证。双轨并行最终产生类别均衡的数据集，为稳健的气泡检测训练提供支撑。在预留的真实测试集上，仅使用自动化采集的真实图像训练的模型达到99.6%准确率；而混合使用真实与生成数据进行训练时，准确率仍保持99.4%，同时显著降低了数据采集与审核成本。该方法为自动驾驶实验室工作流程提供了可扩展、高性价比的视觉反馈数据供给策略，并为罕见事件检测及更广泛的视觉任务中的数据稀缺问题提供了实用解决方案。 |
| 视觉同步：通过跨视角物体运动实现多摄像头同步 | Shaowei Liu | [PDF](https://arxiv.org/pdf/2512.02017v1) | 如今，人们能够轻松使用多种消费级相机记录下各类值得纪念的时刻——无论是音乐会、体育赛事、学术讲座、家庭聚会还是生日派对。然而，如何实现这些跨相机视频流的高精度同步仍是一个技术难题。现有方法通常依赖于受控拍摄环境、特定拍摄目标、人工校正或昂贵硬件设备。我们提出的VisualSync框架基于多视角动态优化方法，能够以毫秒级精度对齐无固定机位、未同步拍摄的视频序列。我们的核心发现是：任意运动中的三维空间点，只要在两个相机视野中同时可见，在实现精确同步后必然满足极线几何约束。基于这一原理，VisualSync通过整合现成的三维重建、特征匹配与密集追踪技术，提取运动轨迹片段、相对位姿及跨视角对应关系，进而通过联合最小化极线误差来估计各相机的时间偏移量。在四个多样化高难度数据集上的实验表明，VisualSync性能显著优于基线方法，其中位数同步误差控制在50毫秒以内。 |
| 生成视频中的物体运动速度比实际更慢：模型受制于亚地球重力影响，尚未掌握伽利略原理……目前如此。 | Varun Varma Thozhiyoor | [PDF](https://arxiv.org/pdf/2512.02016v1) | 视频生成器正日益被评估为潜在的世界模型，这要求它们能够编码并理解物理定律。我们研究了其对一项基本定律——引力的表征能力。未经专门调整的视频生成器持续生成以明显更慢加速度下落的物体。然而，这些物理测试常因度量尺度模糊而受到干扰。我们首先探究观察到的物理误差是否源于此类模糊性（例如错误的帧率假设）。研究发现，即使进行时间尺度重标定也无法修正高方差的引力伪影。为从这些干扰因素中严格分离出底层物理表征，我们提出一种无量纲的双物体测试方案，通过时序比 $t_1^2/t_2^2 = h_1/h_2$ 进行验证——该关系独立于重力加速度 $g$、焦距及尺度参数。这项相对性测试揭示了伽利略等效原理的违反现象。随后我们证明，通过定向专业化训练可部分弥补这种物理表征缺陷：仅使用100段单球下落视频微调的轻量级低秩适配器，能将有效重力加速度 $g_{\mathrm{eff}}$ 从 $1.81\,\mathrm{m/s^2}$ 提升至 $6.43\,\mathrm{m/s^2}$（达到地球重力的65%）。该专用适配器还能零样本泛化至双球下落与斜面运动场景，初步证明特定物理定律可通过极少量数据得到修正。 |
| 基于3D点轨迹的生成式视频运动编辑 | Yao-Chih Lee | [PDF](https://arxiv.org/pdf/2512.02015v1) | 相机与物体运动是视频叙事的关键要素。然而，如何精确编辑这些已捕捉的运动仍是重大挑战，尤其在复杂物体运动场景下更为突出。现有运动控制的图像转视频方法常因缺乏全景上下文而难以保持编辑一致性，而视频转视频技术虽能实现视角变换或基础物体位移，却对细粒度物体运动的控制能力有限。我们提出一种轨迹约束的视频转视频框架，能够实现对相机与物体运动的联合编辑。该框架通过将视频生成模型与源视频及成对的3D轨迹点进行耦合来实现这一目标——这些轨迹点分别表征源运动与目标运动，建立的稀疏对应关系可将丰富上下文从源视频迁移至新运动，同时保持时空连贯性。关键突破在于，相较于2D轨迹，3D轨迹提供的显式深度线索使模型能够解析深度层级关系并处理遮挡问题，从而实现精确运动编辑。通过合成数据与真实数据的双阶段训练，我们的模型支持多种运动编辑任务，包括相机/物体联合操控、运动迁移及非刚性形变，为视频编辑领域开启了全新的创作可能。 |
| TUNA：驯服统一视觉表征以构建原生统一多模态模型 | Zhiheng Liu | [PDF](https://arxiv.org/pdf/2512.02014v1) | 统一多模态模型（UMMs）旨在单一框架内联合执行多模态理解与生成任务。本文提出TUNA——一种原生统一多模态模型，通过将变分自编码器编码器与表征编码器级联，构建出统一的连续视觉表征空间。该统一表征空间支持对图像和视频进行端到端的理解与生成任务处理。相较于采用解耦表征的先前统一多模态模型，TUNA的统一视觉空间避免了因独立编码器引入的表征格式失配问题，在理解与生成任务上均优于解耦方案。此外，我们发现更强的预训练表征编码器能在所有多模态任务中持续提升性能，这凸显了表征编码器的重要性。最后，在此统一框架下，联合训练理解与生成数据能使两项任务相互促进而非相互干扰。我们在多模态理解与生成基准测试上的大量实验表明，TUNA在图像/视频理解、图像/视频生成以及图像编辑任务中均取得最先进成果，充分证明了其统一表征设计的有效性与可扩展性。 |
| 改进的平均流：论快速前向生成模型面临的挑战 | Zhengyang Geng | [PDF](https://arxiv.org/pdf/2512.02012v1) | 均值流（MeanFlow，MF）近期已被确立为一步生成建模的框架。然而，其"快速前向"特性在训练目标和引导机制两方面均引入了关键挑战。首先，原始MF的训练目标不仅依赖于基础的真实场，还依赖于网络本身。为解决这一问题，我们将目标重新表述为对瞬时速度$v$的损失函数，并通过预测平均速度$u$的网络进行重新参数化。我们的重构形成了更标准的回归问题，并提升了训练稳定性。其次，原始MF在训练期间固定了无分类器引导的尺度，这牺牲了灵活性。我们通过将引导机制构建为显式条件变量来解决此问题，从而在测试阶段保持灵活性。多样化的条件通过上下文条件处理机制进行处理，这既减少了模型规模，又提升了性能。总体而言，我们提出的$\textbf{改进型均值流}$（$\textbf{iMF}$）方法完全从零开始训练，在ImageNet 256$\times$256数据集上以单次函数评估（1-NFE）实现了$\textbf{1.72}$的FID分数。iMF显著超越了同类先前方法，并在不使用任何蒸馏技术的情况下缩小了与多步方法的差距。我们希望这项工作能进一步推动快速前向生成建模作为一个独立范式的发展。 |
| 四比六：采用自适应块缩放的更精确NVFP4量化方法 | Jack Cook | [PDF](https://arxiv.org/pdf/2512.02010v1) | 随着大语言模型规模不断扩大，低精度数值格式（如NVFP4）因其在计算速度与内存占用方面的优势而日益普及。然而，要利用NVFP4加速计算，所有矩阵乘法操作数——前向传播中的权重和激活值，以及反向传播中的权重、激活值与梯度——都必须量化为NVFP4格式，这往往导致训练过程中的发散现象和推理阶段的性能下降。传统NVFP4量化方法通过为每个数值块评估多个潜在缩放因子来实现量化。为应对这一挑战，本研究提出"四分之六"（4/6）改进方案，该方案对NVFP4量化算法进行修正，将每个数值块的潜在缩放因子评估数量精简为两个。与整数格式不同，FP4等浮点格式在每个数值块中对于接近最大值的数值会产生最显著的量化误差，我们发现这主要是导致下游性能下降的根源。研究表明，对于某些数值块，通过缩放至较小的FP4值域可使可表示数值的分布更均匀，从而提升接近最大值数值的表征精度。值得注意的是，4/6方案可在英伟达Blackwell GPU架构上高效实现，使得在NVFP4训练大语言模型时具备实际可行性。在基于Transformer与混合模型架构的预训练实验中，我们发现4/6方案在多种情况下能有效防止训练发散，相较于采用当前最先进NVFP4训练方案的模型，其训练损失显著更接近BF16基准。此外，4/6方案可轻松集成到多种训练后量化方法中，普遍提升下游任务准确率。我们期待这项工作能激发未来在NVFP4模型训练与部署领域的进一步探索。 |
| AirSim360：无人机视角下的全景仿真平台 | Xian Ge | [PDF](https://arxiv.org/pdf/2512.02009v1) | 360度全方位理解领域因推动空间智能发展而受到日益广泛的关注，但大规模多样化数据的缺乏仍是主要制约因素。本研究提出AirSim360——一个面向空中视角全方位数据的仿真平台，能够通过无人机实现大范围场景采样。该平台聚焦三个核心维度：采用渲染对齐的数据与标注范式，实现像素级几何、语义及实体层面的理解；构建交互式行人感知系统以模拟人类行为；建立自动化轨迹生成范式以支持导航任务。此外，我们采集了超过6万组全景样本，并通过多任务实验验证了仿真平台的有效性。与现有仿真器相比，本研究首次在全方位设定下系统化构建了四维真实世界模型。完整平台（含工具包、插件及数据集）将通过https://insta360-research-team.github.io/AirSim360-website 开源发布。 |
