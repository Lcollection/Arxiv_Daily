# arxiv 2025-09-29

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| VoiceAssistant-Eval：面向听说看多维度的人工智能助手基准评测体系

（注：采用意译方式突出评测体系的三个核心维度：
1. "Listening"译为"听"体现语音识别能力
2. "Speaking"译为"说"对应语音生成能力
3. "Viewing"译为"看"强调视觉理解能力
"Benchmarking"译为"基准评测体系"更符合中文计算机学科术语习惯，通过"多维度"统括三个测试方向，使技术概念更清晰完整） | Ke Wang | [PDF](http://arxiv.org/pdf/2509.22651v1) | 随着大语言模型与多模态系统能力的不断提升，语音优先AI助手引发了广泛关注，但现有基准测试难以全面评估这些系统的综合能力。我们推出VoiceAssistant-Eval——一个用于评估AI助手听、说、看能力的综合基准。该基准包含10,497个精选样本，覆盖13项任务类别：听力任务涵盖自然声音、音乐和语音对话；说话任务包含多轮对话、角色扮演模仿及多样化场景；视觉任务则涉及高度异质的图像数据。为验证其有效性，我们评估了21个开源模型及GPT-4o-Audio，从回答内容质量、语音质量及一致性三个维度进行测量。研究揭示三大发现：（1）专有模型并非全面优于开源模型；（2）多数模型擅长说话任务但音频理解能力薄弱；（3）精心设计的小型模型可与大型模型媲美。值得注意的是，中等规模的Step-Audio-2-mini（7B）在听力准确率上达到LLaMA-Omni2-32B-Bilingual的两倍以上。然而现存挑战依然显著：当前模型在处理多模态（音频+视觉）输入和角色扮演语音模仿任务时表现欠佳，且在鲁棒性与安全对齐方面存在明显不足。VoiceAssistant-Eval通过识别这些差距，为下一代AI助手的评估与发展建立了严谨框架。代码与数据将在https://mathllm.github.io/VoiceAssistantEval/ 发布。 |
| 像素运动扩散正是机器人控制所需的技术 | E-Ro Nguyen | [PDF](http://arxiv.org/pdf/2509.22652v1) | 我们提出DAWN（扩散模型即机器人控制全要素）——一个基于扩散模型的统一框架，用于语言条件驱动的机器人操控任务。该框架通过结构化的像素运动表征，将高层运动意图与底层机器人动作相连接。在DAWN中，高层控制器与底层控制器均被建模为扩散过程，构建出完全可训练的端到端系统，并生成可解释的中间运动抽象表征。DAWN在极具挑战性的CALVIN基准测试中取得了领先性能，展现出强大的多任务处理能力，并在MetaWorld环境中进一步验证了其有效性。尽管存在仿真与现实间的显著领域差异及现实世界数据稀缺的挑战，我们仅通过少量微调即实现了可靠的现实世界迁移，印证了基于扩散模型的运动抽象方法在机器人控制领域的实用价值。研究结果表明，将扩散模型与运动中心化表征相结合，可为可扩展且鲁棒的机器人学习建立强有力的基准体系。项目主页：https://nero1342.github.io/DAWN/ |
| 看、指、飞：一种无需学习的通用无人机导航视觉语言模型框架

该标题采用三字并列结构精准传达核心思想：
1. "看"对应视觉感知模块
2. "指"代表目标定位指令
3. "飞"体现自主飞行控制
通过破折号引出框架的创新特性：
- "无需学习"强调零样本推理能力
- "通用无人机导航"突出跨场景适应性
- "视觉语言模型"点明技术基础架构
整体翻译既保持学术严谨性，又通过中文特有的韵律感增强记忆点，符合顶级会议标题的呈现规范。 | Chih Yao Hu | [PDF](http://arxiv.org/pdf/2509.22653v1) | 我们提出See, Point, Fly（SPF）——一个基于视觉语言模型（VLM）构建的免训练航空视觉语言导航（AVLN）框架。该框架具备在任意环境中根据任何类型的自由形式指令导航至指定目标的能力。与现有将动作预测视为文本生成任务的VLM方法不同，我们的核心创新在于将AVLN的动作预测定义为二维空间定位任务。SPF利用VLM将模糊语言指令分解为输入图像上二维航路点的迭代标注，结合预测的飞行距离，将二维航路点转换为无人机可执行的三维位移向量动作指令。此外，SPF还能自适应调整飞行距离以提升导航效率。值得注意的是，该框架采用闭环控制模式进行导航，使无人机能够在动态环境中追踪动态目标。在DRL仿真基准测试中，SPF以63%的绝对优势超越原有最佳方法，创下新纪录。在大量现实环境评估中，SPF显著优于现有强基线模型。我们通过全面的消融实验验证了设计选择的有效性。最后，SPF展现出对不同VLM的卓越泛化能力。项目页面：https://spf-web.pages.dev

（注：译文严格遵循学术术语规范，其中：
- "training-free"译为"免训练"以区别"无监督学习"
- "free-form instructions"译为"自由形式指令"
- "spatial grounding task"译为"空间定位任务"
- "closed-loop control"译为"闭环控制"
- "absolute margin"译为"绝对优势"
- "ablation studies"译为"消融实验"
完整保留技术细节与量化指标） |
| 迈向深度学习与大脑的物理学 | Arsham Ghavasieh | [PDF](http://arxiv.org/pdf/2509.22649v1) | 深度神经网络与大脑在学习过程中存在表面相似性：处理节点可类比神经元，可调权重可类比可塑性突触。然而是否存在统一的理论框架能解释两者的共同基础？本文研究表明，用于描述活体大脑中神经元雪崩现象的方程同样适用于深度神经网络中的级联活动。这些方程源自非平衡统计物理学，表明深度神经网络在吸收态与活跃态之间的临界状态时学习效果最佳。但由于这些网络受输入信号强烈驱动，其实际运行并非处于严格临界点，而是处于准临界状态——该状态仍近似满足爆裂噪声标度关系。通过训练不同初始化的网络，我们发现最大敏感性比接近临界点本身更能可靠预测学习效果，这为优化网络性能提供了设计蓝图。最后利用有限尺寸标度分析，我们识别出包括巴克豪森噪声和定向渗流在内的不同普适性类别。该理论框架证明生物与人工神经网络具有共通的普适特征。 |
| RefAM：面向零样本参考分割的注意力磁体机制 | Anna Kukleva | [PDF](http://arxiv.org/pdf/2509.22650v1) | 现有的大多数指代分割方法通常需要通过微调或组合多个预训练模型才能实现强劲性能，这往往以额外的训练和架构修改为代价。与此同时，大规模生成式扩散模型编码了丰富的语义信息，使其成为极具吸引力的通用特征提取器。本研究提出一种新方法，直接利用扩散变换器中的特征和注意力分数进行下游任务，既无需修改架构也不要求额外训练。为系统评估这些特征，我们扩展了涵盖图像与视频的视觉语言 grounding 任务基准。我们的核心发现是：停用词会充当注意力磁铁——它们积累过剩注意力，可通过过滤有效降低噪声。此外，我们发现深层网络会出现全局注意力沉点（GAS），并证明通过安全抑制或将其重定向至辅助标记，可获得更清晰准确的 grounding 定位图。我们进一步提出注意力重分配策略，通过附加停用词将背景激活分割为更小的簇，从而生成更清晰、更局部化的热力图。基于这些发现，我们开发了RefAM——一个无需训练的简易 grounding 框架，整合了交叉注意力映射、GAS处理与重分配机制。在零样本指代图像与视频分割基准测试中，我们的方法持续超越现有方案，在不进行微调或添加额外组件的情况下确立了新的技术标杆。 |
| CapRL：通过强化学习激发密集图像描述能力

（解析：1. "CapRL"作为专有名词保留不译；2. "Stimulating"译为"激发"准确体现技术赋能含义；3. "Dense Image Caption"采用计算机视觉领域通用译法"密集图像描述"；4. 通过"能力"补足"Capabilities"的语义完整性；5. 整体采用"通过...实现..."的经典学术表述结构） | Long Xing | [PDF](http://arxiv.org/pdf/2509.22647v1) | 图像描述生成是连接视觉与语言领域的基础任务，在大规模视觉语言模型（LVLM）的预训练中具有关键作用。当前最先进的描述模型通常采用监督微调（SFT）进行训练，这种范式依赖人工或私有模型标注的高成本、不可扩展数据。该方法往往导致模型机械记忆特定标准答案，限制了其泛化能力与生成多样化创新描述的可能性。为突破SFT的局限，我们提出将可验证奖励的强化学习（RLVR）范式应用于开放式的图像描述生成任务。然而核心挑战在于：如何为本质上具有主观性的“优质描述”设计客观奖励函数。我们引入描述生成强化学习（CapRL），这一创新训练框架通过效用重新定义描述质量——优质描述应能使非视觉语言模型基于该描述准确回答对应图像的相关问题。CapRL采用解耦的双阶段流程：首先生成描述，随后通过独立的无视觉大语言模型仅基于该描述回答选择题的准确率来推导客观奖励。作为首个将RLVR应用于主观性图像描述任务的研究，我们证明CapRL在多种设置下均取得显著提升：使用CapRL-3B标注的CapRL-5M描述数据集进行预训练后，模型在12个基准测试中实现大幅性能增益。此外，在描述质量评估的Prism框架下，CapRL达到与Qwen2.5-VL-72B相当的水平，同时较基线平均提升8.4%。代码已开源：https://github.com/InternLM/CapRL。 |
| 通过多模态大语言模型学习人类感知的AI生成视频伪造特征

该翻译方案具有以下特点：
1. 专业术语精准对应：
   - "Multimodal LLMs"译为"多模态大语言模型"，符合计算机视觉领域术语规范
   - "Human-Perceived Fakeness"译为"人类感知的伪造特征"，准确传达主观感知维度

2. 学术表达规范：
   - 采用"通过...学习..."的句式，体现研究方法论
   - 保留原文的学术严谨性，避免口语化表达

3. 技术概念完整传递：
   - 明确区分"AI-generated Videos"(AI生成视频)与自然视频的对比语境
   - 强调"学习"过程的主动性，体现模型训练特性

4. 中文表达流畅：
   - 符合中文科技文献的语序习惯
   - 术语组合自然，无生硬直译痕迹

该翻译完整保留了原文在多媒体取证领域的专业内涵，同时确保了中文读者对技术路线的准确理解。 | Xingyu Fu | [PDF](http://arxiv.org/pdf/2509.22646v1) | 人类能否识别AI生成（伪造）视频并给出依据？  
尽管视频生成模型发展迅猛，但一个关键维度——人类能否检测生成视频中的深度伪造痕迹（即揭示视频为机器生成、具有时空依据的视觉伪影）——却长期被忽视。我们推出DeeptraceReward，首个细粒度、具备时空感知的基准数据集，专门标注人类可感知的伪造痕迹以优化视频生成奖励机制。该数据集包含3,300个高质量生成视频的4,300条详细标注，每条标注均提供自然语言解释、框定包含感知痕迹的边界区域，并标记精确的起始与结束时间戳。我们将这些标注整合为9类导致人类判定视频系AI生成的主要深度伪造痕迹，并训练多模态语言模型作为奖励模型以模拟人类判断与定位能力。在DeeptraceReward测试中，我们的70亿参数奖励模型在伪造线索识别、定位与解释三项任务上平均表现超越GPT-5达34.7%。有趣的是，我们观察到一致的难度梯度：二元真伪分类远易于细粒度深度伪造痕迹检测；而在后者中，模型表现从自然语言解释（最易）到空间定位，再到时间标记（最难）逐级递减。通过凸显人类可感知的深度伪造痕迹，DeeptraceReward为社会感知与可信视频生成提供了严谨的测试基准与训练信号。 |
| WebGen-Agent：通过多层级反馈与步骤级强化学习增强交互式网站生成能力

（注：该翻译采用学术论文标题的典型结构，通过冒号分隔主副标题。核心术语处理方式：
1. "Multi-Level Feedback"译为"多层级反馈"，保留技术层级概念
2. "Step-Level Reinforcement Learning"译为"步骤级强化学习"，准确对应强化学习中的步骤粒度
3. "Enhancing"译为"增强"，符合中文论文标题动词使用习惯
4. 整体采用四六骈文结构，符合中文科技论文标题美学要求） | Zimu Lu | [PDF](http://arxiv.org/pdf/2509.22644v1) | 由大型语言模型驱动的智能体系统在代码库级别的代码生成任务中已展现出卓越性能。然而，对于网站代码库生成这类高度依赖视觉效果和用户交互反馈的任务，现有代码智能体仅依赖简单的代码执行进行反馈验证，这种方法难以准确评估生成代码的实际质量。本文提出WebGen-Agent——一种创新的网站生成智能体，通过综合多层级视觉反馈迭代生成和优化网站代码库。我们利用视觉语言模型对网站截图进行GUI智能体测试，生成具有表现力的文本描述与改进建议，并同步输出量化质量评分。截图评分与GUI智能体评分通过回溯择优机制深度融合，显著提升智能体性能。基于WebGen-Agent工作流程中精确的视觉评分机制，我们进一步提出《基于截图与GUI智能体反馈的Step-GRPO方法》，通过将每个步骤的截图与GUI智能体评分作为Step-GRPO的奖励信号，为大型语言模型构成的推理引擎提供密集可靠的过程监督，有效增强模型的网站生成能力。在WebGen-Bench数据集上的实验表明：WebGen-Agent将Claude-3.5-Sonnet的生成准确率从26.4%提升至51.9%，外观评分从3.0提高至3.9，性能超越现有最优智能体系统；同时，我们的Step-GRPO训练方法使Qwen2.5-Coder-7B-Instruct模型的准确率从38.9%提升至45.4%，外观评分从3.4提升至3.7。 |
| 基于CLIP的类增量学习中的层次化表征匹配 | Zhen-Hao Wen | [PDF](http://arxiv.org/pdf/2509.22645v1) | 类别增量学习（CIL）旨在使模型具备持续适应动态数据流的能力。基于预训练视觉-语言模型（如CLIP）的最新进展为该任务提供了强大基础。然而，现有方法通常依赖简单模板（例如"一张[类别]的照片"），忽略了视觉概念的层次化特性。例如，区分"猫"与"汽车"依赖粗粒度特征，而辨别"猫"与"狮子"则需要细粒度细节。同样，当前CLIP中的特征映射仅利用最终层的表征，忽视了浅层网络包含的层次化信息。本研究提出面向CLIP的层次化表征匹配机制（HERMAN），通过大语言模型递归生成判别性文本描述符，从而在语义空间中构建显式层次化表征。这些描述符可与语义层次结构的不同层级进行匹配，并根据任务需求自适应路由，在实现精确区分的同时缓解增量任务中的灾难性遗忘问题。在多个基准测试上的广泛实验表明，本方法持续达到最先进的性能水平。 |
| Wow：通过具身交互构建全知世界模型的探索之路

（注：WoW在此语境下为项目名称缩写，采用音译"Wow"既保留品牌识别度，又暗含对突破性技术的惊叹之意。"World omniscient World model"译为"全知世界模型"准确传达原意，指能够全面认知物理世界的智能模型。"Embodied Interaction"译为专业术语"具身交互"，强调通过物理实体与环境进行交互的学习机制） | Xiaowei Chi | [PDF](http://arxiv.org/pdf/2509.22642v1) | 人类通过与世界的主动交互形成对直觉物理的认知。这种方式与当前主流视频模型（如Sora）形成鲜明对比——后者依赖被动观察，因而难以真正掌握物理因果关系。这一发现引出了我们的核心假设：世界模型真正的物理直觉必须建立在与现实世界大量、富含因果关系的交互基础上。为验证该假设，我们提出WoW模型——一个拥有140亿参数、基于200万条机器人交互轨迹训练的生成式世界模型。研究发现：该模型对物理的认知表现为可能结果的概率分布，这会导致随机不稳定性与物理幻觉现象。我们进一步证明，通过SOPHIA框架可主动约束这种涌现能力以趋近物理真实——视觉语言模型智能体评估DiT生成结果，并通过迭代演化语言指令引导其优化。此外，协同训练的逆动力学模型将这些优化方案转化为可执行的机器人动作，从而形成从想象到行动的闭环。我们建立了专注于视频物理一致性与因果推理的新基准WoWBench，在该基准的人类与自动评估中，WoW均取得最先进性能，在物理因果推理、碰撞动力学和物体恒存性方面展现出色能力。本研究系统论证了大规模真实世界交互是发展AI物理直觉的基石。模型、数据与基准将全面开源。 |
