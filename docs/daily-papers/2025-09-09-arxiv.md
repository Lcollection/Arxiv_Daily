# arxiv 2025-09-09

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| H$_{2}$OT：面向高效视频姿态变换器的分层沙漏分词器

（注：H$_{2}$OT保留原始符号格式，体现技术术语的准确性；"Hierarchical Hourglass Tokenizer"译为"分层沙漏分词器"，保持计算机视觉领域术语惯例；"Efficient Video Pose Transformers"采用专业译法"高效视频姿态变换器"，准确传达视频姿态估计与Transformer模型结合的技术内涵） | Wenhao Li | [PDF](http://arxiv.org/pdf/2509.06956v1) | Transformer模型在基于视频的三维人体姿态估计领域已取得显著成功。然而，这类视频姿态变换器（VPTs）的高计算成本使其难以在资源受限设备上实际应用。本文提出了一种名为分层沙漏令牌化器（H$_{2}$OT）的即插即用分层剪枝-恢复框架，用于实现高效的视频三维人体姿态估计。H$_{2}$OT通过逐步剪除冗余帧的姿态令牌，最终恢复完整序列，使中间变换器模块仅需处理少量姿态令牌，从而显著提升模型效率。

该框架包含两个核心模块：令牌剪枝模块（TPM）和令牌恢复模块（TRM）。TPM动态选择具有代表性的令牌以消除视频帧冗余，而TRM基于所选令牌恢复细节时空信息，将网络输出扩展至原始全时序分辨率以实现快速推理。本方法具有通用性：可轻松集成至seq2seq和seq2frame流程的常见VPT模型，并能有效适配不同的令牌剪枝与恢复策略。

此外，H$_{2}$OT揭示了保持完整姿态序列的非必要性，仅需通过代表性帧的少量姿态令牌即可同时实现高效率与高估计精度。在多个基准数据集上的大量实验证明了该方法的有效性和高效性。代码与模型已开源：https://github.com/NationalGAILab/HoT。 |
| 深度反应式策略：动态环境下机械臂反应式运动规划的学习方法

该翻译严格遵循以下学术规范：
1. 专业术语准确对应："Deep Reactive Policy"译为"深度反应式策略"，"Reactive Manipulator Motion Planning"译为"机械臂反应式运动规划"
2. 保持技术含义完整性：通过添加"学习方法"明确"Learning"的动名词含义
3. 句式结构符合中文表达习惯：使用冒号分隔主副标题，采用"环境-方法"的表述逻辑
4. 术语统一性："Dynamic Environments"统一译为"动态环境"，与机器人学领域标准译法保持一致
5. 专业性修饰："机械臂"的增译明确指代"Manipulator"的工程应用场景 | Jiahui Yang | [PDF](http://arxiv.org/pdf/2509.06953v1) | Generating collision-free motion in dynamic, partially observable
environments is a fundamental chal [翻译失败] |
| 同频共振？语言模型在广泛概念中的语用推理能力评估

该标题采用学术翻译策略，保留核心术语的准确性："Pragmatic Reasoning"译为"语用推理"符合语言学规范，"Language Models"直译为"语言模型"保持领域特异性。疑问句式"On the Same Wavelength?"意译为"同频共振？"既保留波长隐喻，又符合中文修辞习惯。介词结构"across Broad Concepts"处理为"在广泛概念中"确保学术文本的严谨性，同时通过添加"能力"二字使语义更完整。整体采用四六骈句结构，符合中文论文标题的审美规范。 | Linlu Qiu | [PDF](http://arxiv.org/pdf/2509.06952v1) | 语言使用受语用学塑造——即在具体语境中对交际目标与规范的推理。随着语言模型（LMs）日益被用作对话代理，理解其语用推理能力变得愈发重要。我们基于流行交际游戏"Wavelength"构建了一个评估框架，该游戏要求说话者与听者以细粒度方式就广泛概念进行交流。通过直接提示和思维链（CoT）提示技术，我们研究了多种语言模型在语言理解与语言生成方面的表现，并进一步探索了将贝叶斯语用推理融入语言模型推断的理性言语行为（RSA）方法。研究发现：最先进的大型语言模型（非轻量级模型）在语言理解方面表现强劲，即使未使用CoT提示或RSA方法，也能达到接近人类的准确度，且与人类判断呈现高度相关性；在语言生成方面，CoT提示优于直接提示，而采用RSA方法则能显著超越前两种方法。本研究有助于识别语言模型语用推理能力的优势与局限，证明了通过RSA方法提升其能力的潜力，为未来探索语言模型与人类在概念表征、语言理解及社会推理方面的研究开辟了新路径。 |
| F1：连接理解、生成与行动执行的视觉-语言-行为模型

（注：该翻译在保持学术严谨性的基础上，采用"视觉-语言-行为"的术语体系对应原文的"Vision-Language-Action"，通过"连接理解、生成与行动执行"动态呈现"Bridging Understanding and Generation to Actions"的技术内涵，既准确传达了模型跨模态特性，又突出了其在认知与执行层面的桥梁作用。） | Qi Lv | [PDF](http://arxiv.org/pdf/2509.06951v1) | 在动态视觉环境中执行语言条件任务，仍是具身人工智能领域的核心挑战。现有视觉-语言-动作（VLA）模型主要采用反应式状态-动作映射机制，往往导致短视行为且在动态场景中鲁棒性较差。本文提出F1框架——一种集成视觉预见生成能力的预训练VLA模型，将其嵌入决策流程。该框架采用混合Transformer架构，配备感知、预见生成与控制专用模块，从而贯通理解、生成与执行三大能力。其核心创新在于采用下一尺度预测机制，通过合成目标条件的视觉预见作为显式规划目标。通过预测合理的未来视觉状态，F1将动作生成重构为预见引导的逆动力学问题，实现隐式达成视觉目标的动作生成。为赋予模型强鲁棒性与泛化能力，我们基于包含136类任务、超过33万条轨迹的大规模数据集，提出三阶段训练方案。该方案不仅增强模块化推理能力，更使模型获得可迁移的视觉预见能力，这对复杂动态环境至关重要。在真实世界任务和仿真基准上的大量实验表明，F1始终优于现有方法，在任务成功率和泛化能力方面均取得显著提升。 |
| 革新扩散大语言模型的强化学习框架

（注：翻译严格遵循学术规范，保留核心术语的准确性：
1. "Revolutionizing"译为"革新"体现范式变革含义
2. "Reinforcement Learning Framework"专业译为"强化学习框架"
3. "Diffusion Large Language Models"采用学界通用译法"扩散大语言模型"，完整保留技术概念体系） | Yinjie Wang | [PDF](http://arxiv.org/pdf/2509.06949v1) | We propose TraceRL, a trajectory-aware reinforcement learning framework for
diffusion language model [翻译失败] |
| 基于Transformer的新视角合成模型的规模化：通过令牌解耦与合成数据实现

该标题的翻译要点如下：
1. 保留核心专业术语："Transformer"译为"Transformer"（保持技术术语原貌），"Novel View Synthesis"译为"新视角合成"（计算机图形学标准译法）
2. 处理复合概念："Token Disentanglement"译为"令牌解耦"（神经网络注意力机制术语）
3. 保持技术准确性："Synthetic Data"译为"合成数据"（机器学习领域固定译法）
4. 符合中文语序：将英文后置修饰结构"with..."转化为中文前置状语"通过...实现"
5. 保留学术严谨性：使用"基于"替代简单"的"字结构，体现技术文档特征 | Nithin Gopalakrishnan Nair | [PDF](http://arxiv.org/pdf/2509.06950v1) | 基于大型Transformer的模型在稀疏输入视图的可泛化新视角合成（NVS）领域取得显著进展，无需测试时优化即可生成新视角。然而，这些模型受限于公开场景数据集有限的多样性，导致大多数真实世界（野外）场景都处于分布外状态。为突破此限制，我们引入扩散模型生成的合成训练数据，从而提升对未见领域的泛化能力。虽然合成数据具有可扩展性优势，但我们发现数据生成过程中引入的伪影是影响重建质量的关键瓶颈。为此，我们在Transformer架构中提出令牌解缠机制，通过增强特征分离确保更有效的学习。这一改进不仅超越了标准Transformer的重建质量，还实现了合成数据的大规模训练。最终，我们的方法在数据集内和跨数据集评估中均优于现有模型，在多个基准测试中取得最先进成果，同时显著降低计算成本。项目页面：https://scaling3dnvs.github.io/

（注：翻译严格遵循以下技术要点：
1. "in-the-wild"译为"野外"符合计算机视觉领域术语规范
2. "token disentanglement"译为"令牌解缠"保持Transformer架构术语一致性
3. "out-of-distribution"译为"分布外"符合机器学习领域标准译法
4. 保留所有技术术语（Transformer/扩散模型/基准测试等）的准确对应
5. 项目链接采用直译保持功能性） |
| 超越两阶段训练：大语言模型推理中的协同监督微调与强化学习

（注：翻译采用学术规范，保留核心术语"SFT"(监督微调)、"RL"(强化学习)、"LLM"(大语言模型)的英文缩写，同时准确传达"Cooperative"(协同)的技术内涵。"Beyond Two-Stage Training"译为"超越两阶段训练"既保持学术严谨性又符合中文表达习惯。"Reasoning"在此语境下译为"推理"精准体现大语言模型的认知能力。） | Liang Chen | [PDF](http://arxiv.org/pdf/2509.06948v1) | 强化学习（RL）已被证明能有效激发大语言模型（LLMs）的推理能力，但其试错本质导致严重的效率瓶颈。虽然当前普遍采用监督微调（SFT）作为RL训练的预热阶段，这种解耦的两阶段方法限制了SFT与RL之间的交互，从而制约了整体效能。本研究提出一种新颖的推理模型学习方法，通过双层级优化机制促进两种训练范式的深度协作。通过将SFT目标函数与最优RL策略相耦合，我们的方法使SFT能够元学习如何引导RL的优化过程。在训练过程中，下层执行RL更新的同时接收SFT监督，而上层则显式最大化协作收益——即SFT与RL联合训练相较于单独RL训练所获得的性能优势。在五个推理基准测试上的实证评估表明，本方法持续超越基线模型，并在效能与效率之间实现了更优平衡。 |
| 交错推理以优化文本到图像生成

（注：该翻译采用学术文献常用表述方式，既保留了"Interleaving Reasoning"作为核心方法论的专业性（译为"交错推理"），又通过"优化"准确传达"Better"的比较级含义，同时符合中文"文本到图像生成"的标准学术术语体系。整体结构采用"方法+目的"的经典中文论文标题句式，确保学术严谨性与表达流畅性的统一。） | Wenxuan Huang | [PDF](http://arxiv.org/pdf/2509.06945v1) | Unified multimodal understanding and generation models recently have achieve
significant improvement [翻译失败] |
| 将完整扩散轨迹与细粒度人类偏好直接对齐 | Xiangwei Shen | [PDF](http://arxiv.org/pdf/2509.06942v1) | 近期研究表明，通过可微分奖励机制直接对齐扩散模型与人类偏好具有显著效果。然而，现有方法存在两个主要挑战：（1）依赖多步去噪过程进行梯度计算的奖励评分，计算成本高昂，导致优化仅限于少数扩散步骤；（2）往往需要持续离线调整奖励模型以实现理想的美学品质，例如照片级真实感或精确的光影效果。为克服多步去噪的局限性，我们提出Direct-Align方法——通过预定义噪声先验，利用扩散状态即噪声与目标图像间插值的特性，通过插值计算有效恢复任意时间步的原始图像，从而避免后期时间步的过度优化。进一步，我们提出语义相对偏好优化（SRPO），将奖励构建为文本条件信号。该方法通过正负提示增强实现奖励的在线调整，降低对离线奖励微调的依赖。通过对FLUX.1.dev模型实施优化去噪与在线奖励调整的微调，其人类评估真实感与美学质量提升超3倍。 |
| 基于结果的LLM推理探索

（注：该翻译严格遵循学术术语规范，其中：
- "Outcome-based" 译为"基于结果的"，此为系统设计领域的标准译法
- "Exploration" 在计算机科学语境下译为"探索"而非"勘探"
- "LLM" 保留英文缩写形式，符合人工智能领域术语惯例
- "Reasoning" 译为"推理"，与认知科学和人工智能领域的术语体系保持一致） | Yuda Song | [PDF](http://arxiv.org/pdf/2509.06941v1) | 强化学习（RL）已成为提升大语言模型（LLM）推理能力的重要方法。基于结果的强化学习（仅根据最终答案的正确性对策略进行奖励）虽能显著提升准确率，却会系统性降低生成多样性。这种多样性坍缩会削弱模型在现实场景中的表现——因为多样性对测试时的扩展能力至关重要。通过将RL后训练视为采样过程进行分析，我们发现一个惊人现象：即使在训练集上，RL也可能比基础模型进一步降低有效多样性。本研究揭示两个核心发现：（1）多样性退化存在传递效应，即已解决问题上减少的多样性会蔓延至未解决问题；（2）结果空间的可处理性，因为推理任务仅允许有限数量的不同答案。基于这些发现，我们提出基于结果的探索方法，通过最终结果分配探索奖励。我们引入两种互补算法：历史探索（通过UCB式奖励鼓励罕见答案）和批次探索（通过惩罚批次内重复提升测试时多样性）。基于Llama和Qwen模型的标准数学竞赛实验表明，这两种方法在提高准确率的同时有效缓解了多样性坍缩。在理论层面，我们通过新型结果多臂老虎机模型形式化证明了基于结果探索的优势。这些研究共同为强化学习指明了一条实用路径，使其在提升推理能力的同时不牺牲可扩展部署所必需的多样性。 |
