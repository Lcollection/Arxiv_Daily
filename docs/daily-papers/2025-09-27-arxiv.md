# arxiv 2025-09-27

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| SciReasoner：奠定跨学科科学推理基础 | Yizhou Wang | [PDF](http://arxiv.org/pdf/2509.21320v1) | 我们提出一种科学推理基础模型，该模型实现了自然语言与异构科学表征的对齐。该模型基于包含科学文本、纯序列及序列-文本对的206B标记语料库进行预训练，随后通过40M指令进行指令微调对齐，采用退火冷启动自举法激发长链思维推理，并结合任务特定奖励塑造的强化学习机制，从而培养出严谨的科学推理能力。该模型支持五大能力体系，覆盖工作流中多达103项任务：(i)文本与科学格式间的精准转换，(ii)文本/知识提取，(iii)属性预测，(iv)属性分类，(v)无条件与条件序列生成与设计。相较于专业系统，我们的方法拓展了指令覆盖范围，提升了跨领域泛化能力，并增强了输出保真度。我们详细阐述了数据策展与训练流程，论证了跨学科学习如何强化知识迁移与下游任务可靠性。该模型、指令微调数据集及评估代码已开源，访问地址为https://huggingface.co/SciReason 与 https://github.com/open-sciencelab/SciReason。 |
| RLBFF：二进制灵活反馈——连接人类反馈与可验证奖励的桥梁 | Zhilin Wang | [PDF](http://arxiv.org/pdf/2509.21319v1) | Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards [翻译失败] |
| SD3.5-Flash：基于分布引导的生成流蒸馏模型

（解析：该翻译通过以下方式实现学术准确性：
1. 保留核心技术代号"SD3.5-Flash"不翻译
2. 将"Distribution-Guided Distillation"译为专业术语"基于分布引导的蒸馏"
3. 使用生成式AI领域的标准译法"生成流"对应"Generative Flows"
4. 采用"模型"作为隐含的技术主体补充，符合中文论文标题惯例
5. 保持原标题的破折号结构和技术层次关系） | Hmrishav Bandyopadhyay | [PDF](http://arxiv.org/pdf/2509.21318v1) | 我们推出SD3.5-Flash——一种高效少步蒸馏框架，将高质量图像生成能力引入普及型消费设备。该方法通过专为少步生成设计的重构分布匹配目标，对计算成本高昂的修正流模型进行蒸馏优化。我们引入两项关键创新："时间步共享"机制降低梯度噪声，"分时步微调"技术提升提示对齐精度。结合文本编码器重构与专用量化等全流程优化，本系统可在不同硬件配置上实现高速生成与内存高效部署，使从手机到台式机的全谱系设备均能获得先进生成能力。通过包含大规模用户研究在内的综合评估，我们证明SD3.5-Flash持续超越现有少步生成方法，真正实现先进生成式AI技术的普惠化部署。 |
| 基于主动用户指令的交互式推荐代理 | Jiakai Tang | [PDF](http://arxiv.org/pdf/2509.21317v1) | 传统推荐系统依赖被动反馈机制，仅允许用户进行"喜欢/不喜欢"等简单选择。然而这类粗粒度信号无法捕捉用户细粒度的行为动机与意图，导致现有系统难以区分驱动用户满意度的具体物品属性，从而造成偏好建模失准。这些根本性局限使用户意图与系统解读间存在持续性偏差，最终既损害用户体验又影响系统效能。

为突破这些限制，我们提出交互式推荐信息流（IRF）这一创新范式，首次在主流推荐流中实现自然语言指令交互。与传统系统将用户局限于被动隐式行为影响不同，IRF通过实时语言指令赋予用户对推荐策略的主动显式控制权。为支撑该范式，我们开发了RecBot双智能体架构：解析智能体将语言表达转化为结构化偏好，规划智能体则动态编排自适应工具链实现实时策略调整。为实现实际部署，我们采用仿真增强的知识蒸馏技术，在保持强大推理能力的同时实现高效性能。经大规模离线实验与长期在线测试，RecBot在用户满意度与商业指标上均展现出显著提升。 |
| SAGE：语义理解领域的现实基准测试 | Samarth Goel | [PDF](http://arxiv.org/pdf/2509.21310v1) | As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an ur [翻译失败] |
| 牛顿生成器：基于神经牛顿动力学的物理一致且可控的文本到视频生成技术

（注：该翻译采用学术术语规范化处理，保留核心概念"Neural Newtonian Dynamics"的直译"神经牛顿动力学"，同时通过"物理一致""可控"准确传达"Physics-Consistent and Controllable"的技术特性，采用"文本到视频生成"这一领域标准表述对应"Text-to-Video Generation"。） | Yu Yuan | [PDF](http://arxiv.org/pdf/2509.21309v1) | 当前大规模文本到视频生成的主要瓶颈在于物理一致性与可控性。尽管近期取得进展，最先进的模型仍常产生违反物理规律的运动，例如物体向上坠落、速度与方向的突变等。更为关键的是，现有模型缺乏精确的参数控制能力，难以在不同初始条件下生成物理规律一致的运动动态。我们认为这一根本性局限源于当前模型仅从外观特征学习运动分布，而缺乏对底层动力学原理的理解。本研究提出NewtonGen框架，将数据驱动合成与可学习的物理原理相融合。其核心是可训练的神经牛顿动力学模块，能够建模并预测多种牛顿力学运动，从而将隐式动力学约束注入视频生成过程。通过协同利用数据先验与动力学指导，NewtonGen实现了具有精确参数控制的物理一致性视频合成。 |
| 谄媚并非单一现象：大语言模型中谄媚行为的因果分离

（注：译文采用学术论文标题的简洁风格，通过冒号实现主副标题结构。"Causal Separation"译为"因果分离"以保持认知语言学领域的术语准确性，使用"大语言模型"对应"LLMs"这一通用译法，整体句式符合中文社科论文标题的凝练特征。） | Daniel Vennemeyer | [PDF](http://arxiv.org/pdf/2509.21305v1) | 大型语言模型（LLMs）常表现出谄媚行为——例如过度迎合用户或进行奉承——但尚不清楚这些行为源于单一机制还是多个不同过程。我们将谄媚行为分解为迎合式赞同与奉承式赞美，并将其与真诚赞同进行对比。通过在不同模型和数据集上应用均值差异方向、激活叠加及子空间几何分析方法，我们发现：（1）这三种行为在潜在空间中沿不同线性方向编码；（2）每种行为均可被独立增强或抑制而不影响其他行为；（3）其表征结构在不同模型家族和规模中保持一致性。这些结果表明，谄媚行为对应着相互独立且可分别调控的差异化表征。 |
| 量化视觉几何基础变换器 | Weilun Feng | [PDF](http://arxiv.org/pdf/2509.21302v1) | Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs [翻译失败] |
| 无先验，无泄露：重审训练神经网络中的重构攻击

（注：该翻译采用学术论文标题的经典对仗结构，通过"无...无..."的排比句式突出核心论点。"Revisiting"译为"重审"体现学术研究的批判性视角，"Trained Neural Networks"采用"训练神经网络"这一通用译法确保专业性。标题整体在保持学术严谨性的同时，兼顾中文表达习惯。） | Yehonatan Refael | [PDF](http://arxiv.org/pdf/2509.21296v1) | 神经网络对训练数据的记忆化引发了隐私与安全领域的迫切关注。近期研究表明，在特定条件下，训练集的片段可直接从模型参数中被重构。部分方法利用了模型对间隔最大化的隐式偏好，这表明通常被认为有利于泛化的特性实际上可能损害隐私。尽管已有引人注目的实证演示，但这些攻击的可靠性仍缺乏深入理解与坚实理论基础。本研究采取互补视角：不追求设计更强攻击手段，而是系统分析现有重构方法的内在缺陷与局限，并界定其失效条件。我们严格证明，在不引入数据先验知识的情况下，存在无限多个与真实训练集任意偏离的替代解，这使得重构从根本上不可靠。实证研究进一步表明，训练样本的精确复制仅具偶然性。我们的研究成果完善了关于训练集泄露可能性的理论认知，并为缓解重构攻击提供了新思路。值得注意的是，研究发现训练更充分的网络（即更满足隐式偏好条件的模型）实际上对重构攻击具有更强抵抗力，这在需要强泛化能力的场景中实现了隐私保护与模型性能的协同优化。 |
| 合成数据在多语言、多文化人工智能系统中的作用：来自印度诸语言的启示 | Pranjal A. Chitale | [PDF](http://arxiv.org/pdf/2509.21294v1) | Developing AI systems that operate effectively across languages while
remaining culturally grounded  [翻译失败] |
