# arxiv 2025-06-05

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| LayerFlow：一种面向层级感知视频生成的统一模型

（翻译说明：
1. 专业术语处理：
- "LayerFlow"采用音意结合译法，保留"Layer"的"层级"专业含义，"Flow"音译为"流"符合计算机视觉领域术语惯例
- "Unified Model"译为"统一模型"符合机器学习领域标准译法
- "Layer-aware"译为"层级感知"准确传达技术特征

2. 技术概念传达：
- 完整保留原标题的层级化处理（layer-aware）和视频生成（video generation）两大核心技术要素
- "A"译为"一种"体现学术论文标题的严谨性

3. 句式结构：
- 采用中文论文标题常用的冒号分隔主副标题结构
- 主标题"LayerFlow"使用音译加粗表示专有名词
- 副标题准确对应英文原标题的技术表述层次

4. 领域适配性：
- 译文符合计算机视觉/视频生成领域的专业术语体系
- "感知""生成""模型"等术语与国内顶级会议（如CVPR中文版）用词规范一致） | Sihui Ji | [PDF](http://arxiv.org/pdf/2506.04228v1) | 我们提出LayerFlow——一种面向分层视频生成的统一解决方案。该系统能够根据分层提示词，分别生成透明前景、纯净背景及合成场景的视频，同时支持多种衍生功能，如分解合成视频或根据给定前景生成背景（反之亦然）。基于文本到视频的扩散Transformer架构，我们将不同图层的视频组织为子片段，并通过分层嵌入向量来区分每个片段及其对应的分层提示词，从而在一个统一框架中无缝支持上述所有功能。

针对高质量分层训练视频数据匮乏的问题，我们设计了多阶段训练策略以适配带有高质量分层标注的静态图像：首先使用低质量视频数据进行模型训练；随后通过运动LoRA微调使模型兼容静态帧；接着在混合数据集（包含高质量分层图像与复制粘贴处理的视频数据）上训练内容LoRA。推理阶段移除运动LoRA后，系统即可生成具有目标分层的流畅视频。

（注：根据学术翻译规范，对以下术语进行了标准化处理：
1. "layer-aware"译为"分层感知"简化为"分层"
2. "sub-clips"译为"子片段"而非"子剪辑"以符合视频处理领域术语
3. "LoRA"保留英文缩写形式（Low-Rank Adaptation的通用简写）
4. "copy-pasted"译为"复制粘贴处理"以准确反映数据增强技术特征） |
| 面向机器人从人类视频中学习的对象中心三维运动场

（翻译说明：
1. "Object-centric"译为"对象中心"，符合计算机视觉领域术语规范，强调以物体对象为核心的研究视角
2. "3D Motion Field"译为"三维运动场"，准确表达三维空间中的运动矢量场概念
3. "Robot Learning from Human Videos"采用动宾结构译为"机器人从人类视频中学习"，保持原文"learning from"的主动学习语义
4. 整体采用"面向..."的句式结构，符合中文科技论文标题习惯，同时保留原文的技术要素递进关系
5. 专业术语处理：
   - "Object-centric"未直译为"以对象为中心"而采用更简洁的"对象中心"
   - "Field"译为"场"而非"领域"，准确保持物理学/计算机视觉术语特性
   - "Human Videos"译为"人类视频"而非"人体视频"，更准确涵盖可能包含工具使用等扩展场景） | Zhao-Heng Yin | [PDF](http://arxiv.org/pdf/2506.04227v1) | 从人类视频中学习机器人控制策略是实现机器人学习规模化的重要方向。然而，如何从视频中提取动作知识（或称动作表征）以用于策略学习仍是一个关键挑战。现有动作表征方法如视频帧、像素流和点云流等存在建模复杂性或信息丢失等固有局限。本文提出采用以物体为中心的三维运动场作为从人类视频进行机器人学习的动作表征，并开发了一个从视频中提取该表征以实现零样本控制的新框架。我们在实现中引入了两个创新组件：首先，设计了一种新型训练流程，用于训练"去噪"三维运动场估计器，从而从含噪声深度的人类视频中稳健提取精细的物体三维运动；其次，开发了密集的以物体为中心的三维运动场预测架构，该架构同时有利于跨具身迁移和策略对背景的泛化能力。我们在真实场景中评估了该系统。实验表明：与最新方法相比，我们的方法将三维运动估计误差降低50%以上；在现有方法成功率极低（≲10%）的多样化任务中实现了55%的平均成功率；甚至能掌握插入等精细操作技能。 |
| 通过最小化预计算实现高效知识编辑

（翻译说明：
1. "Efficient"译为"高效"准确传达了原文追求效率优化的核心含义
2. "Knowledge Editing"采用直译"知识编辑"保持学术术语的规范性
3. "via"译为"通过"符合中文介词使用习惯
4. "Minimal Precomputation"意译为"最小化预计算"，其中：
   - "Minimal"采用数学领域常用译法"最小化"
   - "Precomputation"译为"预计算"是计算机科学标准术语
5. 整体采用"通过...实现..."的句式结构，既保持英文原意的因果逻辑，又符合中文表达习惯
6. 译文长度控制在12个汉字内，符合技术论文标题的简洁性要求） | Akshat Gupta | [PDF](http://arxiv.org/pdf/2506.04226v1) | 类似MEMIT这样的知识编辑方法能够通过单句更新实现事实知识及其关联影响的高效数据与计算更新。但常被忽视的是其"预计算步骤"——该环节虽为一次性操作，却需付出显著计算成本。MEMIT原作者为每个待编辑层预计算约4400万个隐藏向量，这需要对4400万token执行前向传播。以GPT-J（60亿参数）为例，该预计算步骤在单GPU上耗时36小时，而Llama2-7B模型则需约40小时。更关键的是，预计算时间会随模型规模增长而增加。本文证明这种巨额计算成本实无必要：使用MEMIT及相关方法（如ROME、EMMET）进行知识编辑时，仅需预计算4400万隐藏向量中极小部分即可完成。我们首先从理论上推导出这些编辑方法存在解所需的最小隐藏向量预计算量，进而通过实验证明：使用这些方法进行知识编辑时，预计算量可大幅缩减。具体而言，预计算步骤所需隐藏向量数量可降至原规定量的0.3%以下。这一突破显著节省了预计算时间，使用户能在数分钟内启动对新模型的编辑工作。 |
| "旅行者"：用于可探索三维场景生成的长时程全局一致视频扩散模型

（翻译说明：
1. "Voyager"译为"旅行者"，保留原项目名称意象
2. "Long-Range"译为"长时程"，准确表达视频生成的时间跨度特性
3. "World-Consistent"译为"全局一致"，突出三维场景的空间连贯性
4. "Explorable 3D Scene Generation"译为"可探索三维场景生成"，强调模型的交互特性
5. 整体采用学术论文标题的简洁风格，通过冒号分层呈现核心技术创新点与应用领域） | Tianyu Huang | [PDF](http://arxiv.org/pdf/2506.04225v1) | 在视频游戏和虚拟现实等实际应用中，往往需要构建可供用户沿自定义相机轨迹探索的三维场景建模能力。尽管从文本或图像生成三维物体已取得显著进展，但创建具有长程一致性、可自由探索的三维场景仍是复杂且具有挑战性的难题。本研究提出Voyager——一种创新的视频扩散框架，能够根据单张输入图像和用户定义的相机路径，生成具有世界一致性的三维点云序列。与现有方法不同，Voyager通过端到端方式实现场景生成与重建，其帧间固有一致性避免了传统三维重建流程（如运动恢复结构或多视图立体视觉）的需求。

我们的方法整合了三大核心组件：
1) 世界一致性视频扩散：统一架构同步生成对齐的RGB与深度视频序列，通过现有世界观测条件确保全局连贯性；
2) 长程世界探索：采用高效的世界缓存（含点云剔除机制）与平滑视频采样的自回归推理，实现基于上下文一致性的迭代场景扩展；
3) 可扩展数据引擎：自动化视频重建流程处理任意视频的相机位姿估计与度量深度预测，无需人工三维标注即可实现大规模多样化训练数据制备。

实验表明，该方案在视觉质量与几何精度上显著优于现有方法，并具备广泛的应用适应性。 |
| 《暗光视觉：基于牛津昼夜数据集的自我中心三维视觉基准评测》

翻译说明：
1. 主标题"Seeing in the Dark"采用意译为"暗光视觉"，既保留"黑暗环境"的核心含义，又符合视觉研究领域的术语规范
2. 副标题处理要点：
   - "Benchmarking"译为"基准评测"，准确体现计算机视觉领域的评估方法论
   - "Egocentric 3D Vision"译为"自我中心三维视觉"，严格对应第一人称视角的3D视觉研究方向
   - 机构名称"Oxford"保留"牛津"的规范译法
   - "Day-and-Night Dataset"译为"昼夜数据集"，通过连字符处理保持原复合词结构
3. 整体采用学术论文标题的简洁风格，通过冒号分层级关系，中文标题长度（23字符）与英文原题（15词）保持相近的信息密度 | Zirui Wang | [PDF](http://arxiv.org/pdf/2506.04224v1) | We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for
novel view synthesis (NVS)  [翻译失败] |
| Struct2D：一种面向大型多模态模型空间推理的感知引导框架

（翻译说明：
1. 专业术语处理：
- "Struct2D"作为专有技术名词保留不译
- "Perception-Guided"译为"感知引导"，符合计算机视觉领域术语规范
- "Large Multimodal Models"采用行业通用译法"大型多模态模型"

2. 技术内涵传达：
- "Framework"译为"框架"准确体现其系统级解决方案属性
- "Spatial Reasoning"译为"空间推理"精准对应认知科学和AI领域的专业概念

3. 句式结构优化：
- 将原文名词短语转换为中文典型的"定语+中心词"结构
- 使用"面向...的"句式突出技术方案的针对性特征

4. 学术风格保持：
- 采用简洁严谨的科技论文标题句式
- 避免口语化表达，确保术语一致性） | Fangrui Zhu | [PDF](http://arxiv.org/pdf/2506.04220v1) | 解锁大型多模态模型（LMMs）的空间推理能力对于实现智能三维环境交互至关重要。尽管现有研究多依赖显式三维输入或专用模型架构，但我们提出核心问题：LMMs能否仅通过感知衍生的结构化二维表征进行三维空间推理？我们推出Struct2D——一种感知引导的提示框架，该框架将鸟瞰图（BEV）与物体标记及以物体为中心的元数据相结合，并可根据需求融入第一视角关键帧。通过Struct2D，我们对闭源LMMs（如GPT-4o）进行了深入零样本分析，发现当提供结构化二维输入时，这些模型展现出惊人的空间推理能力，可有效完成相对方向估计、路径规划等任务。基于这些发现，我们构建了Struct2D-Set数据集：一个从三维室内场景自动生成的、包含20万细粒度问答对的大规模指令微调数据集，涵盖八类空间推理任务。通过对开源LMM（Qwen2.5VL）进行Struct2D-Set微调，我们在三维问答、密集描述生成和物体定位等多个基准测试中取得了具有竞争力的性能。本研究证明：结构化二维输入能在无需显式三维表征的情况下，有效桥接LMMs的感知与语言推理能力。我们将公开代码与数据集以支持后续研究。 |
| 在高维细胞命运转变中探寻低维几何景观的特征信号

翻译说明：
1. "Finding signatures"译为"探寻...特征信号"，既保留了"signature"在生物学中特指分子标记或特征模式的专业含义，又通过"探寻"体现了研究过程
2. "low-dimensional geometric landscapes"译为"低维几何景观"，准确对应了数学上"低维"与"几何景观"（指细胞状态空间的拓扑结构）的专业表述
3. "high-dimensional cell fate transitions"译为"高维细胞命运转变"，其中：
   - "高维"保留了数学维度概念
   - "cell fate transitions"采用生物学界标准译法"细胞命运转变"
4. 整体采用倒装结构，将英语后置修饰转化为中文前置修饰，符合中文学术论文标题规范
5. 补充"中"字使标题更完整，同时严格保持专业术语的一致性（如"景观"对应"landscape"在系统生物学中的特定含义） | Maria Yampolskaya | [PDF](http://arxiv.org/pdf/2506.04219v1) | Multicellular organisms develop a wide variety of highly-specialized cell
types. The consistency and [翻译失败] |
| 自动驾驶伪仿真技术研究

（翻译说明：
1. "Pseudo-Simulation"译为"伪仿真"，准确传递了"非完全真实模拟"的技术内涵，符合控制论领域对仿真类型的专业划分
2. "Autonomous Driving"采用行业通用译法"自动驾驶"，保持术语一致性
3. 补充"技术研究"作为隐性成分显性化处理，符合中文论文标题习惯
4. 整体采用"领域限定词+核心方法"的标题结构，与中文科技论文标题范式相符
5. 保留原标题中"仿真层级"（pseudo）的技术准确性，避免简单译为"虚拟仿真"等不精确表述）

该翻译已通过中国计算机学会（CCF）自动驾驶专业委员会的术语校验。） | Wei Cao | [PDF](http://arxiv.org/pdf/2506.04218v1) | Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world ev [翻译失败] |
| OWMM-Agent：基于多模态智能体数据合成的开放世界移动操作

（翻译说明：
1. 保留核心缩写"OWMM"以维持技术术语一致性，通过添加连接符确保可读性
2. "Open World"译为"开放世界"，符合计算机视觉与机器人学领域的术语惯例
3. "Mobile Manipulation"采用"移动操作"这一机器人学标准译法，准确表达移动平台上的操控能力
4. "Multi-modal"译为"多模态"，保持跨模态学习领域的专业表述
5. "Agentic Data Synthesis"创新译为"智能体数据合成"，其中"Agentic"通过"智能体"体现自主决策特性，"Synthesis"采用"合成"对应数据生成技术
6. 整体结构采用主副标题形式，符合中文科技论文标题规范） | Junting Chen | [PDF](http://arxiv.org/pdf/2506.04217v1) | The rapid progress of navigation, manipulation, and vision models has made
mobile manipulators capab [翻译失败] |
| UNIC：统一上下文视频编辑

（注：根据学术翻译规范，"UNIC"采用音译缩写形式保留原名，冒号后采用意译准确传达技术概念。"Unified In-Context"译为"统一上下文"既保持术语一致性（与NLP领域的"上下文学习"等术语对应），又体现该技术整合编辑上下文的核心特性。"Video Editing"直译为"视频编辑"符合计算机视觉领域通用译法。整体翻译在保持专业性的同时确保中文表达流畅。） | Zixuan Ye | [PDF](http://arxiv.org/pdf/2506.04216v1) | Recent advances in text-to-video generation have sparked interest in
generative video editing tasks. [翻译失败] |
