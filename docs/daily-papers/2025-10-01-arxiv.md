# arxiv 2025-10-01

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 缝合：多模态扩散变换器中的免训练位置控制

说明：
1. "Stitch" 译为"缝合"，该译法在计算机图形学和图像处理领域常用于描述图像拼接技术，与原文在多媒体生成任务中的位置控制功能相契合。
2. "Training-Free" 采用专业术语"免训练"，准确表达无需额外训练的核心特性。
3. "Position Control" 译为"位置控制"，保持计算机视觉领域的专业表述。
4. 整体译文采用学术论文标题常用的名词短语结构，符合中文科技文献的标题规范。 | Jessica Bader | [PDF](http://arxiv.org/pdf/2509.26644v1) | 近年来，文生图（T2I）生成模型发展迅猛，但在准确捕捉"上方""右侧"等空间关系方面仍面临持续挑战。早期研究通过外部位置控制技术改善了空间关系遵循能力，但随着模型架构演进至提升图像质量的新阶段，这些传统方法已无法适配现代模型。我们提出Stitch——一种无需训练即可将外部位置控制融入多模态扩散变换器（MMDiT）的创新方法，该方法通过自动生成边界框实现技术突破。Stitch通过在指定边界框内生成独立对象并无缝拼接，既能确保空间关系精确性，又能保持视觉吸引力。研究发现，特定注意力头能在生成过程中捕获隔离与截取独立对象所需信息，无需完成整图生成。我们在自主构建的PosEval基准上评估Stitch，该基准包含五项延伸基础GenEval任务的新型定位任务，结果表明即使顶尖模型在基于位置的生成任务中仍有显著提升空间。在Qwen-Image、FLUX和SD3.5上的测试显示，Stitch能持续增强基础模型性能：在GenEval定位任务中使FLUX提升218%，在PosEval基准上提升206%。Stitch与Qwen-Image组合在PosEval上达到当前最优效果，相较前代模型提升54%，所有成果均在无需训练的前提下实现了对主流模型的定位控制集成。代码已发布于https://github.com/ExplainableML/Stitch。 |
| TTT3R：将三维重建作为测试时训练方法 | Xingyu Chen | [PDF](http://arxiv.org/pdf/2509.26645v1) | 现代循环神经网络因其线性时间复杂度，已成为三维重建领域具有竞争力的架构。然而，当处理超出训练上下文长度的数据时，其性能会显著下降，暴露出长度泛化能力有限的问题。本研究从测试时训练的角度重新审视三维重建基础模型，将其设计框架转化为在线学习问题。基于这一视角，我们利用记忆状态与输入观测数据之间的对齐置信度，推导出记忆更新的闭式学习率，以平衡历史信息保留与新观测适应的关系。这种无需重新训练的方法TTT3R显著提升了长度泛化能力，在全局姿态估计任务上较基线方法实现2倍性能提升，同时仅需6GB显存即可在20帧/秒的速率下处理数千张图像。代码详见https://rover-xingyu.github.io/TTT3R |
| 不同随机种子下语言模型的收敛性与发散性 | Finlay Fehlauer | [PDF](http://arxiv.org/pdf/2509.26643v1) | 本文研究了不同随机种子训练下的语言模型收敛性，通过计算各种子间每词元KL散度的期望值来衡量收敛程度。通过比较模型规模与训练检查点对收敛的影响，我们识别出四阶段收敛模式：（i）初始均匀阶段；（ii）急剧收敛阶段；（iii）急剧发散阶段；（iv）缓慢再收敛阶段。进一步发现，较大模型在训练后期再收敛速度更快，而较小模型始终无法实现再收敛，这表明学习稳定分布可能需要特定模型规模。针对特定词频和词性标注的细化分析表明，不同语言学范畴的收敛存在不均衡性：高频词与功能词比其对应类别（低频词与实义词）收敛更快更稳定。总体而言，我们的研究结果揭示了影响模型训练中分布稳定性的关键因素。 |
| 查询-上下文：一种用于图像生成与编辑的统一多模态模型 | Yuxin Song | [PDF](http://arxiv.org/pdf/2509.26641v1) | Unified Multimodal Models (UMMs) have demonstrated remarkable performance in
text-to-image generatio [翻译失败] |
| SPATA：面向精细化透明数据卡片的系统性模式分析框架

（解析：该翻译在保持专业性的同时实现了以下处理：
1. 保留首字母缩略词"SPATA"不译，符合学术惯例
2. 将"Systematic Pattern Analysis"译为"系统性模式分析"，准确传达方法论特征
3. "Detailed and Transparent Data Cards"译为"精细化透明数据卡片"，其中：
   - "Detailed"译为"精细化"而非字面的"详细"，更符合技术文档表述
   - 使用连接符保持"透明"与"精细化"的并列关系
4. 补充"框架"二字，使技术体系的概念更完整，符合中文技术文献表述习惯
5. 整体采用"面向...的..."句式，体现工具的目标导向性） | João Vitorino | [PDF](http://arxiv.org/pdf/2509.26640v1) | 由于人工智能（AI）易受数据扰动和对抗样本的影响，在部署任何机器学习（ML）模型前进行全面的鲁棒性评估至关重要。然而，检查模型的决策边界并识别潜在漏洞通常需要访问训练和测试数据集，这可能对数据隐私与保密性构成风险。为提升涉及机密数据或关键基础设施管理机构的透明度，必须允许在不公开私有数据集的前提下对AI系统进行外部验证。本文提出系统性模式分析（SPATA）——一种将任意表格数据集转换为统计模式的领域无关表征的确定性方法，以提供更详尽透明的数据卡片。SPATA将每个数据实例投影至离散空间，在避免数据泄露风险的前提下实现数据实例的分析与比对。这些投影数据集可可靠用于评估不同特征对ML模型鲁棒性的影响，并生成可解释的行为说明，从而助力构建更可信的AI系统。 |
| 在城市尺度下对以自我为中心的视觉惯性SLAM进行基准测试 | Anusha Krishnan | [PDF](http://arxiv.org/pdf/2509.26639v1) | 通过内置传感器实现精确的六自由度同步定位与建图（SLAM）技术，对于采集以自我为中心数据的可穿戴设备至关重要。这类设备面临特殊挑战，包括更复杂的运动模式与观察视角、普遍存在的动态视觉内容，以及因时变传感器校准而受影响的长时间作业场景。尽管SLAM技术近期发展迅速，但学术研究仍受限于未能反映这些挑战或缺乏高精度真实位姿标注的基准数据集。本文提出一个包含自我中心多模态数据的视觉-惯性SLAM新型数据集与基准测试框架。我们通过配备多种传感器的类眼镜设备，在市中心区域记录了长达数小时、覆盖数公里的运动轨迹。通过运用测绘工具获取控制点作为间接位姿标注，实现了公制尺度、厘米级精度且适用于城市规模的精确定位。这使得评估极端运动轨迹（如夜间步行或车载行进）成为可能。实验表明，学界开发的先进系统尚无法稳健应对这些挑战，我们进一步明确了导致该现象的核心组件。此外，我们设计了具有不同难度等级的测试赛道，以促进对尚未成熟方法的深度分析与评估。数据集与基准测试平台详见https://www.lamaria.ethz.ch。 |
| 事故基准：车辆事故及更广泛场景中多模态理解与推理能力评测体系 | Shangding Gu | [PDF](http://arxiv.org/pdf/2509.26636v1) | 随着多模态模型的快速发展，亟需能够严格评估安全关键型动态现实场景中理解与推理能力的基准测试。我们推出AccidentBench——一个融合交通事故场景与跨域安全关键场景的大规模基准，涵盖航空与水域中强调时空推理的场景（如导航、方位判定、多载具运动）。该基准包含约2000段视频及超过19000组人工标注的问答对，覆盖短/中/长三种视频时长与易/中/难三个难度层级。测试任务系统化考察核心能力：时序理解推理、空间理解推理及意图理解推理。通过整合以事故为中心的交通场景与更广泛的空域水域安全关键场景，AccidentBench构建出全面且基于物理现实的可变环境测试平台。对前沿模型（如Gemini-2.5 Pro与GPT-5）的评估表明，即使在最强模型上，最困难任务与最长视频的准确率仅约18%，揭示出现实场景中时空与意图推理能力的显著缺陷。AccidentBench旨在暴露这些关键不足，推动构建更安全、更鲁棒、更契合现实安全挑战的多模态模型。代码与数据集详见：https://github.com/SafeRL-Lab/AccidentBench |
| 通过音节语音标记化扩展口语语言模型的规模 | Nicholas Lee | [PDF](http://arxiv.org/pdf/2509.26634v1) | 口语模型通常将从自监督学习语音模型中提取的高帧率标记作为离散化语音单元。由于最成功的语言模型基于Transformer架构，使用自注意力机制处理这些长标记序列的计算成本极高——注意力机制的计算复杂度随序列长度呈平方级增长。近期一项自监督学习研究引入了音节层级的语音标记化方法，该方法不仅更具可解释性，还能通过显著压缩标记长度（4-5赫兹）实现更好的可扩展性。然而，这种标记化方法在口语语言建模中的价值尚未得到充分探索。我们首次系统研究了音节标记化在口语语言建模中的应用，通过在不同规模的训练数据上评估模型在一系列口语理解基准测试中的表现。研究结果表明：音节标记在显著降低训练与推理成本（训练时间缩短逾2倍，浮点运算量减少5倍）的同时，能够达到甚至超越传统高帧率标记的性能。我们的发现证实，音节级语言建模是实现高效长上下文口语语言模型的一条重要技术路径。 |
| OmniRetarget：面向人形机器人全身移动操作与场景交互的交互保持数据生成方法

（注：该翻译在保持专业术语准确性的基础上，采用中文常见的学术表达方式：
1. "Humanoid Whole-Body Loco-Manipulation"译为"人形机器人全身移动操作"，准确体现机器人领域术语
2. "Scene Interaction"译为"场景交互"，符合人机交互领域表述规范
3. "Interaction-Preserving"译为"交互保持"，精准传达保持交互特性的核心概念
4. 通过添加"方法"二字，符合中文论文标题命名习惯，同时保持原标题的技术内涵） | Lujie Yang | [PDF](http://arxiv.org/pdf/2509.26633v1) | 当前人形机器人复杂技能教学的主流范式，是将人体运动重定向为运动学参考以训练强化学习策略。然而，现有重定向流程常因人体与机器人间显著的形态差异而难以处理，导致足部滑动、穿模等物理失真现象。更重要的是，常规重定向方法忽略了对于表现性移动与移动操作至关重要的丰富人-物、人-环境交互关系。为此，我们提出OmniRetarget——基于交互网格的交互保持型数据生成引擎，通过显式建模并保持智能体、地形与操作对象之间的关键空间关系及接触状态。该方法在满足运动学约束的同时最小化人体与机器人网格间的拉普拉斯形变，从而生成运动学可行的轨迹。此外，通过保持任务相关交互，实现了从单次演示到不同机器人形态、地形和物体配置的高效数据增强。我们通过重定向OMOMO、LAFAN1及自研动捕数据集中的动作对OmniRetarget进行全面评估，生成长达8小时以上的轨迹数据，在运动学约束满足度与接触保持方面均优于现有基线方法。此类高质量数据使本体感知强化学习策略仅需5个奖励项及跨任务通用的简单域随机化，无需任何课程学习，即可在Unitree G1人形机器人上成功执行长达30秒的跑酷与移动操作技能。 |
| 拓展边界：运用测量树拓宽人工智能的测量与评估维度

（注：译文采用学术性意译策略，既保留核心概念"Measurement Trees"的直译"测量树"，又通过"拓展边界"和"拓宽...维度"的表述，准确传达原文中"Branching Out"与"Broadening"的动态延伸含义，同时符合中文科技论文标题的表述规范。） | Craig Greenberg | [PDF](http://arxiv.org/pdf/2509.26632v1) | 本文提出了一种新型度量标准——\textit{测量树}，这类创新指标旨在将不同构念整合为可测量对象的多层级可解释表征。与传统生成单一数值、向量、曲面或分类结果的度量方式不同，测量树生成的是层次化有向图结构，其中每个节点通过用户自定义的聚合方法对其子节点进行汇总。为响应近期扩大人工智能系统评估范围的倡议，测量树通过以下方式增强度量透明度：支持整合异质性证据，包括但不限于智能体行为、商业指标、能效数据、社会技术信号或安全指标等。我们给出了具体定义与实例，通过大规模测量实验验证其实用价值，并配套开源Python代码库。通过实现复杂构念的透明化测量方法，本研究为构建更广泛、更具可解释性的人工智能评估体系奠定了理论基础。 |
