# arxiv 2025-09-26

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| SciReasoner：奠定跨学科科学推理基础 | Yizhou Wang | [PDF](http://arxiv.org/pdf/2509.21320v1) | 我们提出一种科学推理基础模型，该模型实现了自然语言与异构科学表征的对齐。该模型基于包含科学文本、纯序列及序列-文本对的206B词元语料库进行预训练，随后通过40M指令的监督微调实现对齐，采用退火冷启动自举法激发长链思维推理，并结合任务特定奖励塑形的强化学习机制，从而形成严谨的科学推理能力。模型支持五大能力体系，覆盖工作流中多达103项任务：(i)文本与科学格式的精准转换，(ii)文本/知识抽取，(iii)属性预测，(iv)属性分类，(v)无条件与条件式序列生成与设计。相较于专业系统，本方法显著扩展了指令覆盖范围，提升了跨领域泛化能力，并增强了输出保真度。我们详细阐述了数据策展与训练流程，论证跨学科学习可强化知识迁移与下游任务可靠性。该模型、指令微调数据集及评估代码已开源发布于https://huggingface.co/SciReason 与 https://github.com/open-sciencelab/SciReason。 |
| RLBFF：二进制灵活反馈——连接人类反馈与可验证奖励的桥梁 | Zhilin Wang | [PDF](http://arxiv.org/pdf/2509.21319v1) | Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards [翻译失败] |
| SD3.5-Flash：基于分布引导的生成流蒸馏方法

（解析：该翻译遵循了学术术语的准确性原则：
1. 保留核心模型名称"SD3.5-Flash"不翻译
2. "Distribution-Guided"译为"基于分布引导"，符合中文论文标题常用表达
3. "Distillation"译为专业术语"蒸馏"，特指模型压缩技术
4. "Generative Flows"译为"生成流"，准确对应生成式模型领域的专业概念
5. 整体采用"方法"作为后缀，符合中文论文标题命名规范） | Hmrishav Bandyopadhyay | [PDF](http://arxiv.org/pdf/2509.21318v1) | 我们推出SD3.5-Flash——一种高效的少步数蒸馏框架，将高质量图像生成能力引入普及型消费设备。该框架通过专为少步生成设计的重构分布匹配目标，对计算成本高昂的修正流模型进行蒸馏。我们引入两项关键创新："时间步共享"机制降低梯度噪声，"分时段微调"技术提升提示词对齐效果。结合文本编码器重构与专用量化等全流程优化，本系统可在不同硬件配置上实现高速生成与内存高效部署，使从手机到台式机的全谱系设备均能获得先进生成能力。通过包含大规模用户研究在内的综合评估，我们证明SD3.5-Flash持续超越现有少步生成方法，真正实现先进生成式AI技术的普惠化部署。 |
| 基于主动用户指令的交互式推荐代理 | Jiakai Tang | [PDF](http://arxiv.org/pdf/2509.21317v1) | 传统推荐系统依赖被动反馈机制，仅允许用户进行"喜欢/不喜欢"等简单选择。然而这类粗粒度信号无法捕捉用户细粒度的行为动机与意图，导致现有系统难以区分具体哪些物品属性驱动着用户满意或不满，进而造成偏好建模失准。这些根本性局限在用户意图与系统解读之间形成持续性鸿沟，最终既损害用户体验又削弱系统效能。

为突破这些局限，我们提出交互式推荐信息流这一创新范式，首次在主流推荐流中实现自然语言指令交互。与传统系统将用户局限于被动隐式行为影响不同，IRF通过实时语言指令赋予用户对推荐策略的主动显式控制权。为支撑该范式，我们开发了RecBot双智能体架构：解析智能体将语言表达转化为结构化偏好，规划智能体则动态编排自适应工具链以实现实时策略调整。为实现实际部署，我们采用仿真增强的知识蒸馏技术，在保持强大推理能力的同时获得高效性能。经大规模离线实验与长期在线测试，RecBot在用户满意度和商业指标上均展现出显著提升。 |
| SAGE：语义理解领域的现实基准测试平台 | Samarth Goel | [PDF](http://arxiv.org/pdf/2509.21310v1) | As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an ur [翻译失败] |
| 牛顿生成器：基于神经牛顿动力学的物理一致且可控的文本到视频生成技术

（注：译文采用学术翻译的"名从主人"原则，将"NewtonGen"意译为"牛顿生成器"以体现其生成功能；通过"物理一致"对应"Physics-Consistent"准确传达物理规律一致性要求；"可控"直译"Controllable"保持术语准确性；"神经牛顿动力学"采用专业复合词翻译法，既保留牛顿力学概念又体现神经网络特性，整体符合中文科技文献标题的简洁规范。） | Yu Yuan | [PDF](http://arxiv.org/pdf/2509.21309v1) | 当前大规模文本到视频生成的主要瓶颈在于物理一致性与可控性。尽管近期取得进展，最先进的模型仍常产生违反物理规律的运动，例如物体向上坠落、速度与方向的突变等。更为关键的是，现有模型缺乏精确的参数控制能力，难以在不同初始条件下生成符合物理规律的动态效果。我们认为这一根本性局限源于现有模型仅从外观特征学习运动分布，而未能理解内在动力学原理。本研究提出NewtonGen框架，将数据驱动合成与可学习的物理原理相融合。其核心是可训练的神经牛顿动力学模块，能够建模并预测多种牛顿力学运动，从而向视频生成过程注入隐式动力学约束。通过协同利用数据先验与动力学指导，NewtonGen实现了具有精确参数控制的物理一致性视频合成。 |
| 奉承并非单一行为：大语言模型中奉承性行为的因果分离 | Daniel Vennemeyer | [PDF](http://arxiv.org/pdf/2509.21305v1) | 大型语言模型（LLMs）常表现出谄媚行为——例如过度迎合或恭维用户——但尚不清楚这些行为源于单一机制还是多个不同过程。我们将谄媚行为分解为迎合式赞同与奉承式赞美，并将其与真诚赞同进行对比。通过在不同模型和数据集上应用均值差异方向、激活叠加及子空间几何分析方法，我们发现：（1）三种行为在潜在空间中沿不同线性方向编码；（2）每种行为均可被独立增强或抑制而不影响其他行为；（3）其表征结构在不同模型家族和规模中保持一致性。这些结果表明，谄媚行为对应着相互独立且可分别调控的表征机制。 |
| 量化视觉几何基础Transformer | Weilun Feng | [PDF](http://arxiv.org/pdf/2509.21302v1) | Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs [翻译失败] |
| 无先验，无泄露：重探训练神经网络中的重构攻击

（注：该翻译采用学术论文标题的简洁对仗结构，通过"无...无..."的排比句式突出核心论点。"Revisiting"译为"重探"体现学术研究的批判性视角，比"重新审视"更具学术张力。"Reconstruction Attacks"采用计算机安全领域标准译法"重构攻击"，与"训练神经网络"形成专业术语搭配。） | Yehonatan Refael | [PDF](http://arxiv.org/pdf/2509.21296v1) | 神经网络对训练数据的记忆化引发了隐私与安全的迫切担忧。近期研究表明，在特定条件下，训练集的某些部分可直接从模型参数中被重构。部分方法利用了模型对间隔最大化的隐式偏好，这表明通常被认为有利于泛化的特性实际上可能危及隐私。然而尽管存在令人瞩目的实证演示，这些攻击的可靠性仍缺乏深入理解与坚实理论基础。本文采取互补视角：不追求设计更强攻击手段，而是系统分析现有重构方法的内在缺陷与局限，并界定其失效的条件。我们严格证明，在不引入数据先验知识的情况下，存在无限多个与真实训练集任意偏离的替代解，这使得重构从根本上具有不可靠性。实证研究进一步表明，训练样本的精确复制仅具偶然性。我们的研究结果完善了关于训练集泄露可能性的理论认知，并为缓解重构攻击提供了新思路。值得注意的是，我们发现训练更充分的网络（即更满足隐式偏好条件的模型）实际上对重构攻击具有更强抵抗力，这在需要强泛化能力的场景中实现了隐私保护与模型性能的协同优化。 |
| 合成数据在多语言、多文化人工智能系统中的作用：来自印度诸语言的启示 | Pranjal A. Chitale | [PDF](http://arxiv.org/pdf/2509.21294v1) | Developing AI systems that operate effectively across languages while
remaining culturally grounded  [翻译失败] |
