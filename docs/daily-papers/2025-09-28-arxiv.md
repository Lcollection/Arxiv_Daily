# arxiv 2025-09-28

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| SciReasoner：奠定跨学科科学推理基础 | Yizhou Wang | [PDF](http://arxiv.org/pdf/2509.21320v1) | 我们提出一种科学推理基础模型，该模型将自然语言与异构科学表征进行对齐。该模型基于涵盖科学文本、纯序列及序列-文本对的2060亿词元语料进行预训练，随后通过4000万条指令的指令微调实现对齐，采用退火冷启动自举法激发长链思维，并结合任务特定奖励塑形的强化学习机制，从而形成严谨的科学推理能力。该模型支持五大能力体系，覆盖工作流中103项任务：(i)文本与科学格式的精准转换，(ii)文本/知识提取，(iii)属性预测，(iv)属性分类，(v)无条件与条件序列生成与设计。相较于专业系统，我们的方法扩展了指令覆盖范围，提升了跨领域泛化能力，并增强了输出保真度。我们详细阐述了数据策展与训练过程，证明跨学科学习能强化知识迁移与下游任务可靠性。该模型、指令微调数据集及评估代码已开源发布于https://huggingface.co/SciReason 与 https://github.com/open-sciencelab/SciReason。 |
| RLBFF：二元灵活反馈机制——连接人类反馈与可验证奖励的桥梁 | Zhilin Wang | [PDF](http://arxiv.org/pdf/2509.21319v1) | 基于人类反馈的强化学习（RLHF）与基于可验证奖励的强化学习（RLVR）是大语言模型后训练阶段采用的两大主流强化学习范式，各具独特优势。然而，RLHF因依赖通常缺乏明确标准的人类判断，存在可解释性不足与奖励破解问题；RLVR则受限于其基于正确性验证器的特性，适用范围较窄。我们提出基于二元灵活反馈的强化学习（RLBFF），该方法融合了人类驱动偏好的灵活性与基于规则验证的精确性，使奖励模型能够捕捉超越单纯正确性的回答质量细微特征。

RLBFF从自然语言反馈中提取可进行二元判定的原则（例如信息准确性：是/否，代码可读性：是/否）。这些原则随后可作为蕴含任务（回答满足/不满足任意原则）用于支撑奖励模型训练。实验表明，在数据量相同的情况下，以此方式训练的奖励模型性能超越Bradley-Terry模型，并在RM-Bench（86.2%）和JudgeBench（81.4%，截至2025年9月24日居排行榜首位）上取得最优表现。此外，与Bradley-Terry模型相比，用户可在推理阶段指定关注原则以定制奖励模型的侧重点。

最后，我们开源了完整实施方案（含训练数据），通过RLBFF与我们的奖励模型对Qwen3-32B进行对齐训练，在MT-Bench、WildBench和Arena Hard v2通用对齐基准测试中达到或超越o3-mini与DeepSeek R1的性能水平（推理成本降低95%以上）。 |
| SD3.5-Flash：基于分布引导的生成流蒸馏方法

（解析说明：
1. 保留核心模型名称"SD3.5-Flash"不译，维持技术术语准确性
2. "Distribution-Guided"译为"基于分布引导"，体现方法的核心指导机制
3. "Distillation"译为"蒸馏"，符合机器学习领域术语规范
4. "Generative Flows"译为"生成流"，准确表达生成模型中的流模型概念
5. 整体采用"方法"作为隐含后缀，符合中文论文标题命名习惯） | Hmrishav Bandyopadhyay | [PDF](http://arxiv.org/pdf/2509.21318v1) | 我们推出SD3.5-Flash——一种高效的少步蒸馏框架，将高质量图像生成能力引入普及型消费设备。该方案通过专为少步生成重构的分布匹配目标，对计算成本高昂的修正流模型进行蒸馏。我们引入两项关键创新："时间步共享"机制降低梯度噪声，"分时步微调"技术提升提示对齐精度。结合文本编码器重构与专用量化等全流程优化，本系统可在不同硬件配置上实现高速生成与内存高效部署，使从手机到台式机的全系列设备均能获得先进生成能力。通过包含大规模用户研究在内的综合评估，我们证明SD3.5-Flash持续超越现有少步生成方法，真正实现先进生成式AI技术的普惠化部署。 |
| 基于主动用户指令的交互式推荐代理 | Jiakai Tang | [PDF](http://arxiv.org/pdf/2509.21317v1) | 传统推荐系统依赖被动反馈机制，仅允许用户进行"喜欢/不喜欢"等简单选择。然而这种粗粒度信号无法捕捉用户细腻的行为动机与意图，导致现有系统难以区分具体哪些物品属性驱动用户产生满意或不满情绪，从而造成偏好建模失准。这些根本性局限在用户意图与系统解读之间形成持续性鸿沟，最终既削弱用户满意度又损害系统效能。

为突破这些限制，我们提出交互式推荐信息流（IRF）这一创新范式，首次在主流推荐流中实现自然语言指令交互。与传统系统将用户局限于被动隐式行为影响不同，IRF通过实时语言指令赋予用户对推荐策略的主动显式控制权。为支撑该范式，我们开发了RecBot双智能体架构：解析智能体将语言表达转化为结构化偏好，规划智能体则动态编排自适应工具链实现实时策略调整。为实现实际部署，我们采用仿真增强的知识蒸馏技术，在保持强大推理能力的同时获得高效性能。经过大规模离线实验与长期在线测试，RecBot在用户满意度与商业指标上均展现出显著提升。 |
| SAGE：语义理解现实基准测试 | Samarth Goel | [PDF](http://arxiv.org/pdf/2509.21310v1) | 随着大语言模型在传统基准测试中展现出强劲性能，我们亟需建立更具挑战性的评估框架，以深入探究语义理解的深层维度。本文提出SAGE（语义对齐与泛化评估）基准，该严谨的评估体系通过五大维度对嵌入模型与相似性度量进行综合评估：人类偏好对齐、变换鲁棒性、信息敏感度、聚类性能及检索鲁棒性。与现有仅关注单一能力的基准不同，SAGE通过对抗条件、噪声变换及涵盖30余个数据集的精细化人工判定任务来评估语义理解能力。

我们对9种嵌入模型与经典度量方法的全面评估揭示了显著的性能差距：没有任何方法能在所有维度表现卓越。例如，虽然OpenAI的text-embedding-3-large等前沿嵌入模型在人类偏好对齐方面占据优势（0.682对比最佳经典度量的0.591），但在信息敏感度任务中却被经典度量方法显著超越——杰卡德相似度取得0.905的评分，而最佳嵌入模型仅获0.794。SAGE进一步揭示了关键权衡关系：OpenAI的text-embedding-3-small实现了最高聚类性能（0.483），却表现出极端脆弱性，其鲁棒性评分最低（0.011）。该基准不仅暴露了当前语义理解能力的根本局限，更为实际应用场景中的模型鲁棒性提供了更贴近现实的评估标准。 |
| 牛顿生成器：基于神经牛顿动力学的物理一致且可控的文本到视频生成

（注：翻译采用"牛顿生成器"作为NewtonGen的意译，既保留原名称中的牛顿概念，又体现其生成功能；"Physics-Consistent"译为"物理一致"准确传达物理规律一致性要求；"Controllable"采用通用译法"可控"；"Neural Newtonian Dynamics"译为"神经牛顿动力学"既保持专业术语准确性，又体现神经网络与经典力学的结合特性） | Yu Yuan | [PDF](http://arxiv.org/pdf/2509.21309v1) | 当前大规模文本到视频生成的主要瓶颈在于物理一致性与可控性。尽管近期取得进展，现有最优模型仍常产生非真实运动，例如物体向上坠落、速度与方向的突变等。更为关键的是，这些模型缺乏精确的参数控制，难以在不同初始条件下生成物理一致的动态效果。我们认为这一根本性局限源于现有模型仅从外观特征学习运动分布，而未能理解底层动力学原理。本研究提出NewtonGen框架，将数据驱动合成与可学习的物理原理相融合。其核心是可训练的神经牛顿动力学模块（NND），能够建模并预测多种牛顿力学运动，从而向视频生成过程注入隐式动力学约束。通过协同利用数据先验与动力学指导，NewtonGen实现了具有精确参数控制的物理一致性视频合成。 |
| 阿谀奉承并非单一行为：大语言模型中谄媚行为的因果分离

（注：该翻译在保持学术严谨性的基础上，采用"阿谀奉承"对应"sycophancy"的文学化表达，同时通过"谄媚行为"实现术语统一。"Causal Separation"译为"因果分离"符合计算社会科学方法论表述，副标题结构完整呈现原文的研究方法论特征。） | Daniel Vennemeyer | [PDF](http://arxiv.org/pdf/2509.21305v1) | 大型语言模型（LLMs）常表现出谄媚行为——例如过度附庸用户或对用户进行奉承——但尚不清楚这些行为源于单一机制还是多个不同过程。我们将谄媚行为解构为“曲意附和”与“阿谀奉承”，并将其与“真诚认同”进行对比研究。通过在多模型和多数据集上运用均值差异方向、激活叠加及子空间几何分析方法，我们发现：（1）三种行为在潜在空间中沿不同线性方向编码；（2）每种行为均可被独立增强或抑制而不影响其他行为；（3）其表征结构在不同模型家族与规模中保持一致性。这些结果表明，谄媚行为对应着相互独立且可分别调控的表征模式。 |
| 量化视觉几何基础Transformer | Weilun Feng | [PDF](http://arxiv.org/pdf/2509.21302v1) | 基于学习的3D重建模型（以视觉几何基干变换器VGGT为代表）借助大规模变换器取得了显著进展，但其高昂的计算与内存成本严重阻碍了实际部署。训练后量化已成为模型压缩与加速的通用技术，然而我们通过实验发现：在压缩十亿级参数的VGGT时，PTQ面临独特挑战——数据无关的特殊令牌导致激活分布呈现重尾特性，而3D数据的多视角特性使得校准样本选择极不稳定。本文首次提出面向VGGT的量化框架QuantVGGT，其核心技术贡献包括：首先提出双平滑细粒度量化，通过融合全局哈达玛变换与局部通道平滑，有效抑制重尾分布并鲁棒降低通道间方差；其次设计噪声过滤多样性采样，借助深层统计量过滤异常值，并构建帧感知的多样化校准簇以确保量化范围稳定性。综合实验表明，QuantVGGT在不同基准测试和比特位宽下均达到最先进水平，大幅超越现有通用量化方法。值得强调的是，4比特QuantVGGT在真实硬件推理中可实现3.7倍内存压缩与2.5倍加速，同时保持重建精度达到全精度模型的98%以上，充分展现了该方法在资源受限场景下的巨大优势与实用性。代码已发布于https://github.com/wlfeng0509/QuantVGGT。 |
| 无先验，无泄露：重探训练神经网络中的重构攻击

（注：该翻译采用学术论文标题常见的对仗结构，通过"无...无..."的排比句式强化核心概念。"Revisiting"译为"重探"既体现学术研究的延续性，又暗含批判性审视的意味。"Reconstruction Attacks"采用计算机安全领域标准译法"重构攻击"，与差分攻击、成员推断攻击等术语保持体系一致性。） | Yehonatan Refael | [PDF](http://arxiv.org/pdf/2509.21296v1) | 神经网络对训练数据的记忆引发了对隐私与安全的迫切担忧。近期研究表明，在特定条件下，训练集中的部分数据可直接从模型参数中被重构。某些方法利用了模型对间隔最大化的隐式偏好，这表明通常被认为有利于泛化的特性实际上可能损害隐私。尽管已有引人注目的实证演示，但这些攻击的可靠性仍缺乏深入理解，且欠缺坚实的理论基础。本研究采取互补视角：不致力于设计更强攻击手段，而是系统分析现有重构方法的内在缺陷与局限，并界定其失效的条件。我们严格证明，在不引入数据先验知识的情况下，存在无限多个与真实训练集可能任意偏离的替代解，这使得重构从根本上不可靠。实证研究进一步表明，训练样本的精确复制仅具偶然性。我们的研究结果完善了关于训练集泄露可能性的理论认知，并为缓解重构攻击提供了新见解。值得注意的是，研究发现训练更充分的网络——即更严格满足隐式偏好条件的模型——实际上更不易受重构攻击，这在需要强泛化能力的场景中实现了隐私保护与模型性能的协调统一。 |
| 合成数据在多语言、多文化人工智能系统中的作用：来自印度诸语言的启示

（注：Indic Languages在此译为"印度诸语言"，指印度次大陆使用的印欧语系语言群，包括印地语、孟加拉语、马拉地语等22种官方语言及数百种方言。该译法既体现语言多样性，又符合我国对印度语言体系的学术表述惯例。） | Pranjal A. Chitale | [PDF](http://arxiv.org/pdf/2509.21294v1) | 开发能够在不同语言间有效运作且保持文化根基的人工智能系统，是一个长期存在的挑战，在资源匮乏环境中尤为突出。合成数据虽提供了前景广阔的解决路径，但其在多语言与跨文化场景中的有效性仍有待深入探索。我们通过自下而上的生成策略，引导大型开源语言模型（参数量≥235B）基于特定语言的维基百科内容进行数据生成，系统研究了面向印度语言的合成文化情境化数据集的创建方法与影响。这一策略对当前主流的将英语等高资源语言合成数据集进行翻译的"自上而下"范式形成了重要补充。

我们正式发布Updesh数据集——一个高质量的大规模合成指令遵循数据集，涵盖13种印度语言共计950万条数据，包含多样化的推理与生成任务，特别注重长上下文处理、多轮对话能力以及与印度文化背景的契合度。通过结合自动化指标与人工标注的综合性评估（涵盖1万条数据评测），结果表明生成数据质量较高，但人工评估也揭示了需要进一步优化的领域。此外，我们通过在该数据集上微调模型并在15个多语言数据集上进行下游任务评估，发现基于Updesh训练的模型在生成任务上持续取得显著提升，在多项选择式自然语言理解任务中保持竞争力。值得注意的是，相对性能提升在低资源与中等资源语言中最为显著，有效缩小了其与高资源语言之间的性能差距。

这些发现为以下观点提供了实证依据：有效的多语言人工智能需要采用融合情境感知与文化根基方法论的多维度数据策展与生成策略。 |
