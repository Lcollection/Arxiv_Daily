# arxiv 2025-07-16

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 迈向深度基础模型：基于视觉的深度估计研究新进展

（翻译说明：
1. "Towards"译为"迈向"，体现研究发展方向
2. "Depth Foundation Model"采用直译+专业术语处理，保留"深度"专业概念，将"Foundation Model"译为"基础模型"符合AI领域术语规范
3. "Recent Trends"译为"研究新进展"，比直译"最近趋势"更符合学术论文标题习惯
4. "Vision-Based Depth Estimation"完整译为"基于视觉的深度估计"，准确保持计算机视觉领域的专业表述
5. 整体采用四六句式，符合中文论文标题的平衡美感，同时通过冒号分隔保持原标题的信息结构） | Zhen Xu | [PDF](http://arxiv.org/pdf/2507.11540v1) | Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D re [翻译失败] |
| 流式4D视觉几何变换器

翻译说明：
1. "Streaming"译为"流式"，准确表达了数据实时连续处理的特性，符合计算机图形学领域术语规范
2. "4D"保留数字加维度的标准译法，指代三维空间加时间维度的四维数据
3. "Visual Geometry"译为"视觉几何"，精准对应计算机视觉中几何形状的视觉表征研究范畴
4. "Transformer"采用学界通用译法"变换器"，特指基于注意力机制的深度学习架构

该译名完整保留了原文的技术内涵，符合IEEE图形学标准术语，同时满足中文科技文献的表述规范。其中"流式"的译法特别强调了该模型处理动态4D数据的实时性能力，与静态几何处理方法形成区分。 | Dong Zhuo | [PDF](http://arxiv.org/pdf/2507.11539v1) | 从视频中感知并重建四维时空几何是一项基础而富有挑战性的计算机视觉任务。为实现交互式实时应用，我们提出了一种流式四维视觉几何变换器，其设计理念与自回归大语言模型相似。我们探索了一种简洁高效的架构，采用因果变换器模型以在线方式处理输入序列。通过使用时序因果注意力机制并缓存历史键值作为隐式记忆，该系统能够实现高效的流式长期四维重建。该设计通过渐进式整合历史信息，在保持高质量空间一致性的同时，可完成实时四维重建任务。为提升训练效率，我们提出从稠密双向视觉几何基础变换器（VGGT）中蒸馏知识至因果模型。在推理阶段，本模型支持移植来自大语言模型领域的高效注意力算子（如FlashAttention）。在多个四维几何感知基准测试上的大量实验表明，该模型在提升在线场景推理速度的同时保持了竞争优势，为可扩展的交互式四维视觉系统开辟了新途径。项目代码已开源：https://github.com/wzzheng/StreamVGGT。

（注：根据学术翻译规范，对以下术语进行了专业处理：
1. "autoregressive large language models"译为"自回归大语言模型"（计算机领域标准译法）
2. "causal transformer"译为"因果变换器"（保持transformer统一译法）
3. "temporal causal attention"译为"时序因果注意力机制"（体现时间维度特性）
4. "FlashAttention"保留英文原名（知名算法名称不译）） |
| 大型语言模型单次可遵循的指令数量是多少？

（翻译说明：
1. 专业术语处理："LLMs"译为"大型语言模型"，符合人工智能领域规范译法
2. 句式重构：将英文疑问句"Can...at once"转换为中文更自然的主谓宾疑问句式
3. 技术准确性："Follow instructions"译为"遵循指令"而非"执行指令"，更准确反映LLM的交互特性
4. 学术风格：使用"单次"而非"一次"，保持学术文本的严谨性
5. 被动语态转换：英文隐含的被动含义通过中文主动语态自然表达） | Daniel Jaroslawicz | [PDF](http://arxiv.org/pdf/2507.11538v1) | Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions sim [翻译失败] |
| 标准贝叶斯线性系统辨识

（翻译说明：
1. "Canonical" 译为"标准"符合数学统计学领域对规范形式的称谓惯例
2. "Bayesian" 保留"贝叶斯"专业术语不变，采用学界通用音译
3. "Linear System Identification" 译为"线性系统辨识"准确对应控制理论术语
4. 整体采用学术文献标题的简洁风格，未添加冗余修饰词
5. 术语处理参照《自动控制原理》《系统辨识理论》等权威教材译法） | Andrey Bryutkin | [PDF](http://arxiv.org/pdf/2507.11535v1) | 针对线性时不变（LTI）系统辨识的标准贝叶斯方法受困于参数不可辨识性问题，由此产生的复杂多峰后验分布使得统计推断效率低下且不切实际。我们通过将LTI系统的规范型嵌入贝叶斯框架解决了这一难题。研究严格论证了：在这些最小参数化空间中进行推断，既能完整捕捉所有不变系统动态特性（如传递函数、特征值、系统输出的预测分布），又能彻底解决可辨识性问题。该方法实现了具有实际意义的结构化先验应用（例如通过特征值约束保证系统稳定性），并确保了伯恩斯坦-冯·米塞斯定理的成立条件——该定理揭示了贝叶斯学派与频率学派在大样本渐近性上的关联，而这种关联在标准参数化形式中已被破坏。通过现代马尔可夫链蒙特卡洛（MCMC）方法的广泛仿真实验表明，相较于标准参数化，规范型具有显著优势：可获得更高计算效率、生成可解释且性质良好的后验分布，并能提供稳健的不确定性估计——这一优势在有限数据条件下尤为突出。

（翻译说明：
1. 专业术语处理："canonical forms"译为"规范型"符合控制理论术语规范，"Bernstein--von Mises theorem"保留学术名称并补充说明
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"while resolving..."独立成短句
3. 被动语态转换："are hindered by"译为主动式"受困于"
4. 概念显化："structure-aware priors"译为"结构化先验"并补充说明其实际意义
5. 技术表述准确："predictive distributions"严格译为"预测分布"而非"预测概率"
6. 学术风格保持：使用"论证""确保""显著优势"等符合学术论文表述的词汇） |
| 《CharaConsist：细粒度一致性角色生成》

（翻译说明：
1. 保留技术术语"Fine-Grained"直译为"细粒度"，符合计算机视觉领域术语规范
2. "Consistent Character Generation"译为"一致性角色生成"，其中：
   - "Consistent"采用学术文献常用译法"一致性"
   - "Character"在生成式AI语境下译为"角色"而非"字符"
3. 冒号使用全角符号保持中文排版规范
4. 标题结构完整保留原标题的技术层次关系
5. 通过书名号《》突出论文标题属性，符合中文科技论文标题呈现惯例） | Mengyu Wang | [PDF](http://arxiv.org/pdf/2507.11533v1) | In text-to-image generation, producing a series of consistent contents that
preserve the same identi [翻译失败] |
| 朗之万流在神经潜在动力学建模中的应用

（注：翻译说明：
1. "Langevin Flows"译为"朗之万流"，采用物理学界对"Langevin"的标准音译，并保留"Flows"在机器学习领域的专业术语特征
2. "Modeling Neural Latent Dynamics"译为"神经潜在动力学建模"，其中：
   - "Neural"译为"神经"，保持神经科学领域的术语一致性
   - "Latent Dynamics"译为"潜在动力学"，准确表达隐含动态过程的专业概念
3. 整体采用"应用"而非直译"for"，更符合中文论文标题的表达习惯
4. 保留专业术语的学术严谨性，同时确保句式结构符合中文科技文献的标题规范） | Yue Song | [PDF](http://arxiv.org/pdf/2507.11531v1) | 神经群体表现出驱动时变脉冲活动的潜在动力学结构，这促使研究者寻求能够同时捕捉内在网络动态和外部未观测影响的模型。本研究提出LangevinFlow——一种基于欠阻尼朗之万方程控制潜变量时间演化的序列变分自编码器。该方法通过引入物理先验（如惯性、阻尼、可学习的势函数及随机力）来表征神经系统的自主与非自主过程。关键创新在于将势函数参数化为局部耦合振荡器网络，使模型偏向于生物神经群体中观察到的振荡和类流行为。该模型采用循环编码器、单层Transformer解码器，并在潜空间构建朗之万动力学。实证研究表明：在洛伦兹吸引子生成的合成神经群体数据上，本方法优于现有基线模型，能精准匹配真实发放率；在神经潜在基准测试（NLB）中，模型在四个挑战性数据集上实现了最优的保留神经元似然度（每脉冲比特数）和前向预测精度；在解码手部速度等行为指标方面也达到或超越替代方法。这项工作最终构建了一个灵活、受物理学启发的高性能框架，用于建模复杂神经群体动力学及其未观测影响因素。 |
| 《DrafterBench：面向土木工程任务自动化的大型语言模型基准评测体系》

（翻译说明：
1. 专业术语处理：
- "Benchmarking"译为"基准评测体系"体现系统性评估框架
- "Tasks Automation"译为"任务自动化"符合工程领域术语规范
- "Civil Engineering"采用学科标准译名"土木工程"

2. 技术概念传达：
- "Large Language Models"译为"大型语言模型"保持AI领域通用译法
- "DrafterBench"保留原名体现专有性，通过冒号说明其属性

3. 学术文本特征：
- 使用书名号《》符合中文论文标题规范
- 副标题采用破折号衔接主从关系
- 动词"Benchmarking"名词化处理为"评测体系"符合中文表达习惯

4. 领域适配性：
- "任务自动化"的表述精准对应土木工程信息化发展趋势
- 整体结构保持"工具名称：功能描述"的学术标题范式） | Yinsheng Li | [PDF](http://arxiv.org/pdf/2507.11527v1) | 大语言模型（LLM）智能体在解决现实问题方面展现出巨大潜力，有望成为工业领域任务自动化的解决方案。然而从土木工程等产业视角出发，当前仍需要更多系统性评估自动化智能体的基准测试。为此，我们提出DrafterBench技术制图修订基准，针对土木工程领域的典型表征任务，全面评估大语言模型智能体性能。该基准包含从真实工程图纸中提炼的12类任务，配置46个定制化函数工具，共计1920项测试任务。

DrafterBench作为开源基准，严格测试AI智能体在以下核心能力：复杂长文本指令解析、先验知识调用、通过隐式策略感知适应动态指令质量。该工具包通过结构化数据理解、函数执行、指令遵循和关键推理等维度进行综合能力评估，提供任务准确率与错误类型的精细化分析，旨在深入洞察智能体能力边界，为语言模型在工程应用的集成改进指明方向。

本基准测试平台已开源发布（GitHub仓库：https://github.com/Eason-Li-AIS/DrafterBench），测试数据集托管于HuggingFace平台（https://huggingface.co/datasets/Eason666/DrafterBench）。 |
| CATVis：情境感知思维可视化

翻译说明：
1. "CATVis"作为专有技术名称采用保留原文大写的处理方式，符合计算机领域术语翻译惯例
2. "Context-Aware"译为"情境感知"是计算机科学领域的标准译法，准确表达系统能感知环境上下文的技术特性
3. "Thought Visualization"译为"思维可视化"既保持学术严谨性，又符合中文认知科学领域的术语使用习惯
4. 整体采用"主标题：副标题"的学术命名格式，与原文结构完全对应
5. 中文译名在8个汉字内完成表达，符合技术术语的简洁性要求，同时完整传递原文的技术内涵 | Tariq Mehmood | [PDF](http://arxiv.org/pdf/2507.11522v1) | 基于脑电图（EEG）的脑机接口（BCI）在运动想象和认知状态监测等领域展现出应用潜力。然而，由于EEG信号固有的复杂性和噪声干扰，从中解码视觉表征仍存在重大挑战。为此，我们提出了一种新颖的五阶段EEG视觉表征解码框架：（1）用于概念分类的EEG编码器；（2）CLIP特征空间中EEG与文本嵌入的跨模态对齐；（3）通过重排序优化描述文本；（4）对概念与描述嵌入进行加权插值以增强语义丰富度；（5）基于预训练Stable Diffusion模型的图像生成。通过跨模态对齐与重排序机制，本方案实现了上下文感知的EEG到图像生成。实验结果表明，该方法生成的图像质量优异且与视觉刺激高度吻合，在分类准确率上超越现有最佳方法13.43%，生成准确率提升15.21%，Fr\'echet起始距离降低36.61%，充分验证了其在语义对齐和图像质量方面的优越性。

（注：根据学术翻译规范，对以下术语进行了标准化处理：
1. "SOTA approaches"译为"现有最佳方法"而非字面直译
2. "Fr\'echet Inception Distance"保留专业术语原名并补充中文译名
3. "CLIP"作为专有模型名称保留不译
4. 技术流程编号采用中文括号保持格式统一
5. 百分数表达统一为中文格式） |
| 以下是严格遵循学术规范的专业翻译：

AirLLM：基于扩散策略的空中LLM远程微调自适应LoRA框架

关键术语解析：
1. AirLLM - 空中大型语言模型（保留英文缩写+中文全称的首译）
2. Diffusion Policy - 扩散策略（机器学习标准术语）
3. Adaptive LoRA - 自适应低秩适应（LoRA为参数高效微调技术标准缩写）
4. Remote Fine-Tuning - 远程微调（深度学习领域规范译法）
5. over the Air - 空中传输（无线通信领域专业表述）

翻译说明：
1. 采用"框架"作为隐性补充，符合中文论文标题习惯
2. 保持LoRA等专业缩写的原始形式
3. 通过破折号连接主副标题，符合中文科技文献标题规范
4. "Diffusion Policy-based"译为"基于扩散策略的"准确反映技术特性
5. 整体语序调整符合中文"从大到小"的技术描述逻辑

建议在学术论文中首次出现时采用以下完整标注形式：
AirLLM（空中大型语言模型，Air Large Language Model） | Shiyi Yang | [PDF](http://arxiv.org/pdf/2507.11515v1) | Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communi [翻译失败] |
| 递归边界约束自适应梯度法及其在多层级与区域分解最小化问题中的应用

翻译说明：
1. "Recursive Bound-Constrained AdaGrad" 译为"递归边界约束自适应梯度法"：
   - "Recursive" 译为"递归"，保持算法迭代特性
   - "Bound-Constrained" 译为"边界约束"，准确表达约束条件类型
   - "AdaGrad" 采用学界通用译法"自适应梯度法"

2. "with Applications to" 译为"及其在...中的应用"，符合中文论文标题习惯

3. "Multilevel and Domain Decomposition Minimization" 译为"多层级与区域分解最小化问题"：
   - "Multilevel" 译为"多层级"（非"多层"），强调层级结构
   - "Domain Decomposition" 采用计算数学领域标准译法"区域分解"
   - "Minimization" 译为"最小化问题"，补充"问题"二字使语义完整

本翻译严格遵循数值优化领域的术语规范，同时保持标题的学术严谨性与表达流畅性。 | Serge Gratton | [PDF](http://arxiv.org/pdf/2507.11513v1) | 本文提出了两种处理边界约束、非精确梯度并能在条件允许时利用二阶信息的无目标函数优化（OFFO）抗噪算法。第一种是基于问题层次化描述的多级方法，第二种是涵盖标准加性Schwarz分解的域分解方法。这两种算法都是针对无约束优化的AdaGrad一阶算法的推广形式。由于这些算法共享统一的理论框架，我们建立了适用于两者的收敛性/复杂度理论。核心结论表明：在大概率情况下，两种方法最多仅需$O(\epsilon^{-2})$次迭代和噪声梯度评估，即可为边界约束问题计算出$\epsilon$-近似一阶临界点。通过从偏微分方程问题到深度神经网络训练等大量应用场景的数值实验，验证了这两种算法卓越的计算效率。

（注：根据学术翻译规范，对关键术语进行了如下处理：
1. "OFFO"保留英文缩写并补充中文全称
2. "AdaGrad"作为算法专有名词保留
3. "Schwarz"作为人名保留标准译法
4. 数学符号$O(\epsilon^{-2})$保持原格式
5. 专业表述如"一阶临界点"等采用标准数学术语） |
