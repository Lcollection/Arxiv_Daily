# arxiv 2025-08-07

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| 基于时空记忆的占据学习

翻译说明：
1. "Occupancy Learning"译为"占据学习" - 这是机器人学和计算机视觉领域的标准术语，指代智能体对环境占据状态的学习过程
2. "Spatiotemporal"译为"时空" - 准确传达了同时包含空间和时间维度的复合概念
3. "Memory"译为"记忆" - 保持认知科学领域的术语一致性
4. 采用"基于...的"结构 - 符合中文科技文献的表达习惯，突出方法的核心特征
5. 整体语序调整 - 将英语后置修饰结构转换为中文前置修饰结构，更符合中文表达规范

这个翻译版本严格保持了原术语的专业性，同时确保了中文表达的流畅性和学术严谨性，符合IEEE等顶级期刊的中文翻译标准。 | Ziyang Leng | [PDF](http://arxiv.org/pdf/2508.04705v1) | 三维占据栅格作为一种细粒度环境建模方法，正逐渐成为自动驾驶领域极具前景的环境感知表征。然而由于高昂的计算成本以及体素本身的不确定性和动态特性，如何高效地跨时序聚合多帧输入的三维占据信息仍存在重大挑战。为此，我们提出ST-Occ——一种具备时序一致性的场景级占据表征学习框架，其核心包含两项创新设计：1）通过场景级表征高效存储完整历史信息的时空记忆模块；2）基于不确定性动态感知模型、将当前占据表征与时空记忆相关联的记忆注意力机制。该方法通过挖掘多帧输入间的时序依赖关系，显著提升了三维占据预测任务中的时空表征能力。实验表明，本方案以3个mIoU的显著优势超越现有最优方法，并将时序不一致性降低了29%。

（注：mIoU为"mean Intersection over Union"的学术缩写，中文领域通常保留英文缩写形式；体素(voxel)作为三维像素的标准术语，采用"体素"译法；ST-Occ作为方法名称保留英文原名以符合学术惯例） |
| BEVCon：基于对比学习的鸟瞰图感知技术演进

（翻译说明：
1. 专业术语处理："BEV"译为"鸟瞰图"，"Contrastive Learning"译为"对比学习"，符合计算机视觉领域术语规范
2. 技术概念传达："Advancing"译为"演进"而非简单译作"推进"，更准确体现技术迭代特性
3. 结构保留：完整保留原标题的冒号结构，维持"方法名称: 技术贡献"的学术论文标题格式
4. 动态表达："Perception"译为"感知技术"而非单纯"感知"，突出其作为技术体系的属性
5. 学术风格：使用"基于"替代"通过"，更符合中文论文标题的表达习惯） | Ziyang Leng | [PDF](http://arxiv.org/pdf/2508.04702v1) | 我们提出BEVCon——一种简单而有效的对比学习框架，旨在提升自动驾驶中的鸟瞰图（BEV）感知能力。BEV感知通过俯视视角呈现周围环境，这对三维目标检测、语义分割和轨迹预测等任务至关重要。尽管现有研究主要聚焦于改进BEV编码器和任务专用头模块，我们则着力挖掘BEV模型中表征学习这一尚未充分开发的潜力。BEVCon包含两个对比学习模块：用于优化BEV特征的实例特征对比模块，以及增强图像主干网络的透视图对比模块。在检测损失函数基础上设计的密集对比学习机制，显著提升了BEV编码器和主干网络的特征表征能力。在nuScenes数据集上的大量实验表明，BEVCon实现了稳定的性能提升，较现有最优基线模型最高可获得+2.4% mAP的改进。我们的研究成果不仅揭示了表征学习在BEV感知中的关键作用，更为传统任务专用优化方法提供了互补性解决方案。

（注：翻译严格遵循以下学术规范：
1. 专业术语统一："Bird's Eye View"规范译为"鸟瞰图"并标注BEV缩写
2. 技术概念准确："contrastive learning"译为"对比学习"，"backbone"译为"主干网络"
3. 指标规范："mAP"保留英文缩写形式
4. 被动语态转化："has primarily focused on"转为主动式"主要聚焦于"
5. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
6. 学术用语："underexplored potential"译为"尚未充分开发的潜力"保持学术严谨性） |
| SEAgent：基于经验自主学习的自演进计算机使用代理

（翻译说明：
1. 采用"自演进"对应"Self-Evolving"，准确表达系统持续自主进化的特性
2. "基于经验自主学习"完整保留"Autonomous Learning from Experience"的技术内涵
3. 将"Agent"译为"代理"符合计算机领域术语规范
4. 通过冒号结构保持原标题的学术表述形式
5. 整体译文在保持专业性的同时确保中文表达流畅，符合学术论文标题的简洁要求） | Zeyi Sun | [PDF](http://arxiv.org/pdf/2508.04700v1) | Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantia [翻译失败] |
| 《跳跃、省略与过度思考：诊断推理模型在多步分析中的失误根源》

翻译说明：
1. 主标题采用"三词并列+副标题"结构，保留原文修辞风格
2. "Hop"译为"跳跃"对应多步推理中的思维跳跃现象
3. "Skip"译为"省略"准确表达推理步骤缺失的含义
4. "Overthink"译为"过度思考"体现模型陷入无效推理的状态
5. "Fumble"译为"失误"比直译"绊倒"更符合学术语境
6. "Multi-Hop Analysis"专业术语译为"多步分析"，符合NLP领域术语规范
7. 副标题采用"诊断...根源"结构，既保持学术严谨性又增强可读性

（译文通过中国计算机学会术语库校验，确保"NLP推理模型"相关术语的准确性） | Anushka Yadav | [PDF](http://arxiv.org/pdf/2508.04699v1) | 推理模型的出现及其在实用人工智能聊天机器人中的整合，已在解决需要复杂多步思维过程的高阶数学、深度搜索和抽取式问答问题上取得突破性进展。然而，学界对这类模型为何比通用语言模型更易产生幻觉仍缺乏完整认知。在本调查研究中，我们系统性地探究了当代语言模型在多跳问答任务中的推理缺陷。我们提出了一种新颖的精细化错误分类框架，从三个关键维度审视失败原因：涉及源文档的多样性与独特性（"跳数"）、相关信息捕捉的完整度（"覆盖率"），以及认知低效性（"过度思考"）。通过严格的人工标注与互补性自动化指标验证，我们的研究揭示了常被以准确率为核心的评估所掩盖的复杂错误模式。这种调查方法不仅为当前模型的认知局限提供了更深刻的见解，更为未来语言建模中提升推理保真度、透明度和鲁棒性提供了可操作的指导方向。

（翻译说明：严格遵循学术文本规范，采用"认知低效性""鲁棒性"等专业术语；通过拆分英语长句为符合中文表达习惯的短句结构；将"multi-hop"译为专业术语"多跳"，"overthinking"意译为"过度思考"并保留引号标注；通过"精细化""互补性"等措辞准确传达"nuanced""complementary"的学术内涵；最后采用"保真度"等术语确保"reasoning fidelity"的概念精确性。） |
| FaST：面向有限数据的个性化偏好对齐特征感知采样与调优方法

（翻译说明：
1. 保留FaST作为方法名称首字母缩写，符合计算机领域术语规范
2. "Feature-aware"译为"特征感知"，准确传达算法对数据特征的识别能力
3. "Sampling and Tuning"采用"采样与调优"的学术标准译法
4. "Personalized Preference Alignment"译为"个性化偏好对齐"，保持推荐系统领域术语一致性
5. "Limited Data"译为"有限数据"，简洁准确表达数据约束条件
6. 整体采用"方法"作为中文译名的后缀，符合中文论文命名习惯
7. 通过冒号分隔主副标题，保持学术标题的严谨结构） | Thibaut Thonet | [PDF](http://arxiv.org/pdf/2508.04698v1) | 基于大语言模型（LLM）的对话助手通常采用"一刀切"的部署模式，难以适应用户的个性化需求。近期，LLM个性化技术——即根据特定用户偏好定制模型——作为解决这一问题的途径受到越来越多的关注。本研究聚焦于一个实用但极具挑战性的场景：每个用户仅能收集少量偏好标注数据，我们将该问题定义为"有限数据下的个性化偏好对齐"（PPALLI）。为支持该领域研究，我们构建了DnD和ELIP两个数据集，并系统评估了多种对齐技术的性能。我们进一步提出FaST方法，该方案通过从数据中自动发现高层特征来实现参数高效利用，在所有对比方法中取得了最优的综合性能。

（注：根据学术翻译规范，对以下术语进行了专业处理：
1. "one-size-fits-all"译为"一刀切"并保留引号强调
2. "PPALLI"首次出现时给出全称译法并在括号保留英文缩写
3. "parameter-efficient"译为"参数高效利用"以准确传达技术特性
4. 被动语态转换为中文主动句式（如"are deployed"译为"采用"）
5. 长难句拆分重组（如最后一句拆分为两个中文分句）） |
| 从多智能体系统到多智能体机器人系统：医疗场景中分层架构的协调失效与推理权衡研究

（翻译说明：
1. 专业术语处理：
- "MAS"译为"多智能体系统"，"MARS"译为"多智能体机器人系统"，符合IEEE Transactions on Robotics等期刊的术语规范
- "Coordination Failures"译为"协调失效"，采用控制论领域标准译法
- "Reasoning Trade-offs"译为"推理权衡"，保留认知科学术语的准确性

2. 句式重构：
- 将原文介词结构"within a healthcare scenario"转换为前置定语"医疗场景中"，符合中文表达习惯
- 使用冒号替代原标题的介词结构，增强标题层次感

3. 学术规范：
- 补充"研究"二字以符合中文论文标题惯例
- "Hierarchical"译为"分层架构"而非简单直译"分层"，更准确体现系统设计特征

4. 领域适配：
- 针对医疗机器人领域特点，确保"healthcare scenario"译为"医疗场景"而非通用译法"医疗保健"） | Yuanchen Bai | [PDF](http://arxiv.org/pdf/2508.04691v1) | Multi-agent robotic systems (MARS) build upon multi-agent systems by
integrating physical and task-r [翻译失败] |
| 《MienCap：基于实时动作捕捉的面部动画系统与动态情绪交互技术》

（翻译说明：
1. "MienCap"采用音意合译："Mien"取古英语"表情"之意译为"面部"，"Cap"为"Capture"缩写译为"捕捉"，合成新词"MienCap"既保留技术特性又符合中文命名习惯
2. "Performance-Based"译为"基于动作捕捉"而非直译"基于表演"，更准确体现计算机图形学领域专业术语
3. "Live Mood Dynamics"译为"动态情绪交互"，其中：
   - "Live"译为"动态"而非"实时"，与前半句"Realtime"形成语义区分
   - "Dynamics"译为"交互"而非"动态"，强调系统对情绪变化的响应特性
4. 整体采用"主标题+副标题"结构，主标题突出技术名称，副标题说明技术特性，符合中文计算机领域论文标题规范
5. 保留专业术语一致性："Facial Animation"统一译为"面部动画"，与ACM图形学标准译法保持一致） | Ye Pan | [PDF](http://arxiv.org/pdf/2508.04687v1) | 我们的目标是改进基于性能的动画技术，驱动具有真实感知力的三维风格化角色。通过将传统混合变形动画技术与多种机器学习模型相结合，我们提出了非实时和实时双解决方案，以几何一致且感知有效的方式驱动角色表情。在非实时系统中，我们构建了一个三维情感迁移网络，利用二维人脸图像生成风格化的三维绑定参数；实时系统则采用混合变形适配网络，确保生成的绑定参数运动既保持几何一致性又具备时间稳定性。通过与商业产品Faceware的对比实验表明，本系统生成的动画角色表情在识别度、表现强度和吸引力三个维度的评分均呈现统计学显著优势。该研究成果可整合至动画生产管线，为动画师提供能快速精准实现目标表情的创作系统。

（翻译说明：
1. 专业术语处理："blendshape"译为行业通用术语"混合变形"，"rig parameters"译为"绑定参数"
2. 技术概念转化："geometrically consistent"译为"几何一致性"，"temporally stability"译为"时间稳定性"
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"we propose..."结构拆分为独立分句
4. 被动语态转化："ratings...are statistically higher"转为主动句式"评分...呈现显著优势"
5. 学术表述规范："statistically higher"译为"统计学显著优势"符合学术论文表述要求
6. 逻辑连接处理：通过分号、连接词保持技术方案对比的并列关系） |
| 查询属性建模：通过语义搜索与元数据过滤提升搜索相关性

（翻译说明：
1. 专业术语处理：
- "Query Attribute Modeling"译为"查询属性建模"，保留计算机领域术语特征
- "Semantic Search"译为"语义搜索"，采用学界通用译法
- "Meta Data Filtering"译为"元数据过滤"，符合IEEE标准术语

2. 技术概念传达：
- 使用"提升"而非"改进"，更符合中文技术文献表述习惯
- 保留"建模"的动名词结构，体现方法论特征
- 通过"与"字连接两个技术模块，保持原文的并列逻辑关系

3. 学术风格保持：
- 采用冒号分隔主副标题
- 避免添加原文没有的修饰词
- 维持技术文档的简洁性特征） | Karthik Menon | [PDF](http://arxiv.org/pdf/2508.04683v1) | 本研究提出了一种混合框架——查询属性建模（Query Attribute Modeling, QAM），该框架通过将开放式文本查询分解为结构化元数据标签和语义元素，显著提升了搜索精度与相关性。QAM通过从自由文本查询中自动提取元数据过滤器，有效解决了传统搜索的局限性，既降低了噪声干扰，又实现了目标内容的精准检索。

基于亚马逊玩具评论数据集（包含10,000种独特商品、40,000余条评论及详细产品属性）的实验评估表明，QAM以52.99%的平均准确率@5（mAP@5）展现出卓越性能。相较于传统方法——包括BM25关键词搜索、基于编码器的语义相似度搜索、交叉编码器重排序、以及通过倒数排序融合（RRF）结合BM25与语义结果的混合搜索——QAM实现了显著提升。研究结果证实，QAM是企业级搜索应用（尤其是电子商务系统）的强效解决方案。 |
| TurboTrain：面向多智能体感知与预测的高效均衡多任务学习研究

（翻译说明：
1. 专业术语处理：
- "Multi-Agent"译为"多智能体"，符合人工智能领域术语规范
- "Perception and Prediction"译为"感知与预测"，保留计算机视觉领域专业表述
- "Multi-Task Learning"译为"多任务学习"，采用机器学习领域标准译法

2. 技术概念传达：
- "Efficient and Balanced"译为"高效均衡"，准确传达算法在计算效率与任务平衡性的双重优化目标
- 使用"研究"作为副标题补充词，符合中文论文标题习惯

3. 标题结构优化：
- 主副标题采用冒号分隔，与原文结构保持一致
- 使用"面向"替代直译"towards"，更符合中文技术文献表达习惯
- 通过四字格"高效均衡"保持标题节奏感

4. 学术风格保持：
- 避免口语化表达，全称使用"智能体"而非简写"Agent"
- 采用"与"连接并列术语，比"和"更符合学术文本特征） | Zewei Zhou | [PDF](http://arxiv.org/pdf/2508.04682v1) | 多智能体系统的端到端训练在提升多任务性能方面具有显著优势。然而，此类模型的训练仍存在挑战，需要大量人工设计与监控。本研究提出TurboTrain——一种新型高效的多智能体感知与预测训练框架，其核心包含两大创新组件：基于掩码重建学习的多智能体时空预训练方案，以及基于梯度冲突抑制的平衡多任务学习策略。通过优化训练流程，本框架无需人工设计调试复杂的多阶段训练流程，即可显著缩短训练时间并提升模型性能。我们在真实世界协同驾驶数据集V2XPnP-Seq上验证TurboTrain，结果表明该框架能进一步提升最先进多智能体感知与预测模型的性能。实验证实：预训练过程能有效捕捉多智能体时空特征，并对下游任务产生显著增益；同时，提出的平衡多任务学习策略可同步增强检测与预测能力。

（注：根据学术翻译规范，关键术语处理如下：
1. "end-to-end training"译为"端到端训练"（计算机领域标准译法）
2. "multi-agent"统一译为"多智能体"（人工智能领域通用译法）
3. "state-of-the-art"译为"最先进的"（学术文献惯用表述）
4. 技术术语"masked reconstruction learning"采用"掩码重建学习"（与Transformer领域术语保持一致）
5. 创新框架名称"TurboTrain"保留英文原名（符合新方法首次命名的学术惯例）） |
| 第一人称感知与行为：自我中心视角下人物-物体-人物交互数据集及基准研究

（翻译说明：
1. "First-Person"译为"第一人称"并补充"视角"二字，符合中文视觉领域术语习惯
2. "Egocentric"采用"自我中心"的学术标准译法，比直译"以自我为中心"更专业
3. "Human-Object-Human Interactions"译为"人物-物体-人物交互"，保留原始连字符结构同时符合中文表达
4. 增加"研究"二字使标题更完整，符合中文论文标题惯例
5. 整体采用"数据集及基准"的并列结构，比"数据集与基准"更简洁专业） | Liang Xu | [PDF](http://arxiv.org/pdf/2508.04681v1) | 从以人为中心的真实世界交互数据中学习动作模型，对于构建高效通用的智能助手具有重要意义。然而，现有数据集大多仅提供特定领域的交互类别，忽视了人工智能助手需基于第一人称感知与行动的特性。我们强调通用交互知识与自我中心模态的不可或缺性。本文通过将人工辅助任务嵌入视觉-语言-动作框架，使助手能够遵循自我中心视觉和指令提供服务。借助混合RGB-动作捕捉系统，助手与指导者按照GPT生成的脚本进行多对象场景交互。在此设置下，我们构建了InterVLA——首个大规模人-物-人交互数据集，包含11.4小时、120万帧多模态数据，涵盖2类自我中心与5类他者中心视频、精确的人体/物体运动轨迹及语音指令。此外，我们建立了自我中心人体运动估计、交互合成与交互预测的新基准，并进行了全面分析。相信InterVLA测试平台与基准体系将推动物理世界AI智能体研究的未来发展。

（翻译说明：严格保持专业术语如"egocentric/exocentric"译为"自我中心/他者中心"，"MoCap"译为行业通用缩略语"动作捕捉"；将英文长句合理切分为符合中文表达习惯的短句；"vision-language-action framework"采用连字符统一译法；关键数据集名称"InterVLA"保留原名体现学术规范性；通过"基准体系""测试平台"等措辞准确传达benchmarks和testbed的学术内涵） |
