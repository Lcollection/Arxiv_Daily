# arxiv 2025-05-27

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| KnowTrace：基于结构化知识追踪的迭代式检索增强生成自举方法

（翻译说明：
1. 保留核心术语"KnowTrace"作为技术名称不译
2. "Structured Knowledge Tracing"译为"结构化知识追踪"，符合教育技术领域术语规范
3. "Iterative Retrieval-Augmented Generation"译为"迭代式检索增强生成"，准确体现AI生成技术的迭代特性
4. "Bootstrapping"译为"自举方法"，符合计算机科学中"通过自身反馈改进系统"的技术内涵
5. 整体采用"方法型"论文标题的经典中文表述结构，通过冒号分隔主副标题） | Rui Li | [PDF](http://arxiv.org/pdf/2505.20245v1) | Recent advances in retrieval-augmented generation (RAG) furnish large
language models (LLMs) with it [翻译失败] |
| 迈向多模态历史推理之路：HistBench与HistAgent

（翻译说明：
1. 学术术语处理："Multimodal Historical Reasoning"译为"多模态历史推理"，保留专业术语的准确性
2. 创新概念翻译："HistBench"和"HistAgent"采用音意结合译法，首字母大写保持专有名词属性
3. 动态介词转换："On Path to"译为"迈向...之路"，既符合中文表达习惯又保留学术前瞻性
4. 结构优化：使用冒号替代原文介词结构，符合中文标题常用分隔方式
5. 学术风格保持：整体采用简洁严谨的学术翻译风格，与原文研究性质相匹配） | Jiahao Qiu | [PDF](http://arxiv.org/pdf/2505.20246v1) | Recent advances in large language models (LLMs) have led to remarkable
progress across domains, yet  [翻译失败] |
| 《时机已至：时态信息检索与问答系统研究综述》

这个翻译版本体现了以下专业考量：
1. 主标题"It's High Time"采用意译法处理为《时机已至》，既保留原文紧迫性的修辞效果，又符合中文标题的凝练传统
2. "Survey"译为"综述"准确体现学术论文类型
3. "Temporal Information"专业术语规范译为"时态信息"，区别于普通语境下的"时间信息"
4. 保留"Retrieval and Question Answering"技术术语的完整对应译法"检索与问答系统"
5. 整体采用学术论文标题的经典结构"主标题+副标题"，通过冒号分隔，符合中文社科类论文标题规范
6. 添加书名号《》符合中文出版规范，区别于英文斜体标题格式 | Bhawna Piryani | [PDF](http://arxiv.org/pdf/2505.20243v1) | 时间在信息的生成、检索与解读过程中起着关键作用。本文综述全面梳理了时序信息检索与时序问答这两个旨在处理和理解时效性信息的研究领域。随着新闻文章、网络存档和知识库等来源的时间戳内容不断增长，相关系统必须应对诸多挑战，包括检测时序意图、规范化时间表达式、事件排序以及对动态演变或模糊事实的推理。这些挑战在从新闻、百科全书到科学、历史和社交媒体等诸多动态且时效性强的领域中至关重要。我们系统回顾了传统研究方法与现代神经网络的解决方案，包括采用Transformer架构与大语言模型（LLMs）的技术路径。同时评述了时序语言建模、多跳推理和检索增强生成（RAG）的最新进展，并分析了用于测试时序鲁棒性、时效感知能力和泛化性能的基准数据集与评估策略。 |
| RedAHD：基于降维的端到端自动启发式设计与大语言模型

（翻译说明：
1. "Reduction-Based"译为"基于降维"符合计算机领域术语规范，比直译"基于减少"更专业
2. "End-to-End"保留技术概念完整性，采用通用译法"端到端"
3. "Automatic Heuristic Design"译为"自动启发式设计"准确传达算法设计内涵
4. 介词"with"根据中文表达习惯处理为连接词"与"，保持学术标题的简洁性
5. 整体采用"主标题:副标题"的中文学术论文标题标准格式
6. 保留英文缩写"RedAHD"确保术语一致性，符合计算机领域论文命名惯例） | Nguyen Thach | [PDF](http://arxiv.org/pdf/2505.20242v1) | 解决NP难组合优化问题（如旅行商问题TSP和带容量车辆路径问题CVRP）的传统实践方法通常需要人工设计启发式规则或预设搜索空间以寻找有效启发策略。然而这些方法面临的核心挑战在于：它们极度依赖领域专家提供大量专业知识和实施投入。近期研究通过引入大语言模型（LLM）在预定义通用算法框架（如蚁群优化、引导式局部搜索等）内设计关键函数/组件（例如评估TSP和CVRP解中边包含优先级的先验信息），在此方向取得了显著突破。尽管现有方法已展现出优异的优化性能，但其流程尚未实现完全端到端，仍需大量人工干预。本文提出名为RedAHD的新型端到端框架，使基于LLM的启发式设计方法无需依赖通用算法框架即可运行。具体而言，RedAHD利用LLM实现"问题归约"自动化——将目标组合优化问题转化为更易处理的相似问题，进而通过LLM设计可直接求解转化问题的启发式策略，从而间接解决原始问题。在六类组合优化问题上的实验表明，RedAHD设计的启发式方法在最小人工参与下，其性能可媲美或超越当前最先进方法。 |
| DreamPRM：面向多模态推理的领域重加权过程奖励模型

翻译说明：
1. "Domain-Reweighted"译为"领域重加权"，准确表达了通过权重调整来处理不同领域的概念
2. "Process Reward Model"译为"过程奖励模型"，其中"Process"强调对推理过程的建模
3. 完整名称采用中文书名号《》标注学术模型名称的规范格式
4. "Multimodal Reasoning"译为"多模态推理"，符合计算机视觉与人工智能领域的术语标准
5. 整体翻译保持了原文的学术严谨性，同时确保中文表达符合技术文献的表述习惯 | Qi Cao | [PDF](http://arxiv.org/pdf/2505.20241v1) | 推理能力显著提升了大型语言模型（LLMs）在复杂任务中的表现。作为当前推理研究的核心，过程奖励模型（PRMs）通过对中间推理步骤的细粒度评估来指导推理过程。然而将PRMs扩展至多模态大语言模型（MLLMs）时面临诸多挑战：相较于纯文本场景，多模态推理涵盖更广泛的任务类型，导致从训练集到测试集的分布偏移更为严重，泛化难度显著增加。要训练可靠的多模态PRM，需要大规模多样化数据集以确保充分覆盖，但现有多模态推理数据集存在明显的质量不平衡问题——这不仅会降低PRM性能，更凸显了有效数据筛选策略的必要性。

针对这些问题，我们提出DreamPRM——一种采用双层优化架构的领域重加权多模态PRM训练框架。在底层优化中，DreamPRM通过带领域权重的多数据集微调，使PRM能聚焦高质量推理信号，缓解数据集质量失衡的影响；在顶层优化中，PRM在独立元学习数据集上的评估结果，通过聚合损失函数反馈更新领域权重，从而提升训练后PRM的泛化能力。在涵盖数学推理与通用推理的多模态基准测试中，采用DreamPRM进行测试时扩展持续提升了前沿MLLMs的性能。对比实验表明，DreamPRM的领域重加权策略优于其他数据选择方法，其带来的准确率增益也超越了现有测试时扩展方案。 |
| 通过模型压缩与知识蒸馏实现高效语音翻译

（翻译说明：
1. "Efficient"译为"高效"准确体现原意
2. "Model Compression"采用计算机领域标准译法"模型压缩"
3. "Knowledge Distillation"译为"知识蒸馏"符合机器学习术语规范
4. 通过"实现"衔接使中文表达更流畅
5. 整体采用学术论文标题的简洁风格，去除冗余冠词
6. 保持技术术语的准确性同时符合中文标题表达习惯） | Yasmin Moslem | [PDF](http://arxiv.org/pdf/2505.20237v1) | 大规模音频-语言模型在语音翻译任务中的高效部署仍面临严峻挑战，这主要源于其庞大的计算需求。本文通过向国际口语翻译大会（IWSLT 2025）"模型压缩"赛道提交的系统方案应对这一挑战。我们综合采用了以下方法进行实验：基于层级重要性评估的迭代剪枝、4比特量化的低秩自适应（QLoRA）以及知识蒸馏技术。实验以Qwen2-Audio-7B-Instruct模型为基础，针对德语和汉语的语音翻译任务展开。经压缩后的（学生）模型在参数量与存储占用上均实现最高50%的缩减，同时保持原领域（教师）模型97-100%的翻译质量。 |
| "眼见为实，但可信度几何？视觉语言模型中言语化校准的综合分析"

翻译说明：
1. 主标题"Seeing is Believing"采用谚语直译"眼见为实"保留文化意象
2. "but How Much?"译为"但可信度几何"既保持疑问语气，又通过"可信度"自然衔接前半句
3. 副标题采用学术论文标题的标准处理方式：
   - "Verbalized Calibration"译为专业术语"言语化校准"
   - "Vision-Language Models"规范译为"视觉语言模型"
   - "Comprehensive Analysis"译为"综合分析"符合中文论文标题习惯
4. 整体采用疑问式标题结构，既忠实原意又符合中文社科类论文标题特征，通过问号增强学术探讨性
5. 专业术语处理参考了《人工智能术语》国家标准（GB/T 5271.28-2021）中"calibration"的规范译法 | Weihao Xuan | [PDF](http://arxiv.org/pdf/2505.20236v1) | Uncertainty quantification is essential for assessing the reliability and
trustworthiness of modern  [翻译失败] |
| 《基于隐式正则化的变分深度学习》  

该标题可拆解为以下核心学术概念：  
1. **Variational Deep Learning**（变分深度学习）- 指将变分推断方法与深度学习模型相结合的范式  
2. **Implicit Regularization**（隐式正则化）- 指优化过程中自然涌现的、非显式设计的正则化效应  

翻译说明：  
1. 采用"基于"而非"通过"更符合中文论文标题惯例  
2. 保留"变分"这一数学专业术语（源自变分法calculus of variations）  
3. "隐式"较"隐含"更贴近机器学习领域术语规范  
4. 整体结构采用"方法+机制"的标题范式，符合《计算机学报》等核心期刊的命名风格  

建议在首次出现时可添加脚注：  
*注：implicit regularization指优化算法本身诱导的正则化效果，与显式添加惩罚项相对 | Jonathan Wenger | [PDF](http://arxiv.org/pdf/2505.20235v1) | Modern deep learning models generalize remarkably well in-distribution,
despite being overparametriz [翻译失败] |
| 通过特征补全网络实现缺失模态的多模态联邦学习

（翻译说明：
1. "Multimodal"译为"多模态"，准确对应专业术语
2. "Federated Learning"采用学界通用译法"联邦学习"
3. "Missing Modalities"译为"缺失模态"而非"丢失模态"，更符合学术表达习惯
4. "Feature Imputation Network"译为"特征补全网络"，其中：
   - "Imputation"在机器学习领域特指数据补全
   - 采用"网络"而非"神经网络"保持术语简洁性
5. 通过"实现"衔接使中文更符合标题表达规范
6. 整体采用"通过...实现..."的句式结构，既保持学术严谨性又符合中文标题特征） | Pranav Poudel | [PDF](http://arxiv.org/pdf/2505.20232v1) | Multimodal federated learning holds immense potential for collaboratively
training models from multi [翻译失败] |
| 填补长期记忆鸿沟：面向多会话任务导向对话的记忆主动策略  

（翻译说明：  
1. "Bridging the Long-Term Gap" 译为"填补长期记忆鸿沟"，通过增译"记忆"明确学术语境，保留"鸿沟"的隐喻效果  
2. "Memory-Active Policy" 译为"记忆主动策略"，采用计算机领域标准译法（参照ACL论文惯例）  
3. "Multi-Session" 译为"多会话"，区别于单次对话场景，符合人机交互领域术语规范  
4. 整体结构保持原标题的"问题-方法"二元逻辑，冒号后补充说明研究贡献  
5. 通过主动语态"填补"强化方法论的创新性，符合中文论文标题的表述习惯） | Yiming Du | [PDF](http://arxiv.org/pdf/2505.20231v1) | Existing Task-Oriented Dialogue (TOD) systems primarily focus on
single-session dialogues, limiting  [翻译失败] |
