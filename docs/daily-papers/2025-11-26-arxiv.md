# arxiv 2025-11-26

| 标题 | 作者 | PDF链接 |  摘要 |
|------|------|--------|------|
| RubricRL：面向文本到图像生成的简易泛化奖励机制 | Xuelu Feng | [PDF](https://arxiv.org/pdf/2511.20651v1) | 强化学习（RL）近期已成为实现文本到图像生成模型与人类偏好对齐的重要方法。然而，核心挑战在于如何设计有效且可解释的奖励机制。现有方法通常依赖固定权重的复合指标（如CLIP、OCR和真实感评分）或从人类偏好模型提炼的单一标量奖励，这限制了方法的可解释性与灵活性。我们提出RubricRL框架，该基于量规的奖励设计方案兼具简洁性与通用性，能显著提升可解释性、组合性及用户控制能力。与使用黑箱标量信号不同，RubricRL会为每个提示动态构建结构化量规——一个可分解的细粒度视觉标准清单（包括物体准确性、属性精确度、OCR保真度和真实感等），并依据输入文本进行定制。每个标准由多模态评判器（如o4-mini）独立评估，并通过提示自适应加权机制突出最关键维度。该设计不仅为策略优化（如GRPO或PPO）提供可解释的模块化监督信号，还允许用户直接调整需要奖励或惩罚的维度。在自回归文本到图像模型上的实验表明，RubricRL在提升提示遵循度、视觉细节和泛化能力的同时，为跨文本到图像架构的可解释强化学习对齐提供了灵活可扩展的基础框架。 |
| MedROV：实现跨多种医学成像模态的实时开放词汇检测 | Tooba Tehreem Sheikh | [PDF](https://arxiv.org/pdf/2511.20650v1) | 医学影像中的传统目标检测模型采用封闭集范式运作，这限制了其检测新型标记目标的能力。开放词汇目标检测（OVOD）虽能突破这一局限，但由于数据集稀缺及图文对齐能力薄弱，在医学影像领域仍探索不足。为弥补这一空白，我们推出了MedROV——首个面向医学影像的实时开放词汇检测模型。为实现开放词汇学习，我们构建了大规模数据集Omnis，涵盖9种成像模态的60万个检测样本，并采用伪标注策略处理多源数据集的缺失标注问题。此外，通过融入大型预训练基础模型的知识，我们进一步增强了模型的泛化能力。MedROV借助对比学习与跨模态表征，可有效检测已知与新型解剖结构。实验结果表明，MedROV以70 FPS的实时速度运行，在医学图像检测任务中较现有最优基础模型平均绝对提升40 mAP50，超越封闭集检测器超过3 mAP50，创下医学检测新标杆。项目源码、数据集及训练模型已公开于：https://github.com/toobatehreem/MedROV。 |
| Infinity-RoPE：自回归自推演驱动的可控无限视频生成技术

（解析说明：
1. "Action-Controllable"译为"可控"并前置，符合中文技术表述习惯
2. "Infinite Video Generation"采用"无限视频生成"标准译法
3. "Autoregressive Self-Rollout"译为"自回归自推演"，其中：
   - "Autoregressive"保持"自回归"标准术语
   - "Self-Rollout"创造性译为"自推演"，准确传达模型自主展开时序预测的内涵
4. 整体采用"技术特性+实现机制"的句式结构，通过冒号衔接形成完整技术命名体系
5. "Emerges From"隐含的因果关系通过"驱动"一词实现动态转化，体现技术演进逻辑） | Hidir Yesiltepe | [PDF](https://arxiv.org/pdf/2511.20649v1) | 当前自回归视频扩散模型受限于三个核心瓶颈：（一）基础模型三维旋转位置编码（3D-RoPE）施加的有限时间跨度；（二）生成长序列视频时提示词响应迟缓导致细粒度动作控制能力减弱；（三）无法在单次生成流中实现非连续的电影式场景转换。我们提出$\infty$-RoPE这一统一推理时框架，通过三个相互关联的组件——块相对论RoPE、KV刷新与RoPE截断——系统解决上述局限。块相对论RoPE将时间编码重构为移动局部参考系：新生成的潜空间块相对于基础模型最大帧范围进行旋转，同时早期块反向旋转以保持相对时间几何关系。这种相对论表述消除了固定时间位置，实现了远超基础位置限制的连续视频生成。为实现无需重新编码的细粒度动作控制，KV刷新机制通过仅保留全局锚点与末帧潜空间来更新KV缓存，从而确保即时提示词响应。最后，RoPE截断通过在时间RoPE坐标中引入受控间断，实现在单次连续生成流中进行多镜头场景转换。这些组件共同使$\infty$-RoPE成为无需训练即可实现无限时长、可控且具电影质感的视频扩散基础框架。综合实验表明，$\infty$-RoPE在VBench综合评分中持续超越现有自回归模型。 |
| 基于行列式点过程引导策略优化的多样化视频生成 | Tahira Kazimi | [PDF](https://arxiv.org/pdf/2511.20647v1) | 尽管近期文本到视频（T2V）扩散模型在生成质量与提示词对齐方面取得显著进展，但在基于单一文本提示生成多个视频时，往往存在输出多样性不足的问题。我们通过将该问题构建为集合级策略优化任务来应对这一挑战，其目标是训练能够覆盖给定提示词对应多种合理结果的策略框架。为此，我们提出DPP-GRPO——一个融合行列式点过程（DPPs）与群组相对策略优化（GRPO）理论的新型多样化视频生成框架，通过对多样化生成结果施加显式奖励来解决该问题。我们的目标函数通过双重机制将多样性转化为显式信号：既对冗余样本施加收益递减约束（通过DPP实现），又对候选视频集提供群组反馈（通过GRPO实现）。该框架具备即插即用和模型无关的特性，能在保持提示词忠实度与感知质量的同时，有效提升视频在视觉外观、摄像机运动和场景结构等方面的多样性。我们在WAN与CogVideoX平台上实现该方法，实验表明在VBench、VideoScore等前沿基准测试及人类偏好研究中，我们的方法持续提升视频多样性。此外，我们开源了代码并发布了包含30,000条多样化提示词的新基准数据集，以支持后续研究。 |
| LocateAnything3D：基于视觉-语言链式观测的三维检测系统

（注：Chain-of-Sight在此语境下译为"链式观测"，既保留了"链式"的连贯性特征，又通过"观测"准确传达视觉感知与语言推理的协同机制，符合三维检测领域的技术内涵） | Yunze Man | [PDF](https://arxiv.org/pdf/2511.20648v1) | 要在现实世界中行动，模型必须能识别所见之物并知晓自身在三维空间中的位置。当前视觉语言模型（VLM）在开放式二维描述与定位方面表现卓越，但多目标三维检测能力在VLM工具箱中仍基本缺失。我们提出LocateAnything3D——一种原生VLM解决方案，将三维检测构建为下一词元预测问题。其核心在于简明的视觉链（Chain-of-Sight）序列，该设计模拟人类从图像推理的认知过程：先在二维图像中定位物体，继而推断其距离、尺寸与位姿。解码器首先通过视觉思维链输出二维检测结果，随后遵循由易到难的学习范式预测三维边界框：跨物体层面采用由近及远的排序策略，既降低早期歧义又符合以自我为中心的实用逻辑；单物体层面则按稳定性与可学习性对信息分级，采用从相机中心出发、依次解构尺寸与旋转角度的因子化方法。这种原生VLM接口在保持开放词汇和视觉提示能力的同时，无需专用检测头。在极具挑战的Omni3D基准测试中，我们的模型以49.89 AP_3D的成绩刷新纪录，即使基线模型使用真实二维边界框标注，本方法仍实现15.51的绝对性能提升。该模型还展现出对未见类别的零样本泛化能力与强大鲁棒性。通过将三维检测转化为规范化的下一词元预测问题，LocateAnything3D为模型实现三维感知提供了实用基础框架。 |
| 面向密集场景理解的三维感知多任务学习与跨视角关联机制 | Xiaoye Wang | [PDF](https://arxiv.org/pdf/2511.20646v1) | 本文致力于解决训练单一网络以联合执行分割与深度估计等多项密集预测任务的挑战，即多任务学习（MTL）。现有方法主要在二维图像空间中捕捉跨任务关联，往往导致生成缺乏三维感知的非结构化特征。我们认为三维感知对于建立跨任务关联至关重要，这种关联是实现全面场景理解的核心。为此，我们提出通过整合跨视角关联（即代价体积）作为几何一致性约束，将其融入多任务学习网络。具体而言，我们设计了一个轻量级跨视角模块（CvM），该模块在多任务间共享，通过跨视角信息交换捕获视角间关联，并与多任务编码器提取的特征融合以进行多任务预测。该模块具备架构无关性，可同时适用于单视角与多视角数据。在NYUv2和PASCAL-Context数据集上的大量实验表明，我们的方法能有效将几何一致性注入现有多任务学习模型，显著提升其性能。 |
| PixelDiT：用于图像生成的像素扩散变换器模型

（注：采用技术术语直译与专业表述结合的方式，既保留了"Pixel"（像素）、"Diffusion"（扩散）、"Transformers"（变换器）的核心技术概念，又通过增补"模型"二字符合中文计算机领域命名规范。"for Image Generation"采用功能化译法处理为前置定语，整体结构符合中文技术名词的命名习惯） | Yongsheng Yu | [PDF](https://arxiv.org/pdf/2511.20645v1) | 潜空间建模一直是扩散变换器（DiTs）的标准范式。然而，该方法依赖包含预训练自编码器的两阶段流程，会引入有损重建导致误差累积，同时阻碍联合优化。为解决这些问题，我们提出PixelDiT——一种消除自编码器需求的单阶段端到端模型，直接在像素空间学习扩散过程。该模型采用全基于Transformer的双层级架构：捕捉全局语义的块级DiT与优化纹理细节的像素级DiT，在保持精细细节的同时实现像素空间扩散模型的高效训练。我们的分析表明，有效的像素级令牌建模是像素扩散成功的关键。PixelDiT在ImageNet 256×256数据集上取得1.61的FID分数，显著超越现有像素生成模型。我们进一步将PixelDiT扩展至文生图领域，在像素空间完成1024×1024分辨率预训练，最终在GenEval获得0.74分、DPG-bench取得83.5分，性能接近最佳潜扩散模型。 |
| 视觉-语言记忆用于空间推理 | Zuntao Liu | [PDF](https://arxiv.org/pdf/2511.20644v1) | 空间推理是智能机器人的核心能力，然而当前视觉语言模型在基于视频的空间推理任务中仍难以达到人类水平。这一差距主要源于两大挑战：语义与几何信息错位导致的三维理解不一致，以及缺乏持续记忆机制来维持三维表征与理解的时序连贯性。为突破这些局限，我们提出VLM$^2$——一种具备持久记忆的视觉语言模型，能够仅从二维视频中构建视角一致的三维感知表征进行空间推理。具体而言，为增强长时序推理能力，我们设计了双记忆模块：工作记忆作为滑动窗口处理即时上下文，情景记忆则负责整合存储关键长期信息。该架构以固定计算成本实现了高效的长时序空间推理。在多个基准测试上的广泛实验表明，VLM$^2$在纯视频模型中取得了最先进的性能，显著推动了视觉空间智能的前沿发展。 |
| 概念感知批量采样优化语言-图像预训练

（注：该翻译采用学术文献常用表达方式，其中"Concept-Aware"译为"概念感知"体现算法对语义概念的识别能力，"Batch Sampling"译为"批量采样"符合机器学习领域术语规范，"Improves"译为"优化"准确传达性能提升含义，整体表述符合计算机视觉与自然语言处理交叉领域的中文表达惯例。） | Adhiraj Ghosh | [PDF](https://arxiv.org/pdf/2511.20643v1) | 视觉语言模型应基于何种数据进行训练？为回答这一问题，当前多数数据策展工作聚焦于数据集质量。然而现有方法普遍存在两大局限：（一）离线性——基于预设过滤标准生成静态数据集；（二）概念无关性——采用基于模型的过滤器会引入额外数据偏差。本研究突破此类离线式、概念无关的方法桎梏，提出更具灵活性、任务自适应的在线概念化策展方案。我们的首要贡献是构建DataConcept数据集，包含1.28亿网络爬取的图文对，并标注其细粒度概念构成。基于此，我们提出概念感知批量采样框架（CABS），该轻量而高效的框架能根据特定目标分布动态构建批次。我们开发两种变体：（一）多样性最大化（CABS-DM）——确保批次覆盖广泛概念；（二）频次最大化（CABS-FM）——构建具有高客体复现度的批次。通过对28个基准的广泛评估，我们证明CABS方法显著提升CLIP/SigLIP模型性能，培育出高效能模型。总体而言，CABS为专有在线数据策展算法提供了强有力的开源替代方案，使实践者能通过自定义概念分布优化特定下游任务。 |
| 释放视觉语言模型在长尾多标签视觉识别中的潜力 | Wei Tang | [PDF](https://arxiv.org/pdf/2511.20641v1) | 长尾多标签视觉识别面临显著挑战：图像通常包含多个标签且类别分布极不均衡，导致模型偏向头部类别而在尾部类别表现欠佳。近期研究利用CLIP等预训练视觉语言模型，结合长尾学习技术挖掘丰富的视觉-文本先验知识以提升性能。然而现有方法通常直接从失衡数据集中推导类间语义关系，因尾部类别数据稀缺导致相关性不可靠。此外，CLIP的零样本范式专为单标签图文匹配优化，难以直接适用于多标签任务。针对这些问题，我们提出相关性自适应提示网络（CAPNET）——一种新颖的端到端框架，通过CLIP文本编码器显式建模标签相关性。该框架集成图卷积网络实现标签感知传播，并采用可学习软提示优化嵌入表示。通过结合分布平衡的Focal损失函数与类别感知重加权策略，在数据不均衡条件下实现优化训练。该框架还通过测试时集成提升泛化能力，并采用参数高效微调技术重新对齐视觉-文本模态，在保持头部类别性能的同时避免对尾部类别过拟合。在VOC-LT、COCO-LT和NUS-WIDE等基准数据集上的大量实验与消融研究表明，CAPNET相较现有最优方法实现显著提升，验证了其在现实长尾多标签视觉识别任务中的有效性。 |
