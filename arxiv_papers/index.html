
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-2024-09-04-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-2024-09-04-papers">Arxiv 2024-09-04 Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
</tr>
</thead>
<tbody>
<tr>
<td>RoboTwin：配备生成式数字孪生的双臂机器人基准测试（早期版本）</td>
<td>Yao Mu</td>
<td><a href="http://arxiv.org/pdf/2409.02920v1">PDF</a></td>
<td>N/A</td>
<td>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</td>
</tr>
<tr>
<td>HiPrompt：使用分层多语言大型语言模型提示实现无调优的高分辨率生成</td>
<td>Xinyu Liu</td>
<td><a href="http://arxiv.org/pdf/2409.02919v1">PDF</a></td>
<td>N/A</td>
<td>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts</td>
</tr>
<tr>
<td>UC-NeRF：从内窥镜稀疏视角实现不确定性感知的条件神经辐射场</td>
<td>Jiaxin Guo</td>
<td><a href="http://arxiv.org/pdf/2409.02917v1">PDF</a></td>
<td>N/A</td>
<td>UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views</td>
</tr>
<tr>
<td>大型语言模型能否获得驾照？一个面向自动驾驶可靠通用智能的基准测试</td>
<td>Yuhang Lu</td>
<td><a href="http://arxiv.org/pdf/2409.02914v1">PDF</a></td>
<td>N/A</td>
<td>Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving</td>
</tr>
<tr>
<td>SITAR：用于动作识别的半监督图像变换器</td>
<td>Owais Iqbal</td>
<td><a href="http://arxiv.org/pdf/2409.02910v1">PDF</a></td>
<td>N/A</td>
<td>SITAR: Semi-supervised Image Transformer for Action Recognition</td>
</tr>
<tr>
<td># Arxiv Papers</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>RoboTwin：配备生成式数字孪生的双臂机器人基准（早期版本）</td>
<td>双臂机器人及其工具使用能力的有效协作在机器人技术的发展中日益重要。这些技能在扩展机器人在多样化的现实世界环境中操作的能力方面起着重要作用。然而，进展因缺乏专门的训练数据而受阻。本文介绍了RoboTwin，这是一个新颖的基准数据集，结合了现实世界的远程操作数据和数字孪生中的合成数据，专为双臂机器人场景设计。利用COBOT Magic平台，我们收集了关于工具使用和人机交互的多样化数据。我们提出了一种创新的方法，使用AI生成的内容创建数字孪生，将2D图像转化为详细的3D模型。此外，我们利用大型语言模型生成专家级别的训练数据和面向功能的任务特定姿态序列。我们的主要贡献包括：1) RoboTwin基准数据集，2) 高效的现实到模拟管道，以及3) 使用语言模型进行自动专家级别数据生成。这些进展旨在解决机器人训练数据的短缺问题，有望加速开发更多能力更强、适应性更广的机器人系统，以应对广泛的现实世界应用。项目页面位于https://robotwin-benchmark.github.io/early-version/。</td>
<td>Yao Mu</td>
<td><a href="http://arxiv.org/pdf/2409.02920v1">PDF</a></td>
<td>N/A</td>
<td>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</td>
<td>Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at https://robotwin-benchmark.github.io/early-version/</td>
</tr>
<tr>
<td>HiPrompt：使用分层多层语言模型提示实现无调优的高分辨率生成</td>
<td>使用预训练扩散模型生成高分辨率图像的潜力巨大，但这些模型在扩展到4K分辨率及以上时，常常面临物体重复和结构伪影的问题。我们发现，问题在于生成多尺度图像时使用单一提示的效率不足。为此，我们提出了HiPrompt，一种无需调整的新解决方案，通过引入分层提示来解决上述问题。分层提示提供了全局和局部的指导。具体来说，全局指导来自描述整体内容的用户输入，而局部指导则利用多语言大模型（MLLMs）的逐块描述，精细地引导区域结构和纹理的生成。此外，在反向去噪过程中，生成的噪声被分解为低频和高频空间成分。这些成分在多个提示层级上进行条件化，包括详细的逐块描述和更广泛的图像级提示，从而在分层语义指导下实现提示引导的去噪。这进一步使得生成过程更专注于局部空间区域，并确保生成的图像在保持高清晰度的同时，具有连贯的局部和全局语义、结构和纹理。广泛的实验表明，HiPrompt在生成高分辨率图像方面优于现有最先进的方法，显著减少了物体重复现象并提高了结构质量。</td>
<td>Xinyu Liu</td>
<td><a href="http://arxiv.org/pdf/2409.02919v1">PDF</a></td>
<td>N/A</td>
<td>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts</td>
<td>The potential for higher-resolution image generation using pretrained diffusion models is immense, yet these models often struggle with issues of object repetition and structural artifacts especially when scaling to 4K resolution and higher. We figure out that the problem is caused by that, a single prompt for the generation of multiple scales provides insufficient efficacy. In response, we propose HiPrompt, a new tuning-free solution that tackles the above problems by introducing hierarchical prompts. The hierarchical prompts offer both global and local guidance. Specifically, the global guidance comes from the user input that describes the overall content, while the local guidance utilizes patch-wise descriptions from MLLMs to elaborately guide the regional structure and texture generation. Furthermore, during the inverse denoising process, the generated noise is decomposed into low- and high-frequency spatial components. These components are conditioned on multiple prompt levels, including detailed patch-wise descriptions and broader image-level prompts, facilitating prompt-guided denoising under hierarchical semantic guidance. It further allows the generation to focus more on local spatial regions and ensures the generated images maintain coherent local and global semantics, structures, and textures with high definition. Extensive experiments demonstrate that HiPrompt outperforms state-of-the-art works in higher-resolution image generation, significantly reducing object repetition and enhancing structural quality.</td>
</tr>
<tr>
<td>SpecMon：安全协议的模块化黑盒运行时监控</td>
<td>在形式化协议规范与其具体实现之间存在验证差距，本工作旨在通过监控以符合形式化规范的方式来弥合这一差距。我们通过应用程序使用的网络和加密库获取事件流，即使没有源代码访问权限也能实现这一点。然后，我们使用一种高效的算法将这些观察结果与规范模型中有效的轨迹进行匹配。与以往的工作相比，我们的算法能够处理非确定性，因此可以处理多个会话。它还实现了低开销，我们在WireGuard参考实现和之前工作的案例研究中展示了这一点。我们发现，WireGuard的参考Tamarin模型只需稍作改动即可使用：我们只需要指定线路格式并修正我们在进行案例研究时发现的一些小错误。我们还为我们的算法提供了一个健全性结果，确保它只接受根据规范模型有效的活动流。</td>
<td>Kevin Morio</td>
<td><a href="http://arxiv.org/pdf/2409.02918v1">PDF</a></td>
<td>N/A</td>
<td>SpecMon: Modular Black-Box Runtime Monitoring of Security Protocols</td>
<td>There exists a verification gap between formal protocol specifications and their actual implementations, which this work aims to bridge via monitoring for compliance to the formal specification. We instrument the networking and cryptographic library the application uses to obtain a stream of events. This is possible even without source code access. We then use an efficient algorithm to match these observations to traces that are valid in the specification model. In contrast to prior work, our algorithm can handle non-determinism and thus, multiple sessions. It also achieves a low overhead, which we demonstrate on the WireGuard reference implementation and a case study from prior work. We find that the reference Tamarin model for WireGuard can be used with little change: We only need to specify wire formats and correct some small inaccuracies that we discovered while conducting the case study. We also provide a soundness result for our algorithm that ensures it accepts only event streams that are valid according to the specification model.</td>
</tr>
<tr>
<td>UC-NeRF：从内窥镜稀疏视图中的不确定性感知条件神经辐射场</td>
<td>可视化手术场景对于在微创手术过程中揭示内部解剖结构至关重要。新视角合成是一项关键技术，提供几何和外观重建，增强对手术场景的理解、规划和决策。尽管神经辐射场（NeRF）取得了显著成就，但其直接应用于手术场景会产生不满意的结果，主要面临两个挑战：内窥镜稀疏视角和显著的光度不一致性。本文提出了一种不确定性感知的条件性NeRF用于新视角合成，以解决从稀疏手术视角中严重的形状-辐射模糊问题。UC-NeRF的核心是将多视角不确定性估计引入神经辐射场，以自适应地建模严重的光度不一致性。具体而言，UC-NeRF首先构建了一个多视角立体网络形式的连续性学习器，以建立稀疏视角的几何对应关系并生成不确定性估计和特征先验。在神经渲染中，我们设计了一个基础自适应的NeRF网络，利用不确定性估计来显式处理光度不一致性。此外，采用不确定性引导的几何蒸馏来增强几何学习。在SCARED和Hamlyn数据集上的实验表明，我们在渲染外观和几何方面表现优越，持续优于当前最先进的方法。我们的代码将在\url{https://github.com/wrld/UC-NeRF}上发布。</td>
<td>Jiaxin Guo</td>
<td><a href="http://arxiv.org/pdf/2409.02917v1">PDF</a></td>
<td>N/A</td>
<td>UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views</td>
<td>Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches. Our code will be released at \url{https://github.com/wrld/UC-NeRF}.</td>
</tr>
<tr>
<td>大语言模型能考取驾照吗？ —— 迈向自动驾驶领域可靠AGI的基准测试</td>
<td>大型视觉语言模型（LVLMs）近期引起了广泛关注，许多研究致力于利用其通用知识来提升自动驾驶模型的可解释性和鲁棒性。然而，LVLMs通常依赖于大规模的通用数据集，缺乏专业和安全驾驶所需的专门知识。现有的视觉语言驾驶数据集主要集中在场景理解和决策上，没有提供关于交通规则和驾驶技能的明确指导，而这些是直接关系到驾驶安全的关键方面。为了填补这一空白，我们提出了IDKB，这是一个包含超过一百万条数据项的大规模数据集，收集自多个国家，包括驾驶手册、理论测试数据和模拟路试数据。IDKB涵盖了从理论到实践几乎所有明确的驾驶所需知识，类似于获取驾驶执照的过程。特别地，我们使用IDKB对15个LVLMs进行了全面测试，以评估其在自动驾驶背景下的可靠性，并提供了广泛的分析。我们还对流行的模型进行了微调，取得了显著的性能提升，进一步验证了我们数据集的重要性。项目页面可以在以下链接找到：\url{https://4dvlab.github.io/project_page/idkb.html}</td>
<td>Yuhang Lu</td>
<td><a href="http://arxiv.org/pdf/2409.02914v1">PDF</a></td>
<td>N/A</td>
<td>Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving</td>
<td>Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guidance on traffic rules and driving skills, which are critical aspects directly related to driving safety. To bridge this gap, we propose IDKB, a large-scale dataset containing over one million data items collected from various countries, including driving handbooks, theory test data, and simulated road test data. Much like the process of obtaining a driver's license, IDKB encompasses nearly all the explicit knowledge needed for driving from theory to practice. In particular, we conducted comprehensive tests on 15 LVLMs using IDKB to assess their reliability in the context of autonomous driving and provided extensive analysis. We also fine-tuned popular models, achieving notable performance improvements, which further validate the significance of our dataset. The project page can be found at: \url{https://4dvlab.github.io/project_page/idkb.html}</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>